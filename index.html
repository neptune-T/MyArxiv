<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-19T00:00:00Z">2024-12-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling 4D Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Vélez, Luisa Polanía, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Pătrăucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling has not yet been convincingly demonstrated for pure self-supervised
learning from video. However, prior work has focused evaluations on
semantic-related tasks $\unicode{x2013}$ action classification, ImageNet
classification, etc. In this paper we focus on evaluating self-supervised
learning on non-semantic vision tasks that are more spatial (3D) and temporal
(+1D = 4D), such as camera pose estimation, point and object tracking, and
depth estimation. We show that by learning from very large video datasets,
masked auto-encoding (MAE) with transformer video models actually scales,
consistently improving performance on these 4D tasks, as model size increases
from 20M all the way to the largest by far reported self-supervised video model
$\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with
many recent image and video models demonstrates the benefits of scaling 4D
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in Large Vision-Language Models (LVLMs),
existing pixel-grounding models operate on single-image settings, limiting
their ability to perform detailed, fine-grained comparisons across multiple
images. Conversely, current multi-image understanding models lack pixel-level
grounding. Our work addresses this gap by introducing the task of multi-image
pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates
pixel-level grounding with robust multi-image reasoning capabilities to produce
contextually rich, pixel-grounded explanations. Central to PRIMA is an
efficient vision module that queries fine-grained visual representations across
multiple images, reducing TFLOPs by $25.3\%$. To support training and
evaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark
consisting of $\sim$224K question-answer pairs that require fine-grained visual
understanding across multiple images. Experimental results demonstrate PRIMA
outperforms state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://plan-lab.github.io/prima</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the advent of Multimodal Large Language Models (MLLMs), they have made
a significant impact across a wide range of real-world applications,
particularly in Autonomous Driving (AD). Their ability to process complex
visual data and reason about intricate driving scenarios has paved the way for
a new paradigm in end-to-end AD systems. However, the progress of developing
end-to-end models for AD has been slow, as existing fine-tuning methods demand
substantial resources, including extensive computational power, large-scale
datasets, and significant funding. Drawing inspiration from recent advancements
in inference computing, we propose OpenEMMA, an open-source end-to-end
framework based on MLLMs. By incorporating the Chain-of-Thought reasoning
process, OpenEMMA achieves significant improvements compared to the baseline
when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates
effectiveness, generalizability, and robustness across a variety of challenging
driving scenarios, offering a more efficient and effective approach to
autonomous driving. We release all the codes in
https://github.com/taco-group/OpenEMMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large vision language models (VLMs) tailored for
autonomous driving (AD) have shown strong scene understanding and reasoning
capabilities, making them undeniable candidates for end-to-end driving systems.
However, limited work exists on studying the trustworthiness of DriveVLMs -- a
critical factor that directly impacts public transportation safety. In this
paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for
large vision-language models in autonomous driving (DriveVLMs), considering
diverse perspectives -- including trustfulness, safety, robustness, privacy,
and fairness. We constructed the largest visual question-answering dataset for
investigating trustworthiness issues in driving scenarios, comprising over 10k
unique scenes and 18k queries. We evaluated six publicly available VLMs,
spanning from generalist to specialist, from open-source to commercial models.
Our exhaustive evaluations have unveiled previously undiscovered
vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found
that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform
specialized models fine-tuned for driving in terms of overall trustworthiness.
DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing
sensitive information. Additionally, both generalist and specialist VLMs remain
susceptible to adversarial attacks and struggle to ensure unbiased
decision-making across diverse environments and populations. Our findings call
for immediate and decisive action to address the trustworthiness of DriveVLMs
-- an issue of critical importance to public safety and the welfare of all
citizens relying on autonomous transportation systems. Our benchmark is
publicly available at \url{https://github.com/taco-group/AutoTrust}, and the
leaderboard is released at \url{https://taco-group.github.io/AutoTrust/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper targets the challenge of real-time LiDAR re-simulation in dynamic
driving scenarios. Recent approaches utilize neural radiance fields combined
with the physical modeling of LiDAR sensors to achieve high-fidelity
re-simulation results. Unfortunately, these methods face limitations due to
high computational demands in large-scale scenes and cannot perform real-time
LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel
framework that supports real-time, physically accurate LiDAR re-simulation for
driving scenes. Our primary contribution is the development of an efficient and
effective rendering pipeline, which integrates Gaussian primitives and
hardware-accelerated ray tracing technology. Specifically, we model the
physical properties of LiDAR sensors using Gaussian primitives with learnable
parameters and incorporate scene graphs to handle scene dynamics. Building upon
this scene representation, our framework first constructs a bounding volume
hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views
through a differentiable rendering algorithm. Importantly, our framework
supports realistic rendering with flexible scene editing operations and various
sensor configurations. Extensive experiments across multiple public benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
rendering quality and efficiency. Our project page is at
https://zju3dv.github.io/lidar-rt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/lidar-rt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preventing Local Pitfalls in Vector Quantization via Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-quantized networks (VQNs) have exhibited remarkable performance across
various tasks, yet they are prone to training instability, which complicates
the training process due to the necessity for techniques such as subtle
initialization and model distillation. In this study, we identify the local
minima issue as the primary cause of this instability. To address this, we
integrate an optimal transport method in place of the nearest neighbor search
to achieve a more globally informed assignment. We introduce OptVQ, a novel
vector quantization method that employs the Sinkhorn algorithm to optimize the
optimal transport problem, thereby enhancing the stability and efficiency of
the training process. To mitigate the influence of diverse data distributions
on the Sinkhorn algorithm, we implement a straightforward yet effective
normalization strategy. Our comprehensive experiments on image reconstruction
tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses
current state-of-the-art VQNs in reconstruction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/zbr17/OptVQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal
  Audio-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video
generation that leverages the activations of frozen video and audio diffusion
models for temporally-aligned cross-modal conditioning. The key to our
framework is a Fusion Block that enables bidirectional information exchange
between our backbone video and audio diffusion models through a
temporally-aligned self attention operation. Unlike prior work that uses
feature extractors pretrained for other tasks for the conditioning signal,
AV-Link can directly leverage features obtained by the complementary modality
in a single framework i.e. video features to generate audio, or audio features
to generate video. We extensively evaluate our design choices and demonstrate
the ability of our method to achieve synchronized and high-quality audiovisual
content, showcasing its potential for applications in immersive media
generation. Project Page: snap-research.github.io/AVLink/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: snap-research.github.io/AVLink/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LlamaFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LlamaFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LlamaFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LlamaFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LlamaFusion improves image understanding by 20% and image generation by 3.6%
using only 50% of the FLOPs while maintaining Llama-3's language capabilities.
We also demonstrate that this framework can adapt existing vision-language
models with multimodal generation ability. Overall, this framework not only
leverages existing computational investments in text-only LLMs but also enables
the parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data for Mathematical Copilots: Better Ways of Presenting Proofs for
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Frieder, Jonas Bayer, Katherine M. Collins, Julius Berner, Jacob Loader, András Juhász, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Thomas Lukasiewicz, Timothy Gowers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The suite of datasets commonly used to train and evaluate the mathematical
capabilities of AI-based mathematical copilots (primarily large language
models) exhibit several shortcomings. These limitations include a restricted
scope of mathematical complexity, typically not exceeding lower
undergraduate-level mathematics, binary rating protocols and other issues,
which makes comprehensive proof-based evaluation suites difficult. We
systematically explore these limitations and contend that enhancing the
capabilities of large language models, or any forthcoming advancements in
AI-based mathematical assistants (copilots or "thought partners"), necessitates
a paradigm shift in the design of mathematical datasets and the evaluation
criteria of mathematical ability: It is necessary to move away from
result-based datasets (theorem statement to theorem proof) and convert the rich
facets of mathematical research practice to data LLMs can train on. Examples of
these are mathematical workflows (sequences of atomic, potentially
subfield-dependent tasks that are often performed when creating new
mathematics), which are an important part of the proof-discovery process.
Additionally, we advocate for mathematical dataset developers to consider the
concept of "motivated proof", introduced by G. P\'olya in 1949, which can serve
as a blueprint for datasets that offer a better proof learning signal,
alleviating some of the mentioned limitations. Lastly, we introduce math
datasheets for datasets, extending the general, dataset-agnostic variants of
datasheets: We provide a questionnaire designed specifically for math datasets
that we urge dataset creators to include with their datasets. This will make
creators aware of potential limitations of their datasets while at the same
time making it easy for readers to assess it from the point of view of training
and evaluating mathematical copilots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, Jonathan Francis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot learning is witnessing a significant increase in the size, diversity,
and complexity of pre-collected datasets, mirroring trends in domains such as
natural language processing and computer vision. Many robot learning methods
treat such datasets as multi-task expert data and learn a multi-task,
generalist policy by training broadly across them. Notably, while these
generalist policies can improve the average performance across many tasks, the
performance of generalist policies on any one task is often suboptimal due to
negative transfer between partitions of the data, compared to task-specific
specialist policies. In this work, we argue for the paradigm of training
policies during deployment given the scenarios they encounter: rather than
deploying pre-trained policies to unseen problems in a zero-shot manner, we
non-parametrically retrieve and train models directly on relevant data at test
time. Furthermore, we show that many robotics tasks share considerable amounts
of low-level behaviors and that retrieval at the "sub"-trajectory granularity
enables significantly improved data utilization, generalization, and robustness
in adapting policies to novel problems. In contrast, existing full-trajectory
retrieval methods tend to underutilize the data and miss out on shared
cross-task content. This work proposes STRAP, a technique for leveraging
pre-trained vision foundation models and dynamic time warping to retrieve
sub-sequences of trajectories from large training corpora in a robust fashion.
STRAP outperforms both prior retrieval algorithms and multi-task learning
methods in simulated and real experiments, showing the ability to scale to much
larger offline datasets in the real world as well as the ability to learn
robust control policies with just a handful of real-world demonstrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://weirdlabuw.github.io/strap/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Chaturvedi, Daniel Nichols, Siddharth Singh, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) based coding tools have been tremendously
successful as software development assistants, yet they are often designed for
general purpose programming tasks and perform poorly for more specialized
domains such as high performance computing. Creating specialized models and
tools for these domains is crucial towards gaining the benefits of LLMs in
areas such as HPC. While previous work has explored HPC-specific models, LLMs
still struggle to generate parallel code and it is not at all clear what
hurdles are still holding back these LLMs and what must be done to overcome
them. In this work, we conduct an in-depth study along the many axes of
fine-tuning a specialized HPC LLM in order to better understand the challenges.
Based on our findings we fine-tune and evaluate a specialized HPC LLM that is
shown to be the best performing open-source code LLM for parallel code
generation to date.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Uncertainty Estimation in Natural Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Aichberger, Kajetan Schweighofer, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly employed in real-world
applications, driving the need to evaluate the trustworthiness of their
generated text. To this end, reliable uncertainty estimation is essential.
Since current LLMs generate text autoregressively through a stochastic process,
the same prompt can lead to varying outputs. Consequently, leading uncertainty
estimation methods generate and analyze multiple output sequences to determine
the LLM's uncertainty. However, generating output sequences is computationally
expensive, making these methods impractical at scale. In this work, we inspect
the theoretical foundations of the leading methods and explore new directions
to enhance their computational efficiency. Building on the framework of proper
scoring rules, we find that the negative log-likelihood of the most likely
output sequence constitutes a theoretically grounded uncertainty measure. To
approximate this alternative measure, we propose G-NLL, which has the advantage
of being obtained using only a single output sequence generated by greedy
decoding. This makes uncertainty estimation more efficient and straightforward,
while preserving theoretical rigor. Empirical results demonstrate that G-NLL
achieves state-of-the-art performance across various LLMs and tasks. Our work
lays the foundation for efficient and reliable uncertainty estimation in
natural language generation, challenging the necessity of more computationally
involved methods currently leading the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Woodgate, Paul Marshall, Nirav Ajmeri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social norms are standards of behaviour common in a society. However, when
agents make decisions without considering how others are impacted, norms can
emerge that lead to the subjugation of certain agents. We present RAWL-E, a
method to create ethical norm-learning agents. RAWL-E agents operationalise
maximin, a fairness principle from Rawlsian ethics, in their decision-making
processes to promote ethical norms by balancing societal well-being with
individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios.
We find that norms emerging in RAWL-E agent societies enhance social welfare,
fairness, and robustness, and yield higher minimum experience compared to those
that emerge in agent societies that do not implement Rawlsian ethics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 8 tables (and supplementary material with
  reproducibility and additional results), accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Color Channel Independence for Improved Unsupervised Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Jäckl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric architectures can learn to extract distinct object
representations from visual scenes, enabling downstream applications on the
object level. Similarly to autoencoder-based image models, object-centric
approaches have been trained on the unsupervised reconstruction loss of images
encoded by RGB color spaces. In our work, we challenge the common assumption
that RGB images are the optimal color space for unsupervised learning in
computer vision. We discuss conceptually and empirically that other color
spaces, such as HSV, bear essential characteristics for object-centric
representation learning, like robustness to lighting conditions. We further
show that models improve when requiring them to predict additional color
channels. Specifically, we propose to transform the predicted targets to the
RGB-S space, which extends RGB with HSV's saturation component and leads to
markedly better reconstruction and disentanglement for five common evaluation
datasets. The use of composite color spaces can be implemented with basically
no computational overhead, is agnostic of the models' architecture, and is
universally applicable across a wide range of visual computing tasks and
training types. The findings of our approach encourage additional
investigations in computer vision tasks beyond object-centric learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages incl. references, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jet: A Modern <span class="highlight-title">Transformer</span>-Based Normalizing Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kolesnikov, André Susano Pinto, Michael Tschannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past, normalizing generative flows have emerged as a promising class
of generative models for natural images. This type of model has many modeling
advantages: the ability to efficiently compute log-likelihood of the input
data, fast generation and simple overall structure. Normalizing flows remained
a topic of active research but later fell out of favor, as visual quality of
the samples was not competitive with other model classes, such as GANs,
VQ-VAE-based approaches or diffusion models. In this paper we revisit the
design of the coupling-based normalizing flow models by carefully ablating
prior design choices and using computational blocks based on the Vision
Transformer architecture, not convolutional neural networks. As a result, we
achieve state-of-the-art quantitative and qualitative performance with a much
simpler architecture. While the overall visual quality is still behind the
current state-of-the-art models, we argue that strong normalizing flow models
can help advancing research frontier by serving as building components of more
powerful generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Pruning for Large Language Models with Structural Importance
  Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in large language models (LLMs) have significantly
improved language understanding and generation capabilities. However, it is
difficult to deploy LLMs on resource-constrained edge devices due to their high
computational and storage resource demands. To address this issue, we propose a
novel LLM model pruning method, namely structurally-aware adaptive pruning
(SAAP), to significantly reduce the computational and memory costs while
maintaining model performance. We first define an adaptive importance fusion
metric to evaluate the importance of all coupled structures in LLMs by
considering their homoscedastic uncertainty. Then, we rank the importance of
all modules to determine the specific layers that should be pruned to meet
particular performance requirements. Furthermore, we develop a new group
fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we
evaluate the proposed SAAP method on multiple LLMs across two common tasks,
i.e., zero-shot classification and text generation. Experimental results show
that our SAAP method outperforms several state-of-the-art baseline methods,
achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and
LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,
showcasing its practical advantages in resource-constrained scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outcome-Refining Process Supervision for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated remarkable capabilities in code
generation, yet they often struggle with complex programming tasks that require
deep algorithmic reasoning. While process supervision through learned reward
models shows promise in guiding reasoning steps, it requires expensive training
data and suffers from unreliable evaluation. We propose Outcome-Refining
Process Supervision, a novel paradigm that treats outcome refinement itself as
the process to be supervised. Our framework leverages concrete execution
signals to ground the supervision of reasoning steps, while using
tree-structured exploration to maintain multiple solution trajectories
simultaneously. Experiments demonstrate that our approach enables even smaller
models to achieve high success accuracy and performance metrics on competitive
programming tasks, creates more reliable verification than traditional reward
models without requiring training PRMs. Our approach achieves significant
improvements across 5 models and 3 datasets: an average of 26.9% increase in
correctness and 42.2% in efficiency. The results suggest that providing
structured reasoning space with concrete verification signals is crucial for
solving complex programming tasks. We open-source all our code and data at:
https://github.com/zhuohaoyu/ORPS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures, Code: https://github.com/zhuohaoyu/ORPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tests for model misspecification in simulation-based inference: from
  local distortions to global model checks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noemi Anau Montel, James Alvey, Christoph Weniger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model misspecification analysis strategies, such as anomaly detection, model
validation, and model comparison are a key component of scientific model
development. Over the last few years, there has been a rapid rise in the use of
simulation-based inference (SBI) techniques for Bayesian parameter estimation,
applied to increasingly complex forward models. To move towards fully
simulation-based analysis pipelines, however, there is an urgent need for a
comprehensive simulation-based framework for model misspecification analysis.
In this work, we provide a solid and flexible foundation for a wide range of
model discrepancy analysis tasks, using distortion-driven model
misspecification tests. From a theoretical perspective, we introduce the
statistical framework built around performing many hypothesis tests for
distortions of the simulation model. We also make explicit analytic connections
to classical techniques: anomaly detection, model validation, and
goodness-of-fit residual analysis. Furthermore, we introduce an efficient
self-calibrating training algorithm that is useful for practitioners. We
demonstrate the performance of the framework in multiple scenarios, making the
connection to classical results where they are valid. Finally, we show how to
conduct such a distortion-driven model misspecification test for real
gravitational wave data, specifically on the event GW150914.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures. Code available on github (NoemiAM/mist) at
  https://github.com/NoemiAM/mist</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Full <span class="highlight-title">Transformer</span>-based Framework for Automatic Pain Estimation using
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefanos Gkikas, Manolis Tsiknakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic estimation of pain is essential in designing an optimal pain
management system offering reliable assessment and reducing the suffering of
patients. In this study, we present a novel full transformer-based framework
consisting of a Transformer in Transformer (TNT) model and a Transformer
leveraging cross-attention and self-attention blocks. Elaborating on videos
from the BioVid database, we demonstrate state-of-the-art performances, showing
the efficacy, efficiency, and generalization capability across all the primary
pain estimation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Disentangled Equivariant Representation for Explicitly
  Controllable 3D Molecule Generation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liu, Youzhi Luo, Tianxiao Li, James Caverlee, Martin Renqiang Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the conditional generation of 3D drug-like molecules with
\textit{explicit control} over molecular properties such as drug-like
properties (e.g., Quantitative Estimate of Druglikeness or Synthetic
Accessibility score) and effectively binding to specific protein sites. To
tackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and
factorize the latent space of our generative model into two disentangled
aspects: molecular properties and the remaining structural context of 3D
molecules. Our model ensures explicit control over these molecular attributes
while maintaining equivariance of coordinate representation and invariance of
data likelihood. Furthermore, we introduce a novel alignment-based coordinate
loss to adapt equivariant networks for auto-regressive de-novo 3D molecule
generation from scratch. Extensive experiments validate our model's
effectiveness on property-guided and context-guided molecule generation, both
for de-novo 3D molecule design and structure-based drug discovery against
protein targets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce AceMath, a suite of frontier math models that
excel in solving complex math problems, along with highly effective reward
models capable of evaluating generated solutions and reliably identifying the
correct ones. To develop the instruction-tuned math models, we propose a
supervised fine-tuning (SFT) process that first achieves competitive
performance across general domains, followed by targeted fine-tuning for the
math domain using a carefully curated set of prompts and synthetically
generated responses. The resulting model, AceMath-72B-Instruct greatly
outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop
math-specialized reward model, we first construct AceMath-RewardBench, a
comprehensive and robust benchmark for evaluating math reward models across
diverse problems and difficulty levels. After that, we present a systematic
approach to build our math reward models. The resulting model, AceMath-72B-RM,
consistently outperforms state-of-the-art reward models. Furthermore, when
combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest
average rm@8 score across the math reasoning benchmarks. We will release model
weights, training data, and evaluation benchmarks at:
https://research.nvidia.com/labs/adlr/acemath
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Till the Layers Collapse: Compressing a Deep Neural Network through the
  Lenses of Batch Normalization Layers <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Liao, Nour Hezbri, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, deep neural networks are widely used since they can handle a variety
of complex tasks. Their generality makes them very powerful tools in modern
technology. However, deep neural networks are often overparameterized. The
usage of these large models consumes a lot of computation resources. In this
paper, we introduce a method called \textbf{T}ill the \textbf{L}ayers
\textbf{C}ollapse (TLC), which compresses deep neural networks through the
lenses of batch normalization layers. By reducing the depth of these networks,
our method decreases deep neural networks' computational requirements and
overall latency. We validate our method on popular models such as Swin-T,
MobileNet-V2, and RoBERTa, across both image classification and natural
language processing (NLP) tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DroughtSet: Understanding Drought Through Spatial-Temporal Learning <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuwei Tan, Qian Zhao, Yanlan Liu, Xueru Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drought is one of the most destructive and expensive natural disasters,
severely impacting natural resources and risks by depleting water resources and
diminishing agricultural yields. Under climate change, accurately predicting
drought is critical for mitigating drought-induced risks. However, the
intricate interplay among the physical and biological drivers that regulate
droughts limits the predictability and understanding of drought, particularly
at a subseasonal to seasonal (S2S) time scale. While deep learning has been
demonstrated with potential in addressing climate forecasting challenges, its
application to drought prediction has received relatively less attention. In
this work, we propose a new dataset, DroughtSet, which integrates relevant
predictive features and three drought indices from multiple remote sensing and
reanalysis datasets across the contiguous United States (CONUS). DroughtSet
specifically provides the machine learning community with a new real-world
dataset to benchmark drought prediction models and more generally, time-series
forecasting methods. Furthermore, we propose a spatial-temporal model SPDrought
to predict and interpret S2S droughts. Our model learns from the spatial and
temporal information of physical and biological features to predict three types
of droughts simultaneously. Multiple strategies are employed to quantify the
importance of physical and biological features for drought prediction. Our
results provide insights for researchers to better understand the
predictability and sensitivity of drought to biological and physical
conditions. We aim to contribute to the climate field by proposing a new tool
to predict and understand the occurrence of droughts and provide the AI
community with a new benchmark to study deep learning applications in climate
science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging
  <span class="highlight-title">Dataset</span>s with In-Context Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical researchers and clinicians often need to perform novel segmentation
tasks on a set of related images. Existing methods for segmenting a new dataset
are either interactive, requiring substantial human effort for each image, or
require an existing set of manually labeled images. We introduce a system,
MultiverSeg, that enables practitioners to rapidly segment an entire new
dataset without requiring access to any existing labeled data from that task or
domain. Along with the image to segment, the model takes user interactions such
as clicks, bounding boxes or scribbles as input, and predicts a segmentation.
As the user segments more images, those images and segmentations become
additional inputs to the model, providing context. As the context set of
labeled images grows, the number of interactions required to segment each new
image decreases. We demonstrate that MultiverSeg enables users to interactively
segment new datasets efficiently, by amortizing the number of interactions per
image to achieve an accurate segmentation. Compared to using a state-of-the-art
interactive segmentation method, using MultiverSeg reduced the total number of
scribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images
from unseen tasks. We release code and model weights at
https://multiverseg.csail.mit.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://multiverseg.csail.mit.edu Keywords:
  interactive segmentation, in-context learning, medical image analysis,
  biomedical imaging, image annotation, visual prompting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Albert Ali Salah, Itir Onal Ertugrul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores image modeling from the frequency space and introduces
DCTdiff, an end-to-end diffusion generative paradigm that efficiently models
images in the discrete cosine transform (DCT) space. We investigate the design
space of DCTdiff and reveal the key design factors. Experiments on different
frameworks (UViT, DiT), generation tasks, and various diffusion samplers
demonstrate that DCTdiff outperforms pixel-based diffusion models regarding
generative quality and training efficiency. Remarkably, DCTdiff can seamlessly
scale up to high-resolution generation without using the latent diffusion
paradigm. Finally, we illustrate several intriguing properties of DCT image
modeling. For example, we provide a theoretical proof of why `image diffusion
can be seen as spectral autoregression', bridging the gap between diffusion and
autoregressive models. The effectiveness of DCTdiff and the introduced
properties suggest a promising direction for image modeling in the frequency
space. The code is at \url{https://github.com/forever208/DCTdiff}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and
  Semantic Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunità, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound designers and Foley artists usually sonorize a scene, such as from a
movie or video game, by manually annotating and sonorizing each action of
interest in the video. In our case, the intent is to leave full creative
control to sound designers with a tool that allows them to bypass the more
repetitive parts of their work, thus being able to focus on the creative
aspects of sound production. We achieve this presenting Stable-V2A, a two-stage
model consisting of: an RMS-Mapper that estimates an envelope representative of
the audio characteristics associated with the input video; and Stable-Foley, a
diffusion model based on Stable Audio Open that generates audio semantically
and temporally aligned with the target video. Temporal alignment is guaranteed
by the use of the envelope as a ControlNet input, while semantic alignment is
achieved through the use of sound representations chosen by the designer as
cross-attention conditioning of the diffusion process. We train and test our
model on Greatest Hits, a dataset commonly used to evaluate V2A models. In
addition, to test our model on a case study of interest, we introduce Walking
The Maps, a dataset of videos extracted from video games depicting animated
characters walking in different locations. Samples and code available on our
demo page at https://ispamm.github.io/Stable-V2A.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Federated Learning in the Face of Covariate Shift: A Magnitude
  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozgu Goksu, Nicolas Pugeault
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of highly sophisticated neural networks has allowed for fast
progress in every field of computer vision, however, applications where
annotated data is prohibited due to privacy or security concerns remain
challenging. Federated Learning (FL) offers a promising framework for
individuals aiming to collaboratively develop a shared model while preserving
data privacy. Nevertheless, our findings reveal that variations in data
distribution among clients can profoundly affect FL methodologies, primarily
due to instabilities in the aggregation process. We also propose a novel FL
framework to mitigate the adverse effects of covariate shifts among federated
clients by combining individual parameter pruning and regularization techniques
to improve the robustness of individual clients' models to aggregate. Each
client's model is optimized through magnitude-based pruning and the addition of
dropout and noise injection layers to build more resilient decision pathways in
the networks and improve the robustness of the model's parameter aggregation
step. The proposed framework is capable of extracting robust representations
even in the presence of very large covariate shifts among client data
distributions and in the federation of a small number of clients. Empirical
findings substantiate the effectiveness of our proposed methodology across
common benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.
Furthermore, we introduce the CelebA-Gender dataset, specifically designed to
evaluate performance on a more realistic domain. The proposed method is capable
of extracting robust representations even in the presence of both high and low
covariate shifts among client data distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start
  Cross-Domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used in various real-world applications, but
they often encounter the persistent challenge of the user cold-start problem.
Cross-domain recommendation (CDR), which leverages user interactions from one
domain to improve prediction performance in another, has emerged as a promising
solution. However, users with similar preferences in the source domain may
exhibit different interests in the target domain. Therefore, directly
transferring embeddings may introduce irrelevant source-domain collaborative
information. In this paper, we propose a novel graph-based disentangled
contrastive learning framework to capture fine-grained user intent and filter
out irrelevant collaborative information, thereby avoiding negative transfer.
Specifically, for each domain, we use a multi-channel graph encoder to capture
diverse user intents. We then construct the affinity graph in the embedding
space and perform multi-step random walks to capture high-order user similarity
relationships. Treating one domain as the target, we propose a disentangled
intent-wise contrastive learning approach, guided by user similarity, to refine
the bridging of user intents across domains. Extensive experiments on four
benchmark CDR datasets demonstrate that DisCo consistently outperforms existing
state-of-the-art baselines, thereby validating the effectiveness of both DisCo
and its components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stitch Contrast and Segment_Learning a Human Action Segmentation Model
  Using Trimmed Skeleton Videos <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Tian, Pierre Payeur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing skeleton-based human action classification models rely on
well-trimmed action-specific skeleton videos for both training and testing,
precluding their scalability to real-world applications where untrimmed videos
exhibiting concatenated actions are predominant. To overcome this limitation,
recently introduced skeleton action segmentation models involve un-trimmed
skeleton videos into end-to-end training. The model is optimized to provide
frame-wise predictions for any length of testing videos, simultaneously
realizing action localization and classification. Yet, achieving such an
improvement im-poses frame-wise annotated skeleton videos, which remains
time-consuming in practice. This paper features a novel framework for
skeleton-based action segmentation trained on short trimmed skeleton videos,
but that can run on longer un-trimmed videos. The approach is implemented in
three steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral
skeleton stitching scheme that treats trimmed skeleton videos as elementary
human motions that compose a semantic space and can be sampled to generate
multi-action stitched se-quences. Contrast learns contrastive representations
from stitched sequences with a novel discrimination pretext task that enables a
skeleton encoder to learn meaningful action-temporal contexts to improve action
segmentation. Finally, Segment relates the proposed method to action
segmentation by learning a segmentation layer while handling particular da-ta
availability. Experiments involve a trimmed source dataset and an untrimmed
target dataset in an adaptation formulation for real-world skeleton-based human
action segmentation to evaluate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Injection via <span class="highlight-title">Prompt</span> Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalle Kujanpää, Harri Valpola, Alexander Ilin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many practical applications, large language models (LLMs) need to
incorporate new knowledge not present in their pre-training data. The primary
methods for this are fine-tuning and retrieval-augmented generation (RAG).
Although RAG has emerged as the industry standard for knowledge injection,
fine-tuning has not yet achieved comparable success. In this paper, we propose
a new fine-tuning technique for learning new knowledge and show that it can
reach the performance of RAG. The proposed method is based on the
self-distillation approach, which we call prompt distillation. First, we
generate question-answer pairs about the new knowledge. Then, we fine-tune a
student model on the question-answer pairs to imitate the output distributions
of a teacher model, which additionally receives the new knowledge in its
prompt. The student model is identical to the teacher, except it is equipped
with a LoRA adapter. This training procedure facilitates distilling the new
knowledge from the teacher's prompt into the student's weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDOL: Instant Photorealistic 3D Human Creation from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating a high-fidelity, animatable 3D full-body avatar from a single image
is a challenging task due to the diverse appearance and poses of humans and the
limited availability of high-quality training data. To achieve fast and
high-quality human reconstruction, this work rethinks the task from the
perspectives of dataset, model, and representation. First, we introduce a
large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K
diverse, photorealistic sets of human images. Each set contains 24-view frames
in specific human poses, generated using a pose-controllable
image-to-multi-view model. Next, leveraging the diversity in views, poses, and
appearances within HuGe100K, we develop a scalable feed-forward transformer
model to predict a 3D human Gaussian representation in a uniform space from a
given human image. This model is trained to disentangle human pose, body shape,
clothing geometry, and texture. The estimated Gaussians can be animated without
post-processing. We conduct comprehensive experiments to validate the
effectiveness of the proposed dataset and method. Our model demonstrates the
ability to efficiently reconstruct photorealistic humans at 1K resolution from
a single input image using a single GPU instantly. Additionally, it seamlessly
supports various applications, as well as shape and texture editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures, includes main content, supplementary materials,
  and references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corn Ear Detection and Orientation Estimation Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Sprague, John Evans, Michael Mardikes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring growth behavior of maize plants such as the development of ears
can give key insights into the plant's health and development. Traditionally,
the measurement of the angle of ears is performed manually, which can be
time-consuming and prone to human error. To address these challenges, this
paper presents a computer vision-based system for detecting and tracking ears
of corn in an image sequence. The proposed system could accurately detect,
track, and predict the ear's orientation, which can be useful in monitoring
their growth behavior. This can significantly save time compared to manual
measurement and enables additional areas of ear orientation research and
potential increase in efficiencies for maize production. Using an object
detector with keypoint detection, the algorithm proposed could detect 90
percent of all ears. The cardinal estimation had a mean absolute error (MAE) of
18 degrees, compared to a mean 15 degree difference between two people
measuring by hand. These results demonstrate the feasibility of using computer
vision techniques for monitoring maize growth and can lead to further research
in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages;15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Point to probabilistic gradient boosting for claim frequency and
  severity prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Chevalier, Marie-Pier Côté
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient boosting for decision tree algorithms are increasingly used in
actuarial applications as they show superior predictive performance over
traditional generalized linear models. Many improvements and sophistications to
the first gradient boosting machine algorithm exist. We present in a unified
notation, and contrast, all the existing point and probabilistic gradient
boosting for decision tree algorithms: GBM, XGBoost, DART, LightGBM, CatBoost,
EGBM, PGBM, XGBoostLSS, cyclic GBM, and NGBoost. In this comprehensive
numerical study, we compare their performance on five publicly available
datasets for claim frequency and severity, of various size and comprising
different number of (high cardinality) categorical variables. We explain how
varying exposure-to-risk can be handled with boosting in frequency models. We
compare the algorithms on the basis of computational efficiency, predictive
performance, and model adequacy. LightGBM and XGBoostLSS win in terms of
computational efficiency. The fully interpretable EGBM achieves competitive
predictive performance compared to the black box algorithms considered. We find
that there is no trade-off between model adequacy and predictive accuracy: both
are achievable simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 4 figures, 26 tables, 7 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion priors for Bayesian 3D reconstruction from incomplete
  measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian L. Möbius, Michael Habeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many inverse problems are ill-posed and need to be complemented by prior
information that restricts the class of admissible models. Bayesian approaches
encode this information as prior distributions that impose generic properties
on the model such as sparsity, non-negativity or smoothness. However, in case
of complex structured models such as images, graphs or three-dimensional (3D)
objects,generic prior distributions tend to favor models that differ largely
from those observed in the real world. Here we explore the use of diffusion
models as priors that are combined with experimental data within a Bayesian
framework. We use 3D point clouds to represent 3D objects such as household
items or biomolecular complexes formed from proteins and nucleic acids. We
train diffusion models that generate coarse-grained 3D structures at a medium
resolution and integrate these with incomplete and noisy experimental data. To
demonstrate the power of our approach, we focus on the reconstruction of
biomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which
is an important inverse problem in structural biology. We find that posterior
sampling with diffusion model priors allows for 3D reconstruction from very
sparse, low-resolution and partial observations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional
  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature
  Screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood
within the skull, which occurs due to the rupture of blood vessels in or around
the brain. If this condition is not diagnosed in a timely manner and
appropriately treated, it can lead to serious complications such as decreased
consciousness, permanent neurological disabilities, or even death.The primary
aim of this study is to detect the occurrence or non-occurrence of ICH,
followed by determining the type of subdural hemorrhage (SDH). These tasks are
framed as two separate binary classification problems. By adding two layers to
the co-scale convolutional attention (CCA) classifier architecture, we
introduce a novel approach for ICH detection. In the first layer, after
extracting features from different slices of computed tomography (CT) scan
images, we combine these features and select the 50 components that capture the
highest variance in the data, considering them as informative features. We then
assess the discriminative power of these features using the bootstrap forest
algorithm, discarding those that lack sufficient discriminative ability between
different classes. This algorithm explicitly determines the contribution of
each feature to the final prediction, assisting us in developing an explainable
AI model. The features feed into a boosting neural network as a latent feature
space. In the second layer, we introduce a novel uncertainty-based fuzzy
integral operator to fuse information from different CT scan slices. This
operator, by accounting for the dependencies between consecutive slices,
significantly improves detection accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Subspaces of Policies for Continual Offline Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Kobanda, Rémy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dynamic domains such as autonomous robotics and video game simulations,
agents must continuously adapt to new tasks while retaining previously acquired
skills. This ongoing process, known as Continual Reinforcement Learning,
presents significant challenges, including the risk of forgetting past
knowledge and the need for scalable solutions as the number of tasks increases.
To address these issues, we introduce HIerarchical LOW-rank Subspaces of
Policies (HILOW), a novel framework designed for continual learning in offline
navigation settings. HILOW leverages hierarchical policy subspaces to enable
flexible and efficient adaptation to new tasks while preserving existing
knowledge. We demonstrate, through a careful experimental study, the
effectiveness of our method in both classical MuJoCo maze environments and
complex video game-like simulations, showcasing competitive performance and
satisfying adaptability according to classical continual learning metrics, in
particular regarding memory usage. Our work provides a promising framework for
real-world applications where continuous learning from pre-collected data is
essential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surrogate-assisted multi-objective design of complex multibody systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Augustina C. Amakor, Manuel B. Berkemeier, Meike Wohlleben, Walter Sextro, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimization of large-scale multibody systems is a numerically
challenging task, in particular when considering multiple conflicting criteria
at the same time. In this situation, we need to approximate the Pareto set of
optimal compromises, which is significantly more expensive than finding a
single optimum in single-objective optimization. To prevent large costs, the
usage of surrogate models, constructed from a small but informative number of
expensive model evaluations, is a very popular and widely studied approach. The
central challenge then is to ensure a high quality (that is, near-optimality)
of the solutions that were obtained using the surrogate model, which can be
hard to guarantee with a single pre-computed surrogate. We present a
back-and-forth approach between surrogate modeling and multi-objective
optimization to improve the quality of the obtained solutions. Using the
example of an expensive-to-evaluate multibody system, we compare different
strategies regarding multi-objective optimization, sampling and also surrogate
modeling, to identify the most promising approach in terms of computational
efficiency and solution quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2412.01566</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy Regularized Task Representation Learning for Offline
  Meta-Reinforcement Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza nakhaei, Aidan Scannell, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline meta-reinforcement learning aims to equip agents with the ability to
rapidly adapt to new tasks by training on data from a set of different tasks.
Context-based approaches utilize a history of state-action-reward transitions
-- referred to as the context -- to infer representations of the current task,
and then condition the agent, i.e., the policy and value function, on the task
representations. Intuitively, the better the task representations capture the
underlying tasks, the better the agent can generalize to new tasks.
Unfortunately, context-based approaches suffer from distribution mismatch, as
the context in the offline data does not match the context at test time,
limiting their ability to generalize to the test tasks. This leads to the task
representations overfitting to the offline training data. Intuitively, the task
representations should be independent of the behavior policy used to collect
the offline data. To address this issue, we approximately minimize the mutual
information between the distribution over the task representations and behavior
policy by maximizing the entropy of behavior policy conditioned on the task
representations. We validate our approach in MuJoCo environments, showing that
compared to baselines, our task representations more faithfully represent the
underlying tasks, leading to outperforming prior methods in both
in-distribution and out-of-distribution tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Answer Set Networks: Casting Answer Set Programming into Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arseny Skryagin, Daniel Ochs, Phillip Deibert, Simon Kohaut, Devendra Singh Dhami, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Answer Set Programming (ASP) allows constraining neural-symbolic
(NeSy) systems, its employment is hindered by the prohibitive costs of
computing stable models and the CPU-bound nature of state-of-the-art solvers.
To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on
Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep
Probabilistic Logic Programming (DPPL). Specifically, we show how to translate
ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded
problem by leveraging GPU's batching and parallelization capabilities. Our
experimental evaluations demonstrate that ASNs outperform state-of-the-art
CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following
two contributions based on the strengths of ASNs. Namely, we are the first to
show the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs
to guide the training with logic. Further, we show the "constitutional
navigation" of drones, i.e., encoding public aviation laws in an ASN for
routing Unmanned Aerial Vehicles in uncertain environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARIA: a Multimodal <span class="highlight-title">Transformer</span> Model for Incomplete Healthcare Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camillo Maria Caruso, Paolo Soda, Valerio Guarrasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In healthcare, the integration of multimodal data is pivotal for developing
comprehensive diagnostic and predictive models. However, managing missing data
remains a significant challenge in real-world applications. We introduce MARIA
(Multimodal Attention Resilient to Incomplete datA), a novel transformer-based
deep learning model designed to address these challenges through an
intermediate fusion strategy. Unlike conventional approaches that depend on
imputation, MARIA utilizes a masked self-attention mechanism, which processes
only the available data without generating synthetic values. This approach
enables it to effectively handle incomplete datasets, enhancing robustness and
minimizing biases introduced by imputation methods. We evaluated MARIA against
10 state-of-the-art machine learning and deep learning models across 8
diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms
existing methods in terms of performance and resilience to varying levels of
data incompleteness, underscoring its potential for critical healthcare
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stack Trace Deduplication: Faster, More Accurately, and in More
  Realistic Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egor Shibaev, Denis Sushentsev, Yaroslav Golubev, Aleksandr Khvorov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large-scale software systems, there are often no fully-fledged bug reports
with human-written descriptions when an error occurs. In this case, developers
rely on stack traces, i.e., series of function calls that led to the error.
Since there can be tens and hundreds of thousands of them describing the same
issue from different users, automatic deduplication into categories is
necessary to allow for processing. Recent works have proposed powerful deep
learning-based approaches for this, but they are evaluated and compared in
isolation from real-life workflows, and it is not clear whether they will
actually work well at scale.
  To overcome this gap, this work presents three main contributions: a novel
model, an industry-based dataset, and a multi-faceted evaluation. Our model
consists of two parts - (1) an embedding model with byte-pair encoding and
approximate nearest neighbor search to quickly find the most relevant stack
traces to the incoming one, and (2) a reranker that re-ranks the most fitting
stack traces, taking into account the repeated frames between them. To
complement the existing datasets collected from open-source projects, we share
with the community SlowOps - a dataset of stack traces from IntelliJ-based
products developed by JetBrains, which has an order of magnitude more stack
traces per category. Finally, we carry out an evaluation that strives to be
realistic: measuring not only the accuracy of categorization, but also the
operation time and the ability to create new categories. The evaluation shows
that our model strikes a good balance - it outperforms other models on both
open-source datasets and SlowOps, while also being faster on time than most. We
release all of our code and data, and hope that our work can pave the way to
further practice-oriented research in the area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at SANER'25. 11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending TWIG: Zero-Shot Predictive Hyperparameter Selection for KGEs
  based on Graph Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Sardina, John D. Kelleher, Declan O'Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) have seen increasing use across various domains --
from biomedicine and linguistics to general knowledge modelling. In order to
facilitate the analysis of knowledge graphs, Knowledge Graph Embeddings (KGEs)
have been developed to automatically analyse KGs and predict new facts based on
the information in a KG, a task called "link prediction". Many existing studies
have documented that the structure of a KG, KGE model components, and KGE
hyperparameters can significantly change how well KGEs perform and what
relationships they are able to learn. Recently, the Topologically-Weighted
Intelligence Generation (TWIG) model has been proposed as a solution to
modelling how each of these elements relate. In this work, we extend the
previous research on TWIG and evaluate its ability to simulate the output of
the KGE model ComplEx in the cross-KG setting. Our results are twofold. First,
TWIG is able to summarise KGE performance on a wide range of hyperparameter
settings and KGs being learned, suggesting that it represents a general
knowledge of how to predict KGE performance from KG structure. Second, we show
that TWIG can successfully predict hyperparameter performance on unseen KGs in
the zero-shot setting. This second observation leads us to propose that, with
additional research, optimal hyperparameter selection for KGE models could be
determined in a pre-hoc manner using TWIG-like methods, rather than by using a
full hyperparameter search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-Temporal Credit Assignment for Optimal Policy Preservation in
  Sparse Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-agent environments, agents often struggle to learn optimal policies
due to sparse or delayed global rewards, particularly in long-horizon tasks
where it is challenging to evaluate actions at intermediate time steps. We
introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach
designed to address the agent-temporal credit assignment problem by
redistributing sparse rewards both temporally and across agents. TAR$^2$
decomposes sparse global rewards into time-step-specific rewards and calculates
agent-specific contributions to these rewards. We theoretically prove that
TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the
optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$
stabilizes and accelerates the learning process. Additionally, we show that
when TAR$^2$ is integrated with single-agent reinforcement learning algorithms,
it performs as well as or better than traditional multi-agent reinforcement
learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in
  Palestine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabee Qasem, Mohannad Hendi, Banan Tantour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable potential in
diverse domains, yet their application in the legal sector, particularly in
low-resource contexts, remains limited. This study addresses the challenges of
adapting LLMs to the Palestinian legal domain, where political instability,
fragmented legal frameworks, and limited AI resources hinder effective
machine-learning applications. We present a fine-tuned model based on a
quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set
derived from Palestinian legal texts. Using smaller-scale models and
strategically generated question-answer pairs, we achieve a cost-effective,
locally sustainable solution that provides accurate and contextually relevant
legal guidance. Our experiments demonstrate promising performance on various
query types, ranging from yes/no questions and narrative explanations to
complex legal differentiations, while highlighting areas for improvement, such
as handling calculation-based inquiries and structured list formatting. This
work provides a pathway for the deployment of AI-driven legal assistance tools
tailored to the needs of resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opportunities and limitations of explaining quantum machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elies Gil-Fuster, Jonas R. Naujoks, Grégoire Montavon, Thomas Wiegand, Wojciech Samek, Jens Eisert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common trait of many machine learning models is that it is often difficult
to understand and explain what caused the model to produce the given output.
While the explainability of neural networks has been an active field of
research in the last years, comparably little is known for quantum machine
learning models. Despite a few recent works analyzing some specific aspects of
explainability, as of now there is no clear big picture perspective as to what
can be expected from quantum learning models in terms of explainability. In
this work, we address this issue by identifying promising research avenues in
this direction and lining out the expected future results. We additionally
propose two explanation methods designed specifically for quantum machine
learning models, as first of their kind to the best of our knowledge. Next to
our pre-view of the field, we compare both existing and novel methods to
explain the predictions of quantum learning models. By studying explainability
in quantum machine learning, we can contribute to the sustainable development
of the field, preventing trust issues in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16+16 pages, 3+4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Based Recalibration of SDSS and DESI BAO Alleviates Hubble
  and Clustering Tensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Shah, Purba Mukherjee, Soumadeep Saha, Utpal Garain, Supratik Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional calibration of Baryon Acoustic Oscillations (BAO) data relies on
estimation of the sound horizon at drag epoch $r_d$ from early universe
observations by assuming a cosmological model. We present a recalibration of
two independent BAO datasets, SDSS and DESI, by employing deep learning
techniques for model-independent estimation of $r_d$, and explore the impacts
on $\Lambda$CDM cosmological parameters. Significant reductions in both Hubble
($H_0$) and clustering ($S_8$) tensions are observed for both the recalibrated
datasets. Moderate shifts in some other parameters hint towards further
exploration of such data-driven approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 2 tables. Comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A parametric algorithm is optimal for non-parametric regression of
  smooth functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Maran, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the regression problem for a general function $f:[-1,1]^d\to
\mathbb R$ when the learner selects the training points $\{x_i\}_{i=1}^n$ to
achieve a uniform error bound across the entire domain. In this setting, known
historically as nonparametric regression, we aim to establish a sample
complexity bound that depends solely on the function's degree of smoothness.
Assuming periodicity at the domain boundaries, we introduce PADUA, an algorithm
that, with high probability, provides performance guarantees optimal up to
constant or logarithmic factors across all problem parameters. Notably, PADUA
is the first parametric algorithm with optimal sample complexity for this
setting. Due to this feature, we prove that, differently from the
non-parametric state of the art, PADUA enjoys optimal space complexity in the
prediction phase. To validate these results, we perform numerical experiments
over functions coming from real audio data, where PADUA shows comparable
performance to state-of-the-art methods, while requiring only a fraction of the
computational time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Inference and Human--Computer Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roderick Murray-Smith, John H. Williamson, Sebastian Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Inference is a closed-loop computational theoretical basis for
understanding behaviour, based on agents with internal probabilistic generative
models that encode their beliefs about how hidden states in their environment
cause their sensations. We review Active Inference and how it could be applied
to model the human-computer interaction loop. Active Inference provides a
coherent framework for managing generative models of humans, their
environments, sensors and interface components. It informs off-line design and
supports real-time, online adaptation. It provides model-based explanations for
behaviours observed in HCI, and new tools to measure important concepts such as
agency and engagement. We discuss how Active Inference offers a new basis for a
theory of interaction in HCI, tools for design of modern, complex sensor-based
systems, and integration of artificial intelligence technologies, enabling it
to cope with diversity in human users and contexts. We discuss the practical
challenges in implementing such Active Inference-based systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Use of Deep Learning Models for Semantic Clone Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subroto Nag Pinku, Debajyoti Mondal, Chanchal K. Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and tracking code clones can ease various software development and
maintenance tasks when changes in a code fragment should be propagated over all
its copies. Several deep learning-based clone detection models have appeared in
the literature for detecting syntactic and semantic clones, widely evaluated
with the BigCloneBench dataset. However, class imbalance and the small number
of semantic clones make BigCloneBench less ideal for interpreting model
performance. Researchers also use other datasets such as GoogleCodeJam,
OJClone, and SemanticCloneBench to understand model generalizability. To
overcome the limitations of existing datasets, the GPT-assisted semantic and
cross-language clone dataset GPTCloneBench has been released. However, how
these models compare across datasets remains unclear. In this paper, we propose
a multi-step evaluation approach for five state-of-the-art clone detection
models leveraging existing benchmark datasets, including GPTCloneBench, and
using mutation operators to study model ability. Specifically, we examine three
highly-performing single-language models (ASTNN, GMN, CodeBERT) on
BigCloneBench, SemanticCloneBench, and GPTCloneBench, testing their robustness
with mutation operations. Additionally, we compare them against cross-language
models (C4, CLCDSA) known for detecting semantic clones. While single-language
models show high F1 scores for BigCloneBench, their performance on
SemanticCloneBench varies (up to 20%). Interestingly, the cross-language model
(C4) shows superior performance (around 7%) on SemanticCloneBench over other
models and performs similarly on BigCloneBench and GPTCloneBench. On
mutation-based datasets, C4 has more robust performance (less than 1%
difference) compared to single-language models, which show high variability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 40th IEEE International Conference on Software
  Maintenance and Evolution (ICSME 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting GNN Performance via Training Sample Selection Based on
  Adversarial Robustness Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have established themselves as one of the most
powerful neural network architectures, excelling in leveraging graph topology
and node features for various tasks. However, GNNs are inherently vulnerable to
noise in their inputs. Such noise can significantly degrade their performance.
To address this challenge, we propose a novel approach that employs adversarial
robustness evaluation techniques to identify nodes in the graph that are most
susceptible to noise. By selecting and constructing a training set composed of
these particularly noise-prone nodes, we then use them to train a Graph
Convolutional Network (GCN). Our experimental results demonstrate that this
strategy leads to substantial improvements in the GCN's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI for Banks: Benchmarks and Algorithms for Synthetic
  Financial Transaction Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Sven Karst, Sook-Yee Chong, Abigail A. Antenor, Enyu Lin, Mahei Manhai Li, Jan Marco Leimeister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The banking sector faces challenges in using deep learning due to data
sensitivity and regulatory constraints, but generative AI may offer a solution.
Thus, this study identifies effective algorithms for generating synthetic
financial transaction data and evaluates five leading models - Conditional
Tabular Generative Adversarial Networks (CTGAN), DoppelGANger (DGAN),
Wasserstein GAN, Financial Diffusion (FinDiff), and Tabular Variational
AutoEncoders (TVAE) - across five criteria: fidelity, synthesis quality,
efficiency, privacy, and graph structure. While none of the algorithms is able
to replicate the real data's graph structure, each excels in specific areas:
DGAN is ideal for privacy-sensitive tasks, FinDiff and TVAE excel in data
replication and augmentation, and CTGAN achieves a balance across all five
criteria, making it suitable for general applications with moderate privacy
concerns. As a result, our findings offer valuable insights for choosing the
most suitable algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 34th Workshop on Information Technologies and
  Systems (WITS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FROC: Building Fair ROC from a Trained Classifier <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avyukta Manjunatha Vummintala, Shantanu Das, Sujit Gujar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of fair probabilistic binary classification
with binary protected groups. The classifier assigns scores, and a practitioner
predicts labels using a certain cut-off threshold based on the desired
trade-off between false positives vs. false negatives. It derives these
thresholds from the ROC of the classifier. The resultant classifier may be
unfair to one of the two protected groups in the dataset. It is desirable that
no matter what threshold the practitioner uses, the classifier should be fair
to both the protected groups; that is, the $\mathcal{L}_p$ norm between FPRs
and TPRs of both the protected groups should be at most $\varepsilon$. We call
such fairness on ROCs of both the protected attributes
$\varepsilon_p$-Equalized ROC. Given a classifier not satisfying
$\varepsilon_1$-Equalized ROC, we aim to design a post-processing method to
transform the given (potentially unfair) classifier's output (score) to a
suitable randomized yet fair classifier. That is, the resultant classifier must
satisfy $\varepsilon_1$-Equalized ROC. First, we introduce a threshold query
model on the ROC curves for each protected group. The resulting classifier is
bound to face a reduction in AUC. With the proposed query model, we provide a
rigorous theoretical analysis of the minimal AUC loss to achieve
$\varepsilon_1$-Equalized ROC. To achieve this, we design a linear time
algorithm, namely \texttt{FROC}, to transform a given classifier's output to a
probabilistic classifier that satisfies $\varepsilon_1$-Equalized ROC. We prove
that under certain theoretical conditions, \texttt{FROC}\ achieves the
theoretical optimal guarantees. We also study the performance of our
\texttt{FROC}\ on multiple real-world datasets with many trained classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, The 39th Annual AAAI Conference on Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Li, Dan Guo, Guoliang Chen, Chunxiao Fan, Jingyuan Xu, Zhiliang Wu, Hehe Fan, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro-Action Recognition (MAR) has gained increasing attention due to its
crucial role as a form of non-verbal communication in social interactions, with
promising potential for applications in human communication and emotion
analysis. However, current approaches often overlook the inherent ambiguity in
micro-actions, which arises from the wide category range and subtle visual
differences between categories. This oversight hampers the accuracy of
micro-action recognition. In this paper, we propose a novel Prototypical
Calibrating Ambiguous Network (\textbf{PCAN}) to unleash and mitigate the
ambiguity of MAR. \textbf{Firstly}, we employ a hierarchical action-tree to
identify the ambiguous sample, categorizing them into distinct sets of
ambiguous samples of false negatives and false positives, considering both
body- and action-level categories. \textbf{Secondly}, we implement an ambiguous
contrastive refinement module to calibrate these ambiguous samples by
regulating the distance between ambiguous samples and their corresponding
prototypes. This calibration process aims to pull false negative
($\mathbb{FN}$) samples closer to their respective prototypes and push false
positive ($\mathbb{FP}$) samples apart from their affiliated prototypes. In
addition, we propose a new prototypical diversity amplification loss to
strengthen the model's capacity by amplifying the differences between different
prototypes. \textbf{Finally}, we propose a prototype-guided rectification to
rectify prediction by incorporating the representability of prototypes.
Extensive experiments conducted on the benchmark dataset demonstrate the
superior performance of our method compared to existing approaches. The code is
available at https://github.com/kunli-cs/PCAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical
  Forecasting Reconciliation and Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengchao Yang, Mithun Ghosh, Anish Saha, Dong Xu, Konstantin Shmakov, Kuang-chih Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ads demand forecasting for Walmart's ad products plays a critical role in
enabling effective resource planning, allocation, and management of ads
performance. In this paper, we introduce a comprehensive demand forecasting
system that tackles hierarchical time series forecasting in business settings.
Though traditional hierarchical reconciliation methods ensure forecasting
coherence, they often trade off accuracy for coherence especially at lower
levels and fail to capture the seasonality unique to each time-series in the
hierarchy. Thus, we propose a novel framework "Multi-Stage Hierarchical
Forecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)" to address
the challenges of preserving seasonality, ensuring coherence, and improving
accuracy. Our system first utilizes diverse models, ensembled through Bayesian
Optimization (BO), achieving base forecasts. The generated base forecasts are
then passed into the Multi-Stage HiFoReAd framework. The initial stage refines
the hierarchy using Top-Down forecasts and "harmonic alignment." The second
stage aligns the higher levels' forecasts using MinTrace algorithm, following
which the last two levels undergo "harmonic alignment" and "stratified
scaling", to eventually achieve accurate and coherent forecasts across the
whole hierarchy. Our experiments on Walmart's internal Ads-demand dataset and 3
other public datasets, each with 4 hierarchical levels, demonstrate that the
average Absolute Percentage Error from the cross-validation sets improve from
3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet)
as well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the
forecasts at all hierarchical levels are proved to be coherent. The proposed
framework has been deployed and leveraged by Walmart's ads, sales and
operations teams to track future demands, make informed decisions and plan
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2024 IEEE International Conference on Big Data (BigData)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing Gram Matrix for SMILES Strings using RDKFingerprint and
  Sinkhorn-Knopp Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarwan Ali, Haris Mansoor, Prakash Chourasia, Imdad Ullah Khan, Murray Patterson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In molecular structure data, SMILES (Simplified Molecular Input Line Entry
System) strings are used to analyze molecular structure design. Numerical
feature representation of SMILES strings is a challenging task. This work
proposes a kernel-based approach for encoding and analyzing molecular
structures from SMILES strings. The proposed approach involves computing a
kernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal
component analysis (PCA) for dimensionality reduction. The resulting
low-dimensional embeddings are then used for classification and regression
analysis. The kernel matrix is computed by converting the SMILES strings into
molecular structures using the Morgan Fingerprint, which computes a fingerprint
for each molecule. The distance matrix is computed using the pairwise kernels
function. The Sinkhorn-Knopp algorithm is used to compute the final kernel
matrix that satisfies the constraints of a probability distribution. This is
achieved by iteratively adjusting the kernel matrix until the marginal
distributions of the rows and columns match the desired marginal distributions.
We provided a comprehensive empirical analysis of the proposed kernel method to
evaluate its goodness with greater depth. The suggested method is assessed for
drug subcategory prediction (classification task) and solubility AlogPS
``Aqueous solubility and Octanol/Water partition coefficient" (regression task)
using the benchmark SMILES string dataset. The outcomes show the proposed
method outperforms several baseline methods in terms of supervised analysis and
has potential uses in molecular design and drug discovery. Overall, the
suggested method is a promising avenue for kernel methods-based molecular
structure analysis and design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistic Adversarially Robust Pruning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zhao, Christian Wressnegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks can be drastically shrunk in size by removing redundant
parameters. While crucial for the deployment on resource-constraint hardware,
oftentimes, compression comes with a severe drop in accuracy and lack of
adversarial robustness. Despite recent advances, counteracting both aspects has
only succeeded for moderate compression rates so far. We propose a novel
method, HARP, that copes with aggressive pruning significantly better than
prior work. For this, we consider the network holistically. We learn a global
compression strategy that optimizes how many parameters (compression rate) and
which parameters (scoring connections) to prune specific to each layer
individually. Our method fine-tunes an existing model with dynamic
regularization, that follows a step-wise incremental function balancing the
different objectives. It starts by favoring robustness before shifting focus on
reaching the target compression rate and only then handles the objectives
equally. The learned compression strategies allow us to maintain the
pre-trained model natural accuracy and its adversarial robustness for a
reduction by 99% of the network original size. Moreover, we observe a crucial
influence of non-uniform compression across layers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziteng Wang, Jianfei Chen, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to
scale up model capacity without increasing the computation budget. However,
vanilla TopK routers are trained in a discontinuous, non-differentiable way,
limiting their performance and scalability. To address this issue, we propose
ReMoE, a fully differentiable MoE architecture that offers a simple yet
effective drop-in replacement for the conventional TopK+Softmax routing,
utilizing ReLU as the router instead. We further propose methods to regulate
the router's sparsity while balancing the load among experts. ReMoE's
continuous nature enables efficient dynamic allocation of computation across
tokens and layers, while also exhibiting domain specialization. Our experiments
demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across
various model sizes, expert counts, and levels of granularity. Furthermore,
ReMoE exhibits superior scalability with respect to the number of experts,
surpassing traditional MoE architectures. The implementation based on
Megatron-LM is available at https://github.com/thu-ml/ReMoE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming the Memory Beast: Strategies for Reliable ML Training on
  Kubernetes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaideep Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kubernetes offers a powerful orchestration platform for machine learning
training, but memory management can be challenging due to specialized needs and
resource constraints. This paper outlines how Kubernetes handles memory
requests, limits, Quality of Service classes, and eviction policies for ML
workloads, with special focus on GPU memory and ephemeral storage. Common
pitfalls such as overcommitment, memory leaks, and ephemeral volume exhaustion
are examined. We then provide best practices for stable, scalable memory
utilization to help ML practitioners prevent out-of-memory events and ensure
high-performance ML training pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lorentzian Residual Neural Networks <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil He, Menglin Yang, Rex Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperbolic neural networks have emerged as a powerful tool for modeling
hierarchical data structures prevalent in real-world datasets. Notably,
residual connections, which facilitate the direct flow of information across
layers, have been instrumental in the success of deep neural networks. However,
current methods for constructing hyperbolic residual networks suffer from
limitations such as increased model complexity, numerical instability, and
errors due to multiple mappings to and from the tangent space. To address these
limitations, we introduce LResNet, a novel Lorentzian residual neural network
based on the weighted Lorentzian centroid in the Lorentz model of hyperbolic
geometry. Our method enables the efficient integration of residual connections
in Lorentz hyperbolic neural networks while preserving their hierarchical
representation capabilities. We demonstrate that our method can theoretically
derive previous methods while offering improved stability, efficiency, and
effectiveness. Extensive experiments on both graph and vision tasks showcase
the superior performance and robustness of our method compared to
state-of-the-art Euclidean and hyperbolic alternatives. Our findings highlight
the potential of \method for building more expressive neural networks in
hyperbolic embedding space as a generally applicable method to multiple
architectures, including CNNs, GNNs, and graph Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Synthesize Text Data without Model Collapse? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model collapse in synthetic data indicates that iterative training on
self-generated data leads to a gradual decline in performance. With the
proliferation of AI models, synthetic data will fundamentally reshape the web
data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend
of synthetic and human-produced data. In this paper, we focus on two questions:
what is the impact of synthetic data on language model training, and how to
synthesize data without model collapse? We first pre-train language models
across different proportions of synthetic data, revealing a negative
correlation between the proportion of synthetic data and model performance. We
further conduct statistical analysis on synthetic data to uncover
distributional shift phenomenon and over-concentration of n-gram features.
Inspired by the above findings, we propose token editing on human-produced data
to obtain semi-synthetic data. As a proof of concept, we theoretically
demonstrate that token-level editing can prevent model collapse, as the test
error is constrained by a finite upper bound. We conduct extensive experiments
on pre-training from scratch, continual pre-training, and supervised
fine-tuning. The results validate our theoretical proof that token-level
editing improves data quality and enhances model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoLaFL: Low-Latency Federated Learning via Forward-only Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Zhang, Jianhao Huang, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has emerged as a widely adopted paradigm for enabling
edge learning with distributed data while ensuring data privacy. However, the
traditional FL with deep neural networks trained via backpropagation can hardly
meet the low-latency learning requirements in the sixth generation (6G) mobile
networks. This challenge mainly arises from the high-dimensional model
parameters to be transmitted and the numerous rounds of communication required
for convergence due to the inherent randomness of the training process. To
address this issue, we adopt the state-of-the-art principle of maximal coding
rate reduction to learn linear discriminative features and extend the resultant
white-box neural network into FL, yielding the novel framework of Low-Latency
Federated Learning (LoLaFL) via forward-only propagation. LoLaFL enables
layer-wise transmissions and aggregation with significantly fewer communication
rounds, thereby considerably reducing latency. Additionally, we propose two
\emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on
the proof that the optimal NN parameter aggregation in LoLaFL should be
harmonic-mean-like. The second scheme further exploits the low-rank structures
of the features and transmits the low-rank-approximated covariance matrices of
features to achieve additional latency reduction. Theoretic analysis and
experiments are conducted to evaluate the performance of LoLaFL. In comparison
with traditional FL, the two nonlinear aggregation schemes for LoLaFL can
achieve reductions in latency of over 91\% and 98\%, respectively, while
maintaining comparable accuracies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Uncertainty: A Deep Dive into Calibration and Performance of
  Multimodal Large Language Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) combine visual and textual data for
tasks such as image captioning and visual question answering. Proper
uncertainty calibration is crucial, yet challenging, for reliable use in areas
like healthcare and autonomous driving. This paper investigates representative
MLLMs, focusing on their calibration across various scenarios, including before
and after visual fine-tuning, as well as before and after multimodal training
of the base LLMs. We observed miscalibration in their performance, and at the
same time, no significant differences in calibration across these scenarios. We
also highlight how uncertainty differs between text and images and how their
integration affects overall uncertainty. To better understand MLLMs'
miscalibration and their ability to self-assess uncertainty, we construct the
IDK (I don't know) dataset, which is key to evaluating how they handle
unknowns. Our findings reveal that MLLMs tend to give answers rather than admit
uncertainty, but this self-assessment improves with proper prompt adjustments.
Finally, to calibrate MLLMs and enhance model reliability, we propose
techniques such as temperature scaling and iterative prompt optimization. Our
results provide insights into improving MLLMs for effective and responsible
deployment in multimodal applications. Code and IDK dataset:
\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trainable Adaptive Activation Function Structure (TAAFS) Enhances Neural
  Network Force Field Performance with Only Dozens of Additional Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enji Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  At the heart of neural network force fields (NNFFs) is the architecture of
neural networks, where the capacity to model complex interactions is typically
enhanced through widening or deepening multilayer perceptrons (MLPs) or by
increasing layers of graph neural networks (GNNs). These enhancements, while
improving the model's performance, often come at the cost of a substantial
increase in the number of parameters. By applying the Trainable Adaptive
Activation Function Structure (TAAFS), we introduce a method that selects
distinct mathematical formulations for non-linear activations, thereby
increasing the precision of NNFFs with an insignificant addition to the
parameter count. In this study, we integrate TAAFS into a variety of neural
network models, resulting in observed accuracy improvements, and further
validate these enhancements through molecular dynamics (MD) simulations using
DeepMD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Permutation recovery of spikes in noisy high-dimensional tensor
  estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gérard Ben Arous, CĆedric Gerbelot, Vanessa Piccolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the dynamics of gradient flow in high dimensions for the
multi-spiked tensor problem, where the goal is to estimate $r$ unknown signal
vectors (spikes) from noisy Gaussian tensor observations. Specifically, we
analyze the maximum likelihood estimation procedure, which involves optimizing
a highly nonconvex random function. We determine the sample complexity required
for gradient flow to efficiently recover all spikes, without imposing any
assumptions on the separation of the signal-to-noise ratios (SNRs). More
precisely, our results provide the sample complexity required to guarantee
recovery of the spikes up to a permutation. Our work builds on our companion
paper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and
determines the sample complexity and separation conditions for the SNRs
necessary for ensuring exact recovery of the spikes (where the recovered
permutation matches the identity). During the recovery process, the
correlations between the estimators and the hidden vectors increase in a
sequential manner. The order in which these correlations become significant
depends on their initial values and the corresponding SNRs, which ultimately
determines the permutation of the recovered spikes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2408.06401</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Prompt</span> Tuning: Vision Guided <span class="highlight-title">Prompt</span> Tuning with Cross-Attention
  for Fine-Grained Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot, fine-grained classification in computer vision poses significant
challenges due to the need to differentiate subtle class distinctions with
limited data. This paper presents a novel method that enhances the Contrastive
Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided
by real-time visual inputs. Unlike existing techniques such as Context
Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by
static prompts or visual token reliance, the proposed approach leverages a
cross-attention mechanism to dynamically refine text prompts for the image at
hand. This enables an image-specific alignment of textual features with image
patches extracted from the Vision Transformer, making the model more effective
for datasets with high intra-class variance and low inter-class differences.
The method is evaluated on several datasets, including CUBirds, Oxford Flowers,
and FGVC Aircraft, showing significant performance gains over static prompt
tuning approaches. To ensure these performance gains translate into trustworthy
predictions, we integrate Monte-Carlo Dropout in our approach to improve the
reliability of the model predictions and uncertainty estimates. This
integration provides valuable insights into the model's predictive confidence,
helping to identify when predictions can be trusted and when additional
verification is necessary. This dynamic approach offers a robust solution,
advancing the state-of-the-art for few-shot fine-grained classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Li, You-wei Wen, Xu Xiao, Mingchao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Principal Component Analysis (RPCA) is a fundamental technique for
decomposing data into low-rank and sparse components, which plays a critical
role for applications such as image processing and anomaly detection.
Traditional RPCA methods commonly use $\ell_1$ norm regularization to enforce
sparsity, but this approach can introduce bias and result in suboptimal
estimates, particularly in the presence of significant noise or outliers.
Non-convex regularization methods have been proposed to mitigate these
challenges, but they tend to be complex to optimize and sensitive to initial
conditions, leading to potential instability in solutions. To overcome these
challenges, in this paper, we propose a novel RPCA model that integrates
adaptive weighted least squares (AWLS) and low-rank matrix factorization
(LRMF). The model employs a {self-attention-inspired} mechanism in its weight
update process, allowing the weight matrix to dynamically adjust and emphasize
significant components during each iteration. By employing a weighted F-norm
for the sparse component, our method effectively reduces bias while simplifying
the computational process compared to traditional $\ell_1$-norm-based methods.
We use an alternating minimization algorithm, where each subproblem has an
explicit solution, thereby improving computational efficiency. Despite its
simplicity, numerical experiments demonstrate that our method outperforms
existing non-convex regularization approaches, offering superior performance
and stability, as well as enhanced accuracy and robustness in practical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DM) have democratized AI image generation through an
iterative denoising process. Quantization is a major technique to alleviate the
inference cost and reduce the size of DM denoiser networks. However, as
denoisers evolve from variants of convolutional U-Nets toward newer Transformer
architectures, it is of growing importance to understand the quantization
sensitivity of different weight layers, operations and architecture types to
performance. In this work, we address this challenge with Qua$^2$SeDiMo, a
mixed-precision Post-Training Quantization framework that generates explainable
insights on the cost-effectiveness of various model weight quantization methods
for different denoiser operation types and block structures. We leverage these
insights to make high-quality mixed-precision quantization decisions for a
myriad of diffusion models ranging from foundational U-Nets to state-of-the-art
Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,
3.65-bit and 3.7-bit weight quantization on PixArt-${\alpha}$,
PixArt-${\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our
weight-quantization configurations with 6-bit activation quantization and
outperform existing approaches in terms of quantitative metrics and generative
image quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025; version includes supplementary material; 22 Pages, 18
  Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous latent representations for modeling precipitation with deep
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Radhakrishnan, Rahul Sundar, Nishant Parashar, Antoine Blanchard, Daiwei Wang, Boyko Dodov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sparse and spatio-temporally discontinuous nature of precipitation data
presents significant challenges for simulation and statistical processing for
bias correction and downscaling. These include incorrect representation of
intermittency and extreme values (critical for hydrology applications), Gibbs
phenomenon upon regridding, and lack of fine scales details. To address these
challenges, a common approach is to transform the precipitation variable
nonlinearly into one that is more malleable. In this work, we explore how deep
learning can be used to generate a smooth, spatio-temporally continuous
variable as a proxy for simulation of precipitation data. We develop a normally
distributed field called pseudo-precipitation (PP) as an alternative for
simulating precipitation. The practical applicability of this variable is
investigated by applying it for downscaling precipitation from \(1\degree\)
(\(\sim\) 100 km) to \(0.25\degree\) (\(\sim\) 25 km).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pitfalls of topology-aware image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Berger, Laurin Lux, Alexander Weers, Martin Menten, Daniel Rueckert, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological correctness, i.e., the preservation of structural integrity and
specific characteristics of shape, is a fundamental requirement for medical
imaging tasks, such as neuron or vessel segmentation. Despite the recent surge
in topology-aware methods addressing this challenge, their real-world
applicability is hindered by flawed benchmarking practices. In this paper, we
identify critical pitfalls in model evaluation that include inadequate
connectivity choices, overlooked topological artifacts in ground truth
annotations, and inappropriate use of evaluation metrics. Through detailed
empirical analysis, we uncover these issues' profound impact on the evaluation
and ranking of segmentation methods. Drawing from our findings, we propose a
set of actionable recommendations to establish fair and robust evaluation
standards for topology-aware medical image segmentation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/AlexanderHBerger/topo-pitfalls</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Scalable and Deep Graph Neural Networks via Noise Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Liang, Wentao Zhang, Zeang Sheng, Ling Yang, Quanqing Xu, Jiawei Jiang, Yunhai Tong, Bin Cu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Graph Neural Networks (GNNs) have achieved remarkable
success in many graph mining tasks. However, scaling them to large graphs is
challenging due to the high computational and storage costs of repeated feature
propagation and non-linear transformation during training. One commonly
employed approach to address this challenge is model-simplification, which only
executes the Propagation (P) once in the pre-processing, and Combine (C) these
receptive fields in different ways and then feed them into a simple model for
better performance. Despite their high predictive performance and scalability,
these methods still face two limitations. First, existing approaches mainly
focus on exploring different C methods from the model perspective, neglecting
the crucial problem of performance degradation with increasing P depth from the
data-centric perspective, known as the over-smoothing problem. Second,
pre-processing overhead takes up most of the end-to-end processing time,
especially for large-scale graphs. To address these limitations, we present
random walk with noise masking (RMask), a plug-and-play module compatible with
the existing model-simplification works. This module enables the exploration of
deeper GNNs while preserving their scalability. Unlike the previous
model-simplification works, we focus on continuous P and found that the noise
existing inside each P is the cause of the over-smoothing issue, and use the
efficient masking mechanism to eliminate them. Experimental results on six
real-world datasets demonstrate that model-simplification works equipped with
RMask yield superior performance compared to their original version and can
make a good trade-off between accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast inverse lithography based on a model-driven block stacking
  convolutional neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Chen, Yang Zhao, Haoqin Li, Rui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of lithography, Optical Proximity Correction (OPC) is a crucial
resolution enhancement technique that optimizes the transmission function of
photomasks on a pixel-based to effectively counter Optical Proximity Effects
(OPE). However, conventional pixel-based OPC methods often generate patterns
that pose manufacturing challenges, thereby leading to the increased cost in
practical scenarios. This paper presents a novel inverse lithographic approach
to OPC, employing a model-driven, block stacking deep learning framework that
expedites the generation of masks conducive to manufacturing. This method is
founded on vector lithography modelling and streamlines the training process by
eliminating the requirement for extensive labeled datasets. Furthermore,
diversity of mask patterns is enhanced by employing a wave function collapse
algorithm, which facilitates the random generation of a multitude of target
patterns, therefore significantly expanding the range of mask paradigm.
Numerical experiments have substantiated the efficacy of the proposed
end-to-end approach, highlighting its superior capability to manage mask
complexity within the context of advanced OPC lithography. This advancement is
anticipated to enhance the feasibility and economic viability of OPC technology
within actual manufacturing environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDP: Generalizing to Multilingual Visual Information Extraction by
  Language Decoupled <span class="highlight-title">Pretrain</span>ing <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Information Extraction (VIE) plays a crucial role in the comprehension
of semi-structured documents, and several pre-trained models have been
developed to enhance performance. However, most of these works are monolingual
(usually English). Due to the extremely unbalanced quantity and quality of
pre-training corpora between English and other languages, few works can extend
to non-English scenarios. In this paper, we conduct systematic experiments to
show that vision and layout modality hold invariance among images with
different languages. If decoupling language bias from document images, a
vision-layout-based model can achieve impressive cross-lingual generalization.
Accordingly, we present a simple but effective multilingual training paradigm
LDP (Language Decoupled Pre-training) for better utilization of monolingual
pre-training data. Our proposed model LDM (Language Decoupled Model) is first
pre-trained on the language-independent data, where the language knowledge is
decoupled by a diffusion model, and then the LDM is fine-tuned on the
downstream languages. Extensive experiments show that the LDM outperformed all
SOTA multilingual pre-trained models, and also maintains competitiveness on
downstream monolingual/English benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,
  and Internal Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Li, Bozhong Zheng, Xiaohao Xu, Jinye Gan, Fading Lu, Xiang Li, Na Ni, Zheng Tian, Xiaonan Huang, Shenghua Gao, Yingna Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object anomaly detection is essential for industrial quality inspection, yet
traditional single-sensor methods face critical limitations. They fail to
capture the wide range of anomaly types, as single sensors are often
constrained to either external appearance, geometric structure, or internal
properties. To overcome these challenges, we introduce MulSen-AD, the first
high-resolution, multi-sensor anomaly detection dataset tailored for industrial
applications. MulSen-AD unifies data from RGB cameras, laser scanners, and
lock-in infrared thermography, effectively capturing external appearance,
geometric deformations, and internal defects. The dataset spans 15 industrial
products with diverse, real-world anomalies. We also present MulSen-AD Bench, a
benchmark designed to evaluate multi-sensor methods, and propose
MulSen-TripleAD, a decision-level fusion algorithm that integrates these three
modalities for robust, unsupervised object anomaly detection. Our experiments
demonstrate that multi-sensor fusion substantially outperforms single-sensor
approaches, achieving 96.1% AUROC in object-level detection accuracy. These
results highlight the importance of integrating multi-sensor data for
comprehensive industrial anomaly detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixLLM: LLM Quantization with Global Mixed-precision between
  Output-features and Highly-efficient System Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zheng, Xiaonan Song, Chuanjie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization has become one of the most effective methodologies to compress
LLMs into smaller size. However, the existing quantization solutions still show
limitations of either non-negligible accuracy drop or system inefficiency. In
this paper, we make a comprehensive analysis of the general quantization
principles on their effect to the triangle of accuracy, memory consumption and
system efficiency. We propose MixLLM that explores the new optimization space
of mixed-precision quantization between output features based on the insight
that different output features matter differently in the model. MixLLM
identifies the output features with high salience in the global view rather
than within each single layer, effectively assigning the larger bit-width to
output features that need it most to achieve good accuracy with low memory
consumption. We present the sweet spot of quantization configuration of
algorithm-system co-design that leads to high accuracy and system efficiency.
To address the system challenge, we design the two-step dequantization to make
use of the int8 Tensor Core easily and fast data type conversion to reduce
dequantization overhead significantly, and present the software pipeline to
overlap the memory access, dequantization and the MatMul to the best. Extensive
experiments show that with only 10% more bits, the PPL increasement can be
reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on
average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In
addition to its superior accuracy, MixLLM also achieves state-of-the-art system
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code will be released in the future</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerated Patient-Specific Calibration via Differentiable Hemodynamics
  Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Renner, Georgios Kissas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the goals of personalized medicine is to tailor diagnostics to
individual patients. Diagnostics are performed in practice by measuring
quantities, called biomarkers, that indicate the existence and progress of a
disease. In common cardiovascular diseases, such as hypertension, biomarkers
that are closely related to the clinical representation of a patient can be
predicted using computational models. Personalizing computational models
translates to considering patient-specific flow conditions, for example, the
compliance of blood vessels that cannot be a priori known and quantities such
as the patient geometry that can be measured using imaging. Therefore, a
patient is identified by a set of measurable and nonmeasurable parameters
needed to well-define a computational model; else, the computational model is
not personalized, meaning it is prone to large prediction errors. Therefore, to
personalize a computational model, sufficient information needs to be extracted
from the data. The current methods by which this is done are either
inefficient, due to relying on slow-converging optimization methods, or hard to
interpret, due to using `black box` deep-learning algorithms. We propose a
personalized diagnostic procedure based on a differentiable 0D-1D Navier-Stokes
reduced order model solver and fast parameter inference methods that take
advantage of gradients through the solver. By providing a faster method for
performing parameter inference and sensitivity analysis through
differentiability while maintaining the interpretability of well-understood
mathematical models and numerical methods, the best of both worlds is combined.
The performance of the proposed solver is validated against a well-established
process on different geometries, and different parameter inference processes
are successfully performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Spatio-Temporal Fusion-based Traffic Prediction Algorithm with
  Anomaly Aware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqun Liu, Xuanpeng Li, Chen Gong, Guangyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic prediction is an indispensable component of urban planning and
traffic management. Achieving accurate traffic prediction hinges on the ability
to capture the potential spatio-temporal relationships among road sensors.
However, the majority of existing works focus on local short-term
spatio-temporal correlations, failing to fully consider the interactions of
different sensors in the long-term state. In addition, these works do not
analyze the influences of anomalous factors, or have insufficient ability to
extract personalized features of anomalous factors, which make them
ineffectively capture their spatio-temporal influences on traffic prediction.
To address the aforementioned issues, We propose a global spatio-temporal
fusion-based traffic prediction algorithm that incorporates anomaly awareness.
Initially, based on the designed anomaly detection network, we construct an
efficient anomalous factors impacting module (AFIM), to evaluate the
spatio-temporal impact of unexpected external events on traffic prediction.
Furthermore, we propose a multi-scale spatio-temporal feature fusion module
(MTSFFL) based on the transformer architecture, to obtain all possible both
long and short term correlations among different sensors in a wide-area traffic
environment for accurate prediction of traffic flow. Finally, experiments are
implemented based on real-scenario public transportation datasets (PEMS04 and
PEMS08) to demonstrate that our approach can achieve state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIArena: A Blockchain-Based Decentralized AI Training Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI has underscored critical challenges in its
development and implementation, largely due to centralized control by a few
major corporations. This concentration of power intensifies biases within AI
models, resulting from inadequate governance and oversight mechanisms.
Additionally, it limits public involvement and heightens concerns about the
integrity of model generation. Such monopolistic control over data and AI
outputs threatens both innovation and fair data usage, as users inadvertently
contribute data that primarily benefits these corporations. In this work, we
propose AIArena, a blockchain-based decentralized AI training platform designed
to democratize AI development and alignment through on-chain incentive
mechanisms. AIArena fosters an open and collaborative environment where
participants can contribute models and computing resources. Its on-chain
consensus mechanism ensures fair rewards for participants based on their
contributions. We instantiate and implement AIArena on the public Base
blockchain Sepolia testnet, and the evaluation results demonstrate the
feasibility of AIArena in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GBRIP: Granular Ball Representation for Imbalanced Partial Label
  Learning <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial label learning (PLL) is a complicated weakly supervised
multi-classification task compounded by class imbalance. Currently, existing
methods only rely on inter-class pseudo-labeling from inter-class features,
often overlooking the significant impact of the intra-class imbalanced features
combined with the inter-class. To address these limitations, we introduce
Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for
imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and
multi-center loss to construct a granular ball-based nfeature space through
unsupervised learning, effectively capturing the feature distribution within
each class. GBRIP mitigates the impact of confusing features by systematically
refining label disambiguation and estimating imbalance distributions. The novel
multi-center loss function enhances learning by emphasizing the relationships
between samples and their respective centers within the granular balls.
Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms
existing state-of-the-art methods, offering a robust solution to the challenges
of imbalanced PLL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Watermarking for AI-Generated Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tramer, Somesh Jha, Lei Li, Yu-Xiang Wang, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the outputs of generative AI (GenAI) techniques improve in quality, it
becomes increasingly challenging to distinguish them from human-created
content. Watermarking schemes are a promising approach to address the problem
of distinguishing between AI and human-generated content. These schemes embed
hidden signals within AI-generated content to enable reliable detection. While
watermarking is not a silver bullet for addressing all risks associated with
GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by
combating misinformation and deception. This paper presents a comprehensive
overview of watermarking techniques for GenAI, beginning with the need for
watermarking from historical and regulatory perspectives. We formalize the
definitions and desired properties of watermarking schemes and examine the key
objectives and threat models for existing approaches. Practical evaluation
strategies are also explored, providing insights into the development of robust
watermarking techniques capable of resisting various attacks. Additionally, we
review recent representative works, highlight open challenges, and discuss
potential directions for this emerging field. By offering a thorough
understanding of watermarking in GenAI, this work aims to guide researchers in
advancing watermarking methods and applications, and support policymakers in
addressing the broader implications of GenAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by
  Structured Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current PEFT methods for LLMs can achieve either high quality, efficient
training, or scalable serving, but not all three simultaneously. To address
this limitation, we investigate sparse fine-tuning and observe a remarkable
improvement in generalization ability. Utilizing this key insight, we propose a
family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which
concurrently achieve state-of-the-art fine-tuning performance, training
efficiency, and inference scalability. S$^{2}$FT accomplishes this by
"selecting sparsely and computing densely". It selects a few heads and channels
in the MHA and FFN modules for each Transformer block, respectively. Next, it
co-permutes weight matrices on both sides of the coupled structures in LLMs to
connect the selected components in each layer into a dense submatrix. Finally,
S$^{2}$FT performs in-place gradient updates on all submatrices. Through
theoretical analysis and empirical results, our method prevents forgetting
while simplifying optimization, delivers SOTA performance on both commonsense
and arithmetic reasoning with 4.6% and 1.3% average improvements compared to
LoRA, and surpasses full FT by 11.5% when generalizing to various domains after
instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT
saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$
compared to full FT, while delivering an average 10% improvement over LoRA on
both metrics. We further demonstrate that the weight updates in S$^{2}$FT can
be decoupled into adapters, enabling effective fusion, fast switch, and
efficient parallelism for serving multiple fine-tuned models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological
  and Multilingual Knowledge Base <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Doğruöz, En-Shiun Annie Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  URIEL is a knowledge base offering geographical, phylogenetic, and
typological vector representations for 7970 languages. It includes distance
measures between these vectors for 4005 languages, which are accessible via the
lang2vec tool. Despite being frequently cited, URIEL is limited in terms of
linguistic inclusion and overall usability. To tackle these challenges, we
introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses
these limitations. In addition to expanding typological feature coverage for
2898 languages, URIEL+ improves the user experience with robust, customizable
distance calculations to better suit the needs of users. These upgrades also
offer competitive performance on downstream tasks and provide distances that
better align with linguistic distance studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04619v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04619v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Qin, Naomi Saphra, David Alvarez-Melis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs), like other neural networks, often favor shortcut
heuristics based on surface-level patterns. Although LMs behave like n-gram
models early in training, they must eventually learn hierarchical syntactic
representations to correctly apply grammatical rules out-of-distribution (OOD).
In this work, we use case studies of English grammar to explore how complex,
diverse training data drives models to generalize OOD. We construct a framework
that unifies our understanding of random variation with training dynamics, rule
selection with memorization, and data diversity with complexity. We show that
these factors are nuanced, and that intermediate levels of diversity and
complexity lead to inconsistent behavior across random seeds and to unstable
training dynamics. Our findings emphasize the critical role of training data in
shaping generalization patterns and illuminate how competing model strategies
lead to inconsistent generalization outcomes across random seeds. Code is
available at https://github.com/sunnytqin/concept_comp.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Ewald summation for machine learning of long-range interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingqing Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning interatomic potentials (MLIPs) often neglect long-range
interactions, such as electrostatic and dispersion forces. In this work, we
introduce a straightforward and efficient method to account for long-range
interactions by learning a latent variable from local atomic descriptors and
applying an Ewald summation to this variable. We demonstrate that in systems
including charged and polar molecular dimers, bulk water, and water-vapor
interface, standard short-ranged MLIPs can lead to unphysical predictions even
when employing message passing. The long-range models effectively eliminate
these artifacts, with only about twice the computational cost of short-range
MLIPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Machine Unlearning with Dimensional Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonguk Seo, Dongwan Kim, Bohyung Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning, an emerging research topic focusing on compliance with
data privacy regulations, enables trained models to remove the information
learned from specific data. While many existing methods indirectly address this
issue by intentionally injecting incorrect supervisions, they can drastically
and unpredictably alter the decision boundaries and feature spaces, leading to
training instability and undesired side effects. To fundamentally approach this
task, we first analyze the changes in latent feature spaces between original
and retrained models, and observe that the feature representations of samples
not involved in training are closely aligned with the feature manifolds of
previously seen samples in training. Based on these findings, we introduce a
novel evaluation metric for machine unlearning, coined dimensional alignment,
which measures the alignment between the eigenspaces of the forget and retain
set samples. We employ this metric as a regularizer loss to build a robust and
stable unlearning framework, which is further enhanced by integrating a
self-distillation loss and an alternating training scheme. Our framework
effectively eliminates information from the forget set and preserves knowledge
from the retain set. Lastly, we identify critical flaws in established
evaluation metrics for machine unlearning, and introduce new evaluation tools
that more accurately reflect the fundamental goals of machine unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Compatible Training for Online Backfilling in Large-Scale
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backfilling is the process of re-extracting all gallery embeddings from
upgraded models in image retrieval systems. It inevitably requires a
prohibitively large amount of computational cost and even entails the downtime
of the service. Although backward-compatible learning sidesteps this challenge
by tackling query-side representations, this leads to suboptimal solutions in
principle because gallery embeddings cannot benefit from model upgrades. We
address this dilemma by introducing an online backfilling algorithm, which
enables us to achieve a progressive performance improvement during the
backfilling process while not sacrificing the final performance of new model
after the completion of backfilling. To this end, we first propose a simple
distance rank merge technique for online backfilling. Then, we incorporate a
reverse transformation module for more effective and efficient merging, which
is further enhanced by adopting a metric-compatible contrastive learning
approach. These two components help to make the distances of old and new models
compatible, resulting in desirable merge results during backfilling with no
extra computational overhead. Extensive experiments show the effectiveness of
our framework on four standard benchmarks in various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Variable Sequence Identification for Cognitive Models with Neural
  Network Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ti-Fen Pan, Jing-Jing Li, Bill Thompson, Anne Collins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting time-varying latent variables from computational cognitive models
is a key step in model-based neural analysis, which aims to understand the
neural correlates of cognitive processes. However, existing methods only allow
researchers to infer latent variables that explain subjects' behavior in a
relatively small class of cognitive models. For example, a broad class of
relevant cognitive models with analytically intractable likelihood is currently
out of reach from standard techniques, based on Maximum a Posteriori parameter
estimation. Here, we present an approach that extends neural Bayes estimation
to learn a direct mapping between experimental data and the targeted latent
variable space using recurrent neural networks and simulated datasets. We show
that our approach achieves competitive performance in inferring latent variable
sequences in both tractable and intractable models. Furthermore, the approach
is generalizable across different computational models and is adaptable for
both continuous and discrete latent spaces. We then demonstrate its
applicability in real world datasets. Our work underscores that combining
recurrent neural networks and simulation-based inference to identify latent
variable sequences can enable researchers to access a wider class of cognitive
models for model-based neural analyses, and thus test a broader set of
theories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with
  LLM Token Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot graph machine learning, especially with graph neural networks
(GNNs), has garnered significant interest due to the challenge of scarce
labeled data. While methods like self-supervised learning and graph prompt
learning have been extensively explored, they often rely on fine-tuning with
task-specific labels, limiting their effectiveness in zero-shot scenarios.
Inspired by the zero-shot capabilities of instruction-fine-tuned large language
models (LLMs), we introduce a novel framework named Token Embedding-Aligned
Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and
cross-task zero-shot learners for graph machine learning. Concretely, we
pretrain a GNN, aligning its representations with token embeddings of an LLM.
We then train a linear projector that transforms the GNN's representations into
a fixed number of graph token embeddings without tuning the LLM. A unified
instruction is designed for various graph tasks at different levels, such as
node classification (node-level) and link prediction (edge-level). These design
choices collectively enhance our method's effectiveness in zero-shot learning,
setting it apart from existing methods. Experiments show that our graph token
embeddings help the LLM predictor achieve state-of-the-art performance on
unseen datasets and tasks compared to other methods using LLMs as predictors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Linear Algebra: A Graph Neural Network Approach to
  Preconditioner Design for Conjugate Gradient Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Trifonov, Alexander Rudikov, Oleg Iliev, Yuri M. Laevsky, Ivan Oseledets, Ekaterina Muravleva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large linear systems are ubiquitous in modern computational science and
engineering. The main recipe for solving them is the use of Krylov subspace
iterative methods with well-designed preconditioners. Deep learning models can
be used as nonlinear preconditioners during the iteration of linear solvers
such as the conjugate gradient (CG) method. Neural network models require an
enormous number of parameters to approximate well in this setup. Another
approach is to take advantage of small graph neural networks (GNNs) to
construct preconditioners with predefined sparsity patterns. Recently, GNNs
have been shown to be a promising tool for designing preconditioners to reduce
the overall computational cost of iterative methods by constructing them more
efficiently than with classical linear algebra techniques. However,
preconditioners designed with these approaches cannot outperform those designed
with classical methods in terms of the number of iterations in CG. In our work,
we recall well-established preconditioners from linear algebra and use them as
a starting point for training the GNN to obtain preconditioners that reduce the
condition number of the system more significantly. Numerical experiments show
that our approach outperforms both classical and neural network-based methods
for an important class of parametric partial differential equations. We also
provide a heuristic justification for the loss function used and show that
preconditioners obtained by learning with this loss function reduce the
condition number in a more desirable way for CG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for
  Lazy Clients <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12012v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12012v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed collaborative machine learning paradigm
that has gained strong momentum in recent years. In federated learning, a
central server periodically coordinates models with clients and aggregates the
models trained locally by clients without necessitating access to local data.
Despite its potential, the implementation of federated learning continues to
encounter several challenges, predominantly the slow convergence that is
largely due to data heterogeneity. The slow convergence becomes particularly
problematic in cross-device federated learning scenarios where clients may be
strongly limited by computing power and storage space, and hence counteracting
methods that induce additional computation or memory cost on the client side
such as auxiliary objective terms and larger training iterations can be
impractical. In this paper, we propose a novel federated aggregation strategy,
TurboSVM-FL, that poses no additional computation burden on the client side and
can significantly accelerate convergence for federated classification task,
especially when clients are "lazy" and train their models solely for few epochs
for next global aggregation. TurboSVM-FL extensively utilizes support vector
machine to conduct selective aggregation and max-margin spread-out
regularization on class embeddings. We evaluate TurboSVM-FL on multiple
datasets including FEMNIST, CelebA, and Shakespeare using user-independent
validation with non-iid data distribution. Our results show that TurboSVM-FL
can significantly outperform existing popular algorithms on convergence rate
and reduce communication rounds while delivering better test metrics including
accuracy, F1 score, and MCC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating federated learning contribution allocation instability
  through randomized aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arno Geimer, Beltran Fiz, Radu State
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a collaborative and privacy-preserving Machine
Learning paradigm, allowing the development of robust models without the need
to centralise sensitive data. A critical challenge in FL lies in fairly and
accurately allocating contributions from diverse participants. Inaccurate
allocation can undermine trust, lead to unfair compensation, and thus
participants may lack the incentive to join or actively contribute to the
federation.
  Various remuneration strategies have been proposed to date, including
auction-based approaches and Shapley-value based methods, the latter offering a
means to quantify the contribution of each participant. However, little to no
work has studied the stability of these contribution evaluation methods.
  In this paper, we focus on calculating contributions using gradient-based
model reconstruction techniques with Shapley values. We first show that
baseline Shapley values do not accurately reflect clients' contributions,
leading to unstable reward allocations amongst participants in a cross-silo
federation. We then introduce \textsc{FedRandom}, a new method that mitigates
these shortcomings with additional data samplings, and show its efficacy at
increasing the stability of contribution evaluation in federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arbitrary Polynomial Separations in Trainable Quantum Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric R. Anschuetz, Xun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent theoretical results in quantum machine learning have demonstrated a
general trade-off between the expressive power of quantum neural networks
(QNNs) and their trainability; as a corollary of these results, practical
exponential separations in expressive power over classical machine learning
models are believed to be infeasible as such QNNs take a time to train that is
exponential in the model size. We here circumvent these negative results by
constructing a hierarchy of efficiently trainable QNNs that exhibit
unconditionally provable, polynomial memory separations of arbitrary constant
degree over classical neural networks -- including state-of-the-art models,
such as Transformers -- in performing a classical sequence modeling task. This
construction is also computationally efficient, as each unit cell of the
introduced class of QNNs only has constant gate complexity. We show that
contextuality -- informally, a quantitative notion of semantic ambiguity -- is
the source of the expressivity separation, suggesting that other learning tasks
with this property may be a natural setting for the use of quantum learning
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 3 figures, strengthened and simplified results and
  presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Gradient Clipping for Noisy Label Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08941v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08941v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xichen Ye, Yifan Wu, Weizhong Zhang, Xiaoqiang Li, Yifan Chen, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has shown that constraining the gradient of loss function
with respect to model-predicted probabilities can enhance the model robustness
against noisy labels. These methods typically specify a fixed optimal threshold
for gradient clipping through validation data to obtain the desired robustness
against noise. However, this common practice overlooks the dynamic distribution
of gradients from both clean and noisy-labeled samples at different stages of
training, significantly limiting the model capability to adapt to the variable
nature of gradients throughout the training process. To address this issue, we
propose a simple yet effective approach called Optimized Gradient Clipping
(OGC), which dynamically adjusts the clipping threshold based on the ratio of
noise gradients to clean gradients after clipping, estimated by modeling the
distributions of clean and noisy samples. This approach allows us to modify the
clipping threshold at each training step, effectively controlling the influence
of noise gradients. Additionally, we provide statistical analysis to certify
the noise-tolerance ability of OGC. Our extensive experiments across various
types of label noise, including symmetric, asymmetric, instance-dependent, and
real-world noise, demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clustering of timed sequences -- Application to the analysis of care
  pathways 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15379v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15379v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Guyet, Pierre Pinson, Enoal Gesny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the future of healthcare starts by better understanding the current
actual practices in hospital settings. This motivates the objective of
discovering typical care pathways from patient data. Revealing typical care
pathways can be achieved through clustering. The difficulty in clustering care
pathways, represented by sequences of timestamped events, lies in defining a
semantically appropriate metric and clustering algorithms. In this article, we
adapt two methods developed for time series to the clustering of timed
sequences: the drop-DTW metric and the DBA approach for the construction of
averaged time sequences. These methods are then applied in clustering
algorithms to propose original and sound clustering algorithms for timed
sequences. This approach is experimented with and evaluated on synthetic and
real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Adaptation of Reinforcement Learning-based NAS Agents through
  Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amber Cassimon, Siegfried Mercelis, Kevin Mets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a novel paradigm has been proposed for reinforcement learning-based
NAS agents, that revolves around the incremental improvement of a given
architecture. We assess the abilities of such reinforcement learning agents to
transfer between different tasks. We perform our evaluation using the
Trans-NASBench-101 benchmark, and consider the efficacy of the transferred
agents, as well as how quickly they can be trained. We find that pretraining an
agent on one task benefits the performance of the agent in another task in all
but 1 task when considering final performance. We also show that the training
procedure for an agent can be shortened significantly by pretraining it on
another task. Our results indicate that these effects occur regardless of the
source or target task, although they are more pronounced for some tasks than
for others. Our results show that transfer learning can be an effective tool in
mitigating the computational cost of the initial training procedure for
reinforcement learning-based NAS agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages, 13 Figures, Corrected data in Figure 5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Samudra: An AI Global Ocean Emulator for Climate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surya Dheeshjith, Adam Subel, Alistair Adcroft, Julius Busecke, Carlos Fernandez-Granda, Shubham Gupta, Laure Zanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI emulators for forecasting have emerged as powerful tools that can
outperform conventional numerical predictions. The next frontier is to build
emulators for long climate simulations with skill across a range of
spatiotemporal scales, a particularly important goal for the ocean. Our work
builds a skillful global emulator of the ocean component of a state-of-the-art
climate model. We emulate key ocean variables, sea surface height, horizontal
velocities, temperature, and salinity, across their full depth. We use a
modified ConvNeXt UNet architecture trained on multidepth levels of ocean data.
We show that the ocean emulator - Samudra - which exhibits no drift relative to
the truth, can reproduce the depth structure of ocean variables and their
interannual variability. Samudra is stable for centuries and 150 times faster
than the original ocean model. Samudra struggles to capture the correct
magnitude of the forcing trends and simultaneously remains stable, requiring
further work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Ethereum Fraud Detection via Generative and Contrastive
  Self-supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxiang Jin, Jiajun Zhou, Chenxuan Xie, Shanqing Yu, Qi Xuan, Xiaoniu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rampant fraudulent activities on Ethereum hinder the healthy development
of the blockchain ecosystem, necessitating the reinforcement of regulations.
However, multiple imbalances involving account interaction frequencies and
interaction types in the Ethereum transaction environment pose significant
challenges to data mining-based fraud detection research. To address this, we
first propose the concept of meta-interactions to refine interaction behaviors
in Ethereum, and based on this, we present a dual self-supervision enhanced
Ethereum fraud detection framework, named Meta-IFD. This framework initially
introduces a generative self-supervision mechanism to augment the interaction
features of accounts, followed by a contrastive self-supervision mechanism to
differentiate various behavior patterns, and ultimately characterizes the
behavioral representations of accounts and mines potential fraud risks through
multi-view interaction feature learning. Extensive experiments on real Ethereum
datasets demonstrate the effectiveness and superiority of our framework in
detecting common Ethereum fraud behaviors such as Ponzi schemes and phishing
scams. Additionally, the generative module can effectively alleviate the
interaction distribution imbalance in Ethereum data, while the contrastive
module significantly enhances the framework's ability to distinguish different
behavior patterns. The source code will be available in
https://github.com/GISec-Team/Meta-IFD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Information Forensics & Security</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SageAttention2: Efficient Attention with Thorough Outlier Smoothing and
  Per-thread INT4 Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although quantization for linear layers has been widely used, its application
to accelerate the attention process remains limited. To further enhance the
efficiency of attention computation compared to SageAttention while maintaining
precision, we propose SageAttention2, which utilizes significantly faster 4-bit
matrix multiplication (Matmul) alongside additional precision-enhancing
techniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a
hardware-friendly thread-level granularity and quantize matrixes $(\widetilde
P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the
accuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$
to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS)
of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on
RTX4090, respectively. Comprehensive experiments confirm that our approach
incurs negligible end-to-end metrics loss across diverse models, including
those for large language processing, image generation, and video generation.
The codes are available at https://github.com/thu-ml/SageAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Large Language Models for Math Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kathrin Seßler, Yao Rong, Emek Gözlüklü, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of Large Language Models (LLMs) in mathematical reasoning has become
a cornerstone of related research, demonstrating the intelligence of these
models and enabling potential practical applications through their advanced
performance, such as in educational settings. Despite the variety of datasets
and in-context learning algorithms designed to improve the ability of LLMs to
automate mathematical problem solving, the lack of comprehensive benchmarking
across different datasets makes it complicated to select an appropriate model
for specific tasks. In this project, we present a benchmark that fairly
compares seven state-of-the-art in-context learning algorithms for mathematical
problem solving across five widely used mathematical datasets on four powerful
foundation models. Furthermore, we explore the trade-off between efficiency and
performance, highlighting the practical applications of LLMs for mathematical
reasoning. Our results indicate that larger foundation models like GPT-4o and
LLaMA 3-70B can solve mathematical reasoning independently from the concrete
prompting strategy, while for smaller models the in-context learning approach
significantly influences the performance. Moreover, the optimal prompt depends
on the chosen foundation model. We open-source our benchmark code to support
the integration of additional models in future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Jalonen, Mohammad Al-Sa'd, Roope Mellanen, Serkan Kiranyaz, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The health and safety hazards posed by worn crane lifting ropes mandate
periodic inspection for damage. This task is time-consuming, prone to human
error, halts operation, and may result in the premature disposal of ropes.
Therefore, we propose using efficient deep learning and computer vision methods
to automate the process of detecting damaged ropes. Specifically, we present a
vision-based system for detecting damage in synthetic fiber rope images using
lightweight convolutional neural networks. We develop a camera-based apparatus
to photograph the lifting rope's surface, while in operation, and capture the
progressive wear-and-tear as well as the more significant degradation in the
rope's health state. Experts from Konecranes annotate the collected images in
accordance with the rope's condition; normal or damaged. Then, we pre-process
the images, systematically design a deep learning model, evaluate its detection
and prediction performance, analyze its computational complexity, and compare
it with various other models. Experimental results show the proposed model
outperforms other similar techniques with 96.5% accuracy, 94.8% precision,
98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the
model's real-time operation, low memory footprint, robustness to various
environmental and operational conditions, and adequacy for deployment in
industrial applications such as lifting, mooring, towing, climbing, and
sailing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Imitation Learning in Single-Agent Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation Learning (IL) is one of the most widely used methods in machine
learning. Yet, many works find it is often unable to fully recover the
underlying expert behavior, even in constrained environments like single-agent
games. However, none of these works deeply investigate the role of scaling up
the model and data size. Inspired by recent work in Natural Language Processing
(NLP) where "scaling up" has resulted in increasingly more capable LLMs, we
investigate whether carefully scaling up model and data size can bring similar
improvements in the imitation learning setting for single-agent games. We first
demonstrate our findings on a variety of Atari games, and thereafter focus on
the extremely challenging game of NetHack. In all games, we find that IL loss
and mean return scale smoothly with the compute budget (FLOPs) and are strongly
correlated, resulting in power laws for training compute-optimal IL agents.
Finally, we forecast and train several NetHack agents with IL and find they
outperform prior state-of-the-art by 1.5x in all settings. Our work both
demonstrates the scaling behavior of imitation learning in a variety of
single-agent games, as well as the viability of scaling up current approaches
for increasingly capable agents in NetHack, a game that remains elusively hard
for current AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TMLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Union-over-Intersections: Object Detection beyond Winner-Takes-All 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Bhowmik, Pascal Mettes, Martin R. Oswald, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the problem of predicting box locations in object
detection architectures. Typically, each box proposal or box query aims to
directly maximize the intersection-over-union score with the ground truth,
followed by a winner-takes-all non-maximum suppression where only the highest
scoring box in each region is retained. We observe that both steps are
sub-optimal: the first involves regressing proposals to the entire ground
truth, which is a difficult task even with large receptive fields, and the
second neglects valuable information from boxes other than the top candidate.
Instead of regressing proposals to the whole ground truth, we propose a simpler
approach: regress only to the area of intersection between the proposal and the
ground truth. This avoids the need for proposals to extrapolate beyond their
visual scope, improving localization accuracy. Rather than adopting a
winner-takes-all strategy, we take the union over the regressed intersections
of all boxes in a region to generate the final box outputs. Our plug-and-play
method integrates seamlessly into proposal-based, grid-based, and query-based
detection architectures with minimal modifications, consistently improving
object localization and instance segmentation. We demonstrate its broad
applicability and versatility across various detection and segmentation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online MDP with Transition Prototypes: A Robust Adaptive Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Sun, Meng Qi, Zuo-Jun Max Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider an online robust Markov Decision Process (MDP)
where we have the information of finitely many prototypes of the underlying
transition kernel. We consider an adaptively updated ambiguity set of the
prototypes and propose an algorithm that efficiently identifies the true
underlying transition kernel while guaranteeing the performance of the
corresponding robust policy. To be more specific, we provide a sublinear regret
of the subsequent optimal robust policy. We also provide an early stopping
mechanism and a worst-case performance bound of the value function. In
numerical experiments, we demonstrate that our method outperforms existing
approaches, particularly in the early stage with limited data. This work
contributes to robust MDPs by considering possible prior information about the
underlying transition probability and online learning, offering both
theoretical insights and practical algorithms for improved decision-making
under uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Parameters Reveal More than Loss for Membership Inference? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11544v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11544v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshuman Suri, Xiao Zhang, David Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference attacks are used as a key tool for disclosure auditing.
They aim to infer whether an individual record was used to train a model. While
such evaluations are useful to demonstrate risk, they are computationally
expensive and often make strong assumptions about potential adversaries' access
to models and training environments, and thus do not provide tight bounds on
leakage from potential attacks. We show how prior claims around black-box
access being sufficient for optimal membership inference do not hold for
stochastic gradient descent, and that optimal membership inference indeed
requires white-box access. Our theoretical results lead to a new white-box
inference attack, IHA (Inverse Hessian Attack), that explicitly uses model
parameters by taking advantage of computing inverse-Hessian vector products.
Our results show that both auditors and adversaries may be able to benefit from
access to model parameters, and we advocate for further research into white-box
methods for membership inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybridization of Persistent Homology with Neural Networks for
  Time-Series Prediction: A Case Study in Wave Height 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01519v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01519v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixin Lin, Nur Fariha Syaqina Zulkepli, Mohd Shareduwan Mohd Kasihmuddin, R. U. Gobithaasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-series prediction is an active area of research across various fields,
often challenged by the fluctuating influence of short-term and long-term
factors. In this study, we introduce a feature engineering method that enhances
the predictive performance of neural network models. Specifically, we leverage
computational topology techniques to derive valuable topological features from
input data, boosting the predictive accuracy of our models. Our focus is on
predicting wave heights, utilizing models based on topological features within
feedforward neural networks (FNNs), recurrent neural networks (RNNs), long
short-term memory networks (LSTM), and RNNs with gated recurrent units (GRU).
For time-ahead predictions, the enhancements in $R^2$ score were significant
for FNNs, RNNs, LSTM, and GRU models. Additionally, these models also showed
significant reductions in maximum errors and mean squared errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the paper contain errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14573v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14573v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, Oriana Riva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents that execute human tasks by controlling computers can
enhance human productivity and application accessibility. However, progress in
this field will be driven by realistic and reproducible benchmarks. We present
AndroidWorld, a fully functional Android environment that provides reward
signals for 116 programmatic tasks across 20 real-world Android apps. Unlike
existing interactive environments, which provide a static test set,
AndroidWorld dynamically constructs tasks that are parameterized and expressed
in natural language in unlimited ways, thus enabling testing on a much larger
and more realistic suite of tasks. To ensure reproducibility, each task
includes dedicated initialization, success-checking, and tear-down logic, which
modifies and inspects the device's system state. We experiment with baseline
agents to test AndroidWorld and provide initial results on the benchmark. Our
best agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for
future work. Furthermore, we adapt a popular desktop web agent to work on
Android, which we find to be less effective on mobile, suggesting future
research is needed to achieve universal, cross-platform agents. Finally, we
also conduct a robustness analysis, showing that task variations can
significantly affect agent performance, demonstrating that without such
testing, agent performance metrics may not fully reflect practical challenges.
AndroidWorld and the experiments in this paper are available at
github.com/google-research/android_world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Erase then Rectify: A Training-Free Parameter Editing Approach for
  Cost-Effective Graph Unlearning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph unlearning, which aims to eliminate the influence of specific nodes,
edges, or attributes from a trained Graph Neural Network (GNN), is essential in
applications where privacy, bias, or data obsolescence is a concern. However,
existing graph unlearning techniques often necessitate additional training on
the remaining data, leading to significant computational costs, particularly
with large-scale graphs. To address these challenges, we propose a two-stage
training-free approach, Erase then Rectify (ETR), designed for efficient and
scalable graph unlearning while preserving the model utility. Specifically, we
first build a theoretical foundation showing that masking parameters critical
for unlearned samples enables effective unlearning. Building on this insight,
the Erase stage strategically edits model parameters to eliminate the impact of
unlearned samples and their propagated influence on intercorrelated nodes. To
further ensure the GNN's utility, the Rectify stage devises a gradient
approximation method to estimate the model's gradient on the remaining dataset,
which is then used to enhance model performance. Overall, ETR achieves graph
unlearning without additional training or full training data access,
significantly reducing computational overhead and preserving data privacy.
Extensive experiments on seven public datasets demonstrate the consistent
superiority of ETR in model utility, unlearning efficiency, and unlearning
effectiveness, establishing it as a promising solution for real-world graph
unlearning challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASTM :Autonomous Smart Traffic Management System Using Artificial
  Intelligence CNN and LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10929v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10929v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christofel Rio Goenawan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the modern world, the development of Artificial Intelligence (AI) has
contributed to improvements in various areas, including automation, computer
vision, fraud detection, and more. AI can be leveraged to enhance the
efficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce
traffic congestion rates. This paper presents an Autonomous Smart Traffic
Management (STM) system that uses AI to improve traffic flow rates. The system
employs the YOLO V5 Convolutional Neural Network to detect vehicles in traffic
management images. Additionally, it predicts the number of vehicles for the
next 12 hours using a Recurrent Neural Network with Long Short-Term Memory
(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the
traffic cycle length based on these vehicle predictions, aided by AI. From the
results of the RNN-LSTM model for predicting vehicle numbers over the next 12
hours, we observe that the model predicts traffic with a Mean Squared Error
(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.
After simulating the STM system in the CARLA simulation environment, we found
that the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per
minute) is 50\% higher than the rate without STM (around 15 vehicles per
minute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5
seconds per vehicle) is 70\% lower than without STM (around 12 seconds per
vehicle). These results demonstrate that the STM system using AI can increase
traffic flow by 50\% and reduce vehicle pass delays by 70\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In process to IEEE Intelligent Vehicle Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sum of Squares Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Loconte, Stefan Mengel, Antonio Vergari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing expressive generative models that support exact and efficient
inference is a core question in probabilistic ML. Probabilistic circuits (PCs)
offer a framework where this tractability-vs-expressiveness trade-off can be
analyzed theoretically. Recently, squared PCs encoding subtractive mixtures via
negative parameters have emerged as tractable models that can be exponentially
more expressive than monotonic PCs, i.e., PCs with positive parameters only. In
this paper, we provide a more precise theoretical characterization of the
expressiveness relationships among these models. First, we prove that squared
PCs can be less expressive than monotonic ones. Second, we formalize a novel
class of PCs -- sum of squares PCs -- that can be exponentially more expressive
than both squared and monotonic PCs. Around sum of squares PCs, we build an
expressiveness hierarchy that allows us to precisely unify and separate
different tractable model classes such as Born Machines and PSD models, and
other recently introduced tractable probabilistic models by using complex
parameters. Finally, we empirically show the effectiveness of sum of squares
circuits in performing distribution estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Re-enable PDE Loss for Physical Systems Modeling Under Partial
  Observation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09116v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09116v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Feng, Yue Wang, Dixia Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In science and engineering, machine learning techniques are increasingly
successful in physical systems modeling (predicting future states of physical
systems). Effectively integrating PDE loss as a constraint of system transition
can improve the model's prediction by overcoming generalization issues due to
data scarcity, especially when data acquisition is costly. However, in many
real-world scenarios, due to sensor limitations, the data we can obtain is
often only partial observation, making the calculation of PDE loss seem to be
infeasible, as the PDE loss heavily relies on high-resolution states. We
carefully study this problem and propose a novel framework named Re-enable PDE
Loss under Partial Observation (RPLPO). The key idea is that although enabling
PDE loss to constrain system transition solely is infeasible, we can re-enable
PDE loss by reconstructing the learnable high-resolution state and constraining
system transition simultaneously. Specifically, RPLPO combines an encoding
module for reconstructing learnable high-resolution states with a transition
module for predicting future states. The two modules are jointly trained by
data and PDE loss. We conduct experiments in various physical systems to
demonstrate that RPLPO has significant improvement in generalization, even when
observation is sparse, irregular, noisy, and PDE is inaccurate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Falsifying Causal Graphs Using a Permutation-Based Test <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Eulig, Atalanti A. Mastakouri, Patrick Blöbaum, Michaela Hardt, Dominik Janzing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding causal relationships among the variables of a system is
paramount to explain and control its behavior. For many real-world systems,
however, the true causal graph is not readily available and one must resort to
predictions made by algorithms or domain experts. Therefore, metrics that
quantitatively assess the goodness of a causal graph provide helpful checks
before using it in downstream tasks. Existing metrics provide an
$\textit{absolute}$ number of inconsistencies between the graph and the
observed data, and without a baseline, practitioners are left to answer the
hard question of how many such inconsistencies are acceptable or expected.
Here, we propose a novel consistency metric by constructing a baseline through
node permutations. By comparing the number of inconsistencies with those on the
baseline, we derive an interpretable metric that captures whether the graph is
significantly better than random. Evaluating on both simulated and real data
sets from various domains, including biology and cloud monitoring, we
demonstrate that the true graph is not falsified by our metric, whereas the
wrong graphs given by a hypothetical user are likely to be falsified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Continuous-Time Memory-Based Symbolic Policies using Genetic
  Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02765v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02765v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sigur de Vries, Sander Keemink, Marcel van Gerven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence techniques are increasingly being applied to solve
control problems, but often rely on black-box methods without transparent
output generation. To improve the interpretability and transparency in control
systems, models can be defined as white-box symbolic policies described by
mathematical expressions. For better performance in partially observable and
volatile environments, the symbolic policies are extended with memory
represented by continuous-time latent variables, governed by differential
equations. Genetic programming is used for optimisation, resulting in
interpretable policies consisting of symbolic expressions. Our results show
that symbolic policies with memory compare with black-box policies on a variety
of control tasks. Furthermore, the benefit of the memory in symbolic policies
is demonstrated on experiments where memory-less policies fall short. Overall,
we present a method for evolving high-performing symbolic policies that offer
interpretability and transparency, which lacks in black-box models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages including references and appendix, 5 figures, 1 algorithm, 5
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape error prediction in 5-axis machining using graph neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Huuk, Abheek Dhingra, Eirini Ntoutsi, Berend Denkena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative method for predicting shape errors in
5-axis machining using graph neural networks. The graph structure is defined
with nodes representing workpiece surface points and edges denoting the
neighboring relationships. The dataset encompasses data from a material removal
simulation, process data, and post-machining quality information. Experimental
results show that the presented approach can generalize the shape error
prediction for the investigated workpiece geometry. Moreover, by modelling
spatial and temporal connections within the workpiece, the approach handles a
low number of labels compared to non-graphical methods such as Support Vector
Machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11448v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11448v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangqiang Hu, Jianfeng Lu, Jianmin Han, Shuqin Cao, Jing Liu, Hao Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the sensitivity of data, Federated Learning (FL) is employed to enable
distributed machine learning while safeguarding data privacy and accommodating
the requirements of various devices. However, in the context of
semi-decentralized FL, clients' communication and training states are dynamic.
This variability arises from local training fluctuations, heterogeneous data
distributions, and intermittent client participation. Most existing studies
primarily focus on stable client states, neglecting the dynamic challenges
inherent in real-world scenarios. To tackle this issue, we propose a
TRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client
states and contributions, enhancing model training efficiency through selective
client participation. We focus on a semi-decentralized FL framework where edge
servers and clients train a shared global model using unreliable intra-cluster
model aggregation and inter-cluster model consensus. First, we propose an
adaptive hidden semi-Markov model to estimate clients' communication states and
contributions. Next, we address a client-server association optimization
problem to minimize global training loss. Using convergence analysis, we
propose a greedy client scheduling algorithm. Finally, our experiments
conducted on real-world datasets demonstrate that TRAIL outperforms
state-of-the-art baselines, achieving an improvement of 8.7% in test accuracy
and a reduction of 15.3% in training loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Diffusion <span class="highlight-title">Transformer</span>s with Token-wise Feature Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion transformers have shown significant effectiveness in both image and
video synthesis at the expense of huge computation costs. To address this
problem, feature caching methods have been introduced to accelerate diffusion
transformers by caching the features in previous timesteps and reusing them in
the following timesteps. However, previous caching methods ignore that
different tokens exhibit different sensitivities to feature caching, and
feature caching on some tokens may lead to 10$\times$ more destruction to the
overall generation quality compared with other tokens. In this paper, we
introduce token-wise feature caching, allowing us to adaptively select the most
suitable tokens for caching, and further enable us to apply different caching
ratios to neural layers in different types and depths. Extensive experiments on
PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image
and video generation with no requirements for training. For instance,
2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and
PixArt-$\alpha$ with almost no drop in generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In this version, we achieved a nearly lossless acceleration of 1.51
  times for ToCa on FLUX in the appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probability Distribution Learning and Its Application in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05666v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05666v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binchuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical learning framework, termed
probability distribution learning (PD learning). Departing from the traditional
statistical learning framework, PD learning focuses on learning the underlying
probability distribution, which is modeled as a random variable within the
probability simplex. In this framework, the optimization objective is the
learning error, which quantifies the posterior expected discrepancy between the
model's predicted distribution and the underlying true distribution, given
available sample data and prior knowledge. To optimize the learning error, this
paper proposes the necessary conditions for loss functions, models, and
optimization algorithms, ensuring that these conditions are met in real-world
machine learning scenarios. Based on these conditions, the non-convex
optimization mechanism corresponding to model training can be theoretically
resolved. Moreover, this paper provides model-dependent and model-independent
bounds on learning error, offering new insights into the model's fitting and
generalization capabilities. Furthermore, the paper applies the PD learning
framework to elucidate the mechanisms by which various techniques, including
random parameter initialization, over-parameterization, and dropout, influence
deep model training. Finally, the paper substantiates the key conclusions of
the proposed framework through experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Score and Distribution Matching Policy: Advanced Accelerated Visuomotor
  Policies via Matched Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09265v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09265v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bofang Jia, Pengxiang Ding, Can Cui, Mingyang Sun, Pengfang Qian, Siteng Huang, Zhaoxin Fan, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-motor policy learning has advanced with architectures like
diffusion-based policies, known for modeling complex robotic trajectories.
However, their prolonged inference times hinder high-frequency control tasks
requiring real-time feedback. While consistency distillation (CD) accelerates
inference, it introduces errors that compromise action quality. To address
these limitations, we propose the Score and Distribution Matching Policy (SDM
Policy), which transforms diffusion-based policies into single-step generators
through a two-stage optimization process: score matching ensures alignment with
true action distributions, and distribution matching minimizes KL divergence
for consistency. A dual-teacher mechanism integrates a frozen teacher for
stability and an unfrozen teacher for adversarial training, enhancing
robustness and alignment with target distributions. Evaluated on a 57-task
simulation benchmark, SDM Policy achieves a 6x inference speedup while having
state-of-the-art action quality, providing an efficient and reliable framework
for high-frequency robotic tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Expressivity of Persistent Homology in Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09826v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09826v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubén Ballester, Bastian Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persistent homology, a technique from computational topology, has recently
shown strong empirical performance in the context of graph classification.
Being able to capture long range graph properties via higher-order topological
features, such as cycles of arbitrary length, in combination with multi-scale
topological descriptors, has improved predictive performance for data sets with
prominent topological structures, such as molecules. At the same time, the
theoretical properties of persistent homology have not been formally assessed
in this context. This paper intends to bridge the gap between computational
topology and graph machine learning by providing a brief introduction to
persistent homology in the context of graphs, as well as a theoretical
discussion and empirical analysis of its expressivity for graph learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 3rd Learning on Graphs Conference (LoG) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Encouragement-Based Instrumental Variables for
  Counterfactual Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anpeng Wu, Kun Kuang, Ruoxuan Xiong, Xiangwei Chen, Zexu Sun, Fei Wu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In causal inference, encouragement designs (EDs) are widely used to analyze
causal effects, when randomized controlled trials (RCTs) are impractical or
compliance to treatment cannot be perfectly enforced. Unlike RCTs, which
directly allocate treatments, EDs randomly assign encouragement policies that
positively motivate individuals to engage in a specific treatment. These random
encouragements act as instrumental variables (IVs), facilitating the
identification of causal effects through leveraging exogenous perturbations in
discrete treatment scenarios. However, real-world applications of encouragement
designs often face challenges such as incomplete randomization, limited
experimental data, and significantly fewer encouragements compared to
treatments, hindering precise causal effect estimation. To address this, this
paper introduces novel theories and algorithms for identifying the Conditional
Average Treatment Effect (CATE) using variations in encouragement. Further, by
leveraging both observational and encouragement data, we propose a generalized
IV estimator, named Encouragement-based Counterfactual Regression (EnCounteR),
to effectively estimate the causal effects. Extensive experiments on both
synthetic and real-world datasets demonstrate the superiority of EnCounteR over
existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smoothness Really Matters: A Simple Yet Effective Approach for
  Unsupervised Graph Domain Adaptation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chen, Guo Ye, Yakun Wang, Zhao Zhang, Libang Zhang, Daxin Wang, Zhiqiang Zhang, Fuzhen Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution
shifts between domains by transferring knowledge from labeled source graphs to
given unlabeled target graphs. Existing UGDA methods primarily focus on
aligning features in the latent space learned by graph neural networks (GNNs)
across domains, often overlooking structural shifts, resulting in limited
effectiveness when addressing structurally complex transfer scenarios. Given
the sensitivity of GNNs to local structural features, even slight discrepancies
between source and target graphs could lead to significant shifts in node
embeddings, thereby reducing the effectiveness of knowledge transfer. To
address this issue, we introduce a novel approach for UGDA called Target-Domain
Structural Smoothing (TDSS). TDSS is a simple and effective method designed to
perform structural smoothing directly on the target graph, thereby mitigating
structural distribution shifts and ensuring the consistency of node
representations. Specifically, by integrating smoothing techniques with
neighborhood sampling, TDSS maintains the structural coherence of the target
graph while mitigating the risk of over-smoothing. Our theoretical analysis
shows that TDSS effectively reduces target risk by improving model smoothness.
Empirical results on three real-world datasets demonstrate that TDSS
outperforms recent state-of-the-art baselines, achieving significant
improvements across six transfer scenarios. The code is available in
https://github.com/cwei01/TDSS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, Accpected by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and
  Activation Functions <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jinyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical formulas serve as the means of communication between humans and
nature, encapsulating the operational laws governing natural phenomena. The
concise formulation of these laws is a crucial objective in scientific research
and an important challenge for artificial intelligence (AI). While traditional
artificial neural networks (MLP) excel at data fitting, they often yield
uninterpretable black box results that hinder our understanding of the
relationship between variables x and predicted values y. Moreover, the fixed
network architecture in MLP often gives rise to redundancy in both network
structure and parameters. To address these issues, we propose MetaSymNet, a
novel neural network that dynamically adjusts its structure in real-time,
allowing for both expansion and contraction. This adaptive network employs the
PANGU meta function as its activation function, which is a unique type capable
of evolving into various basic functions during training to compose
mathematical formulas tailored to specific needs. We then evolve the neural
network into a concise, interpretable mathematical expression. To evaluate
MetaSymNet's performance, we compare it with four state-of-the-art symbolic
regression algorithms across more than 10 public datasets comprising 222
formulas. Our experimental results demonstrate that our algorithm outperforms
others consistently regardless of noise presence or absence. Furthermore, we
assess MetaSymNet against MLP and SVM regarding their fitting ability and
extrapolation capability, these are two essential aspects of machine learning
algorithms. The findings reveal that our algorithm excels in both areas.
Finally, we compared MetaSymNet with MLP using iterative pruning in network
structure complexity. The results show that MetaSymNet's network structure
complexity is obviously less than MLP under the same goodness of fit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training <span class="highlight-title">Dataset</span>s Generation for Machine Learning: Application to Vision
  Based Navigation <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Hans Krüger, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Based Navigation consists in utilizing cameras as precision sensors
for GNC after extracting information from images. To enable the adoption of
machine learning for space applications, one of obstacles is the demonstration
that available training datasets are adequate to validate the algorithms. The
objective of the study is to generate datasets of images and metadata suitable
for training machine learning algorithms. Two use cases were selected and a
robust methodology was developed to validate the datasets including the ground
truth. The first use case is in-orbit rendezvous with a man-made object: a
mockup of satellite ENVISAT. The second use case is a Lunar landing scenario.
Datasets were produced from archival datasets (Chang'e 3), from the laboratory
at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software
high fidelity image simulator using Model Capture and from Generative
Adversarial Networks. The use case definition included the selection of
algorithms as benchmark: an AI-based pose estimation algorithm and a dense
optical flow algorithm were selected. Eventually it is demonstrated that
datasets produced with SurRender and selected laboratory facilities are
adequate to train machine learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, preprint of the proceedings of ESA SPAICE
  conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature selection in linear SVMs via a hard cardinality constraint: a
  scalable SDP decomposition approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Immanuel Bomze, Federico D'Onofrio, Laura Palagi, Bo Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the embedded feature selection problem in linear
Support Vector Machines (SVMs), in which a cardinality constraint is employed,
leading to an interpretable classification model. The problem is NP-hard due to
the presence of the cardinality constraint, even though the original linear SVM
amounts to a problem solvable in polynomial time. To handle the hard problem,
we first introduce two mixed-integer formulations for which novel semidefinite
relaxations are proposed. Exploiting the sparsity pattern of the relaxations,
we decompose the problems and obtain equivalent relaxations in a much smaller
cone, making the conic approaches scalable. To make the best usage of the
decomposed relaxations, we propose heuristics using the information of its
optimal solution. Moreover, an exact procedure is proposed by solving a
sequence of mixed-integer decomposed semidefinite optimization problems.
Numerical results on classical benchmarking datasets are reported, showing the
efficiency and effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Journal of Operational Research. arXiv admin
  note: text overlap with arXiv:1808.02435 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Acceleration for Classification-Based Derivative-Free
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Han, Jingya Li, Zhipeng Guo, Yuan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Derivative-free optimization algorithms play an important role in scientific
and engineering design optimization problems, especially when derivative
information is not accessible. In this paper, we study the framework of
sequential classification-based derivative-free optimization algorithms. By
introducing learning theoretic concept hypothesis-target shattering rate, we
revisit the computational complexity upper bound of SRACOS (Hu, Qian, and Yu
2017). Inspired by the revisited upper bound, we propose an algorithm named
RACE-CARS, which adds a random region-shrinking step compared with SRACOS. We
further establish theorems showing the acceleration by region shrinking.
Experiments on the synthetic functions as well as black-box tuning for
language-model-as-a-service demonstrate empirically the efficiency of
RACE-CARS. An ablation experiment on the introduced hyperparameters is also
conducted, revealing the mechanism of RACE-CARS and putting forward an
empirical hyper-parameter tuning guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding a Diffusion Model with a Bad Version of Itself <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary axes of interest in image-generating diffusion models are image
quality, the amount of variation in the results, and how well the results align
with a given condition, e.g., a class label or a text prompt. The popular
classifier-free guidance approach uses an unconditional model to guide a
conditional model, leading to simultaneously better prompt alignment and
higher-quality images at the cost of reduced variation. These effects seem
inherently entangled, and thus hard to control. We make the surprising
observation that it is possible to obtain disentangled control over image
quality without compromising the amount of variation by guiding generation
using a smaller, less-trained version of the model itself rather than an
unconditional model. This leads to significant improvements in ImageNet
generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using
publicly available networks. Furthermore, the method is also applicable to
unconditional diffusion models, drastically improving their quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanxiang Hu, Tajana Rosing, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Specializing large language models (LLMs) for local deployment in
domain-specific use cases is necessary for strong performance while meeting
latency and privacy constraints. However, conventional task-specific adaptation
approaches do not show simultaneous memory saving and inference speedup at
deployment time. Practical compression techniques like quantization and pruning
require dedicated hardware or kernel support to achieve measured inference
speedup. We develop TrimLLM based on the layer-wise specialization phenomenon
we empirically observed and verified on contemporary LLMs. TrimLLM reduces the
depth of LLMs via progressive layer dropping. We show it retains LLMs' capacity
in specific domains and achieves inference speedup irrespective of hardware and
deep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for
inference; models adapted on medical, legal, and financial datasets all
demonstrate $2.1-5.7\times$ inference speedup on consumer GPUs and up to
$3.1\times$ speedup on A100 when compared to state-of-the-art model compression
algorithms, with no loss in accuracy at 50$\sim$60\% model compression ratio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Deep Dissipative Dynamics <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuji Okamoto, Ryosuke Kojima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study challenges strictly guaranteeing ``dissipativity'' of a dynamical
system represented by neural networks learned from given time-series data.
Dissipativity is a crucial indicator for dynamical systems that generalizes
stability and input-output stability, known to be valid across various systems
including robotics, biological systems, and molecular dynamics. By analytically
proving the general solution to the nonlinear Kalman-Yakubovich-Popov (KYP)
lemma, which is the necessary and sufficient condition for dissipativity, we
propose a differentiable projection that transforms any dynamics represented by
neural networks into dissipative ones and a learning method for the transformed
dynamics. Utilizing the generality of dissipativity, our method strictly
guarantee stability, input-output stability, and energy conservation of trained
dynamical systems. Finally, we demonstrate the robustness of our method against
out-of-domain input through applications to robotic arms and fluid dynamics.
Code is https://github.com/kojima-r/DeepDissipativeModel
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks
  Defending against Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Liu, Wenshan Li, Beibei Li, Wengang Ma, Tao Li, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have revealed the vulnerability of graph neural networks
(GNNs) to adversarial poisoning attacks on node classification tasks. Current
defensive methods require substituting the original GNNs with defense models,
regardless of the original's type. This approach, while targeting adversarial
robustness, compromises the enhancements developed in prior research to boost
GNNs' practical performance. Here we introduce Grimm, the first plug-and-play
defense model. With just a minimal interface requirement for extracting
features from any layer of the protected GNNs, Grimm is thus enabled to
seamlessly rectify perturbations. Specifically, we utilize the feature
trajectories (FTs) generated by GNNs, as they evolve through epochs, to reflect
the training status of the networks. We then theoretically prove that the FTs
of victim nodes will inevitably exhibit discriminable anomalies. Consequently,
inspired by the natural parallelism between the biological nervous and immune
systems, we construct Grimm, a comprehensive artificial immune system for GNNs.
Grimm not only detects abnormal FTs and rectifies adversarial edges during
training but also operates efficiently in parallel, thereby mirroring the
concurrent functionalities of its biological counterparts. We experimentally
confirm that Grimm offers four empirically validated advantages: 1)
Harmlessness, as it does not actively interfere with GNN training; 2)
Parallelism, ensuring monitoring, detection, and rectification functions
operate independently of the GNN training process; 3) Generalizability,
demonstrating compatibility with mainstream GNNs such as GCN, GAT, and
GraphSAGE; and 4) Transferability, as the detectors for abnormal FTs can be
efficiently transferred across different systems for one-step rectification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holdouts set for safe predictive model updating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06374v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06374v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sami Haidar-Wehbe, Samuel R Emerson, Louis J M Aslett, James Liley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive risk scores for adverse outcomes are increasingly crucial in
guiding health interventions. Such scores may need to be periodically updated
due to change in the distributions they model. However, directly updating risk
scores used to guide intervention can lead to biased risk estimates. To address
this, we propose updating using a `holdout set' - a subset of the population
that does not receive interventions guided by the risk score. Balancing the
holdout set size is essential to ensure good performance of the updated risk
score whilst minimising the number of held out samples. We prove that this
approach reduces adverse outcome frequency to an asymptotically optimal level
and argue that often there is no competitive alternative. We describe
conditions under which an optimal holdout size (OHS) can be readily identified,
and introduce parametric and semi-parametric algorithms for OHS estimation. We
apply our methods to the ASPRE risk score for pre-eclampsia to recommend a plan
for updating it in the presence of change in the underlying data distribution.
We show that, in order to minimise the number of pre-eclampsia cases over time,
this is best achieved using a holdout set of around 10,000 individuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript includes supplementary materials and figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text
  Rewriting <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yang, Bardh Prenkaj, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread use of LLMs due to their superior performance in
various tasks, their high computational costs often lead potential users to opt
for the pretraining-finetuning pipeline. However, biases prevalent in manually
constructed datasets can introduce spurious correlations between tokens and
labels, creating so-called shortcuts and hindering the generalizability of
fine-tuned models. Existing debiasing methods often rely on prior knowledge of
specific dataset biases, which is challenging to acquire a priori. We propose
RAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,
and data-focused debiasing approach based on text rewriting for shortcut
mitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text
segments by replacing them with heuristically selected alternatives in a
shortcut space defined by token statistics and positional information. This
process aims to align surface-level text features more closely with diverse
label distributions, thereby promoting the learning of genuine linguistic
patterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the
FEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.
Additionally, RAZOR effectively mitigates specific known biases, reducing
bias-related terms by x2 without requiring prior bias information, a result
that is on par with SoTA models that leverage prior information. Our work
prioritizes data manipulation over architectural modifications, emphasizing the
pivotal role of data quality in enhancing model performance and fairness. This
research contributes to developing more robust evaluation benchmarks for
debiasing methods by incorporating metrics for bias reduction and overall model
efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with
  Selective State Space Models <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08160v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08160v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Yuan, Qingyun Sun, Zhaonan Wang, Xingcheng Fu, Cheng Ji, Yongjian Wang, Bo Jin, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns,
widely existing in the real world. Nevertheless, the structure incompleteness,
noise, and redundancy result in poor robustness for Dynamic Graph Neural
Networks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising
way to optimize graph structures. However, aside from encountering unacceptable
quadratic complexity, it overly relies on heuristic priors, making it hard to
discover underlying predictive patterns. How to efficiently refine the dynamic
structures, capture intrinsic dependencies, and learn robust representations,
remains under-explored. In this work, we propose the novel DG-Mamba, a robust
and efficient Dynamic Graph structure learning framework with the Selective
State Space Models (Mamba). To accelerate the spatio-temporal structure
learning, we propose a kernelized dynamic message-passing operator that reduces
the quadratic time complexity to linear. To capture global intrinsic dynamics,
we establish the dynamic graph as a self-contained system with State Space
Model. By discretizing the system states with the cross-snapshot graph
adjacency, we enable the long-distance dependencies capturing with the
selective snapshot scan. To endow learned dynamic structures more expressive
with informativeness, we propose the self-supervised Principle of Relevant
Information for DGSL to regularize the most relevant yet least redundant
information, enhancing global robustness. Extensive experiments demonstrate the
superiority of the robustness and efficiency of our DG-Mamba compared with the
state-of-the-art baselines against adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Main Technical Track of the 39th Annual AAAI
  Conference on Artificial Intelligence (AAAI-2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-JEPA: Augmentation-Free <span class="highlight-title">Self-Supervised</span> Learning for Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Thimonier, José Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervision is often used for pre-training to foster performance on a
downstream task by constructing meaningful representations of samples.
Self-supervised learning (SSL) generally involves generating different views of
the same sample and thus requires data augmentations that are challenging to
construct for tabular data. This constitutes one of the main challenges of
self-supervision for structured data. In the present work, we propose a novel
augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on
a Joint Embedding Predictive Architecture (JEPA) and is akin to mask
reconstruction in the latent space. It involves predicting the latent
representation of one subset of features from the latent representation of a
different subset within the same sample, thereby learning rich representations
without augmentations. We use our method as a pre-training technique and train
several deep classifiers on the obtained representation. Our experimental
results demonstrate a substantial improvement in both classification and
regression tasks, outperforming models trained directly on samples in their
original data space. Moreover, T-JEPA enables some methods to consistently
outperform or match the performance of traditional methods likes Gradient
Boosted Decision Trees. To understand why, we extensively characterize the
obtained representations and show that T-JEPA effectively identifies relevant
features for downstream tasks without access to the labels. Additionally, we
introduce regularization tokens, a novel regularization method critical for
training of JEPA-based models on structured data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Group Classification with Descending Soft Labeling for Deep
  Imbalanced Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhi Pu, Gezheng Xu, Ruiyi Fang, Binkun Bao, Charles X. Ling, Boyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep imbalanced regression (DIR), where the target values have a highly
skewed distribution and are also continuous, is an intriguing yet
under-explored problem in machine learning.
  While recent works have already shown that incorporating various
classification-based regularizers can produce enhanced outcomes, the role of
classification remains elusive in DIR.
  Moreover, such regularizers (e.g., contrastive penalties) merely focus on
learning discriminative features of data, which inevitably results in ignorance
of either continuity or similarity across the data.
  To address these issues, we first bridge the connection between the
objectives of DIR and classification from a Bayesian perspective.
  Consequently, this motivates us to decompose the objective of DIR into a
combination of classification and regression tasks, which naturally guides us
toward a divide-and-conquer manner to solve the DIR problem.
  Specifically, by aggregating the data at nearby labels into the same groups,
we introduce an ordinal group-aware contrastive learning loss along with a
multi-experts regressor to tackle the different groups of data thereby
maintaining the data continuity.
  Meanwhile, considering the similarity between the groups, we also propose a
symmetric descending soft labeling strategy to exploit the intrinsic similarity
across the data, which allows classification to facilitate regression more
effectively.
  Extensive experiments on real-world datasets also validate the effectiveness
of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Langevin dynamics for high-dimensional optimization: the case of
  multi-spiked tensor PCA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gérard Ben Arous, Cédric Gerbelot, Vanessa Piccolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study nonconvex optimization in high dimensions through Langevin dynamics,
focusing on the multi-spiked tensor PCA problem. This tensor estimation problem
involves recovering $r$ hidden signal vectors (spikes) from noisy Gaussian
tensor observations using maximum likelihood estimation. We study the number of
samples required for Langevin dynamics to efficiently recover the spikes and
determine the necessary separation condition on the signal-to-noise ratios
(SNRs) for exact recovery, distinguishing the cases $p \ge 3$ and $p=2$, where
$p$ denotes the order of the tensor. In particular, we show that the sample
complexity required for recovering the spike associated with the largest SNR
matches the well-known algorithmic threshold for the single-spike case, while
this threshold degrades when recovering all $r$ spikes. As a key step, we
provide a detailed characterization of the trajectory and interactions of
low-dimensional projections that capture the high-dimensional dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>65 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Every Token Counts: Optimal Segmentation for Low-Resource Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06926v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06926v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Raj S, Garvit Suri, Vikrant Dewangan, Raghav Sonavane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional greedy tokenization methods have been a critical step in Natural
Language Processing (NLP), influencing how text is converted into tokens and
directly impacting model performance. While subword tokenizers like Byte-Pair
Encoding (BPE) are widely used, questions remain about their optimality across
model scales and languages. In this work, we demonstrate through extensive
experiments that an optimal BPE configuration significantly reduces token count
compared to greedy segmentation, yielding improvements in token-saving
percentages and performance benefits, particularly for smaller models. We
evaluate tokenization performance across various intrinsic and extrinsic tasks,
including generation and classification. Our findings suggest that
compression-optimized tokenization strategies could provide substantial
advantages for multilingual and low-resource language applications,
highlighting a promising direction for further research and inclusive NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LoResLM @ COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Classification with Rotation-Invariant Variational Quantum
  Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul San Sebastian, Mikel Cañizo, Román Orús
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational quantum algorithms are gaining attention as an early application
of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of
variational methods lies in the phenomenon of Barren Plateaus, present in the
optimization of variational parameters. Adding geometric inductive bias to the
quantum models has been proposed as a potential solution to mitigate this
problem, leading to a new field called Geometric Quantum Machine Learning. In
this work, an equivariant architecture for variational quantum classifiers is
introduced to create a label-invariant model for image classification with
$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against
two different architectures, and it is experimentally observed that the
geometric approach boosts the model's performance. Finally, a classical
equivariant convolution operation is proposed to extend the quantum model for
the processing of larger images, employing the resources available in NISQ
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cherry on the Cake: Fairness is NOT an Optimization Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Favier, Toon Calders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Fair AI literature, the practice of maliciously creating unfair models
that nevertheless satisfy fairness constraints is known as "cherry-picking". A
cherry-picking model is a model that makes mistakes on purpose, selecting bad
individuals from a minority class instead of better candidates from the same
minority. The model literally cherry-picks whom to select to superficially meet
the fairness constraints while making minimal changes to the unfair model. This
practice has been described as "blatantly unfair" and has a negative impact on
already marginalized communities, undermining the intended purpose of fairness
measures specifically designed to protect these communities. A common
assumption is that cherry-picking arises solely from malicious intent and that
models designed only to optimize fairness metrics would avoid this behavior. We
show that this is not the case: models optimized to minimize fairness metrics
while maximizing performance are often forced to cherry-pick to some degree. In
other words, cherry-picking might be an inevitable outcome of the optimization
process itself. To demonstrate this, we use tools from fair cake-cutting, a
mathematical subfield that studies the problem of fairly dividing a resource,
referred to as the "cake," among a number of participants. This concept is
connected to supervised multi-label classification: any dataset can be thought
of as a cake that needs to be distributed among different labels, and the model
is the function that divides the cake. We adapt these classical results for
machine learning and demonstrate how this connection can be prolifically used
for fairness and classification in general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Methods for Full-Scale Gaussian Process Approximations for
  Large Spatial Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Gyger, Reinhard Furrer, Fabio Sigrist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian processes are flexible probabilistic regression models which are
widely used in statistics and machine learning. However, a drawback is their
limited scalability to large data sets. To alleviate this, we consider
full-scale approximations (FSAs) that combine predictive process methods and
covariance tapering, thus approximating both global and local structures. We
show how iterative methods can be used to reduce the computational costs for
calculating likelihoods, gradients, and predictive distributions with FSAs. We
introduce a novel preconditioner and show that it accelerates the conjugate
gradient method's convergence speed and mitigates its sensitivity with respect
to the FSA parameters and the eigenvalue structure of the original covariance
matrix, and we demonstrate empirically that it outperforms a state-of-the-art
pivoted Cholesky preconditioner. Further, we present a novel, accurate, and
fast way to calculate predictive variances relying on stochastic estimations
and iterative methods. In both simulated and real-world data experiments, we
find that our proposed methodology achieves the same accuracy as Cholesky-based
computations with a substantial reduction in computational time. Finally, we
also compare different approaches for determining inducing points in predictive
process and FSA models. All methods are implemented in a free C++ software
library with high-level Python and R packages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a
  Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16149v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16149v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Yang Song, Yu Zhang, Zheli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Consumer Internet of Things (CIoT), a notable segment within the IoT
domain, involves the integration of IoT technology into consumer electronics
and devices, such as smart homes and smart wearables. Compared to traditional
IoT fields, CIoT differs notably in target users, product types, and design
approaches. While offering convenience to users, it also raises new security
and privacy concerns. Network traffic analysis, a widely used technique in the
security community, has been extensively applied to investigate these concerns
about CIoT. Compared to network traffic analysis in other fields such as mobile
apps and websites, CIoT presents unique characteristics, introducing new
challenges and research opportunities. Researchers have made significant
contributions in this area. To aid researchers in understanding the application
of traffic analysis tools for studying CIoT security and privacy risks, this
survey reviews 303 publications on traffic analysis within the CIoT security
and privacy domain from January 2018 to June 2024, focusing on three research
questions. Our work: 1) outlines the CIoT traffic analysis process and
highlights its differences from general network traffic analysis. 2) summarizes
and classifies existing research into four categories according to its
application objectives: device fingerprinting, user activity inference,
malicious traffic detection, and measurement. 3) explores emerging challenges
and potential future research directions based on each step of the CIoT traffic
analysis process. This will provide new insights to the community and guide the
industry towards safer product designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed Semi-Supervised Generalized-Linear-Regression with Applications to
  Deep-Learning and Interpolators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09526v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09526v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Yuval, Saharon Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a methodology for using unlabeled data to design semi supervised
learning (SSL) methods that improve the prediction performance of supervised
learning for regression tasks. The main idea is to design different mechanisms
for integrating the unlabeled data, and include in each of them a mixing
parameter $\alpha$, controlling the weight given to the unlabeled data.
Focusing on Generalized Linear Models (GLM) and linear interpolators classes of
models, we analyze the characteristics of different mixing mechanisms, and
prove that in all cases, it is invariably beneficial to integrate the unlabeled
data with some nonzero mixing ratio $\alpha>0$, in terms of predictive
performance. Moreover, we provide a rigorous framework to estimate the best
mixing ratio $\alpha^*$ where mixed SSL delivers the best predictive
performance, while using the labeled and unlabeled data on hand.
  The effectiveness of our methodology in delivering substantial improvement
compared to the standard supervised models, in a variety of settings, is
demonstrated empirically through extensive simulation, in a manner that
supports the theoretical analysis. We also demonstrate the applicability of our
methodology (with some intuitive modifications) to improve more complex models,
such as deep neural networks, in real-world regression tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Semih Cayci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the convergence of Gauss-Newton dynamics for training neural
networks with smooth activation functions. In the underparameterized regime,
the Gauss-Newton gradient flow induces a Riemannian gradient flow on a
low-dimensional, smooth, embedded submanifold of the Euclidean output space.
Using tools from Riemannian optimization, we prove \emph{last-iterate}
convergence of the Riemannian gradient flow to the optimal in-class predictor
at an \emph{exponential rate} that is independent of the conditioning of the
Gram matrix, \emph{without} requiring explicit regularization. We further
characterize the critical impacts of the neural network scaling factor and the
initialization on the convergence behavior. In the overparameterized regime, we
show that the Levenberg-Marquardt dynamics with an appropriately chosen damping
factor yields robustness to ill-conditioned kernels, analogous to the
underparameterized regime. These findings demonstrate the potential of
Gauss-Newton methods for efficiently optimizing neural networks, particularly
in ill-conditioned problems where kernel and Gram matrices have small singular
values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DualDynamics: Synergizing Implicit and Explicit Methods for Robust
  Irregular Time Series Analysis <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YongKyung Oh, Dongyoung Lim, Sungil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world time series analysis faces significant challenges when dealing
with irregular and incomplete data. While Neural Differential Equation (NDE)
based methods have shown promise, they struggle with limited expressiveness,
scalability issues, and stability concerns. Conversely, Neural Flows offer
stability but falter with irregular data. We introduce 'DualDynamics', a novel
framework that synergistically combines NDE-based method and Neural Flow-based
method. This approach enhances expressive power while balancing computational
demands, addressing critical limitations of existing techniques. We demonstrate
DualDynamics' effectiveness across diverse tasks: classification of robustness
to dataset shift, irregularly-sampled series analysis, interpolation of missing
data, and forecasting with partial observations. Our results show consistent
outperformance over state-of-the-art methods, indicating DualDynamics'
potential to advance irregular time series analysis significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 39th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alt-MoE: Multimodal Alignment via Alternating Optimization of
  Multi-directional MoE with Unimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyang Lei, Xiaolong Cheng, Dan Wang, Kun Fan, Qi Qin, Huazhen Huang, Yetao Wu, Qingqing Gu, Zhonglin Jiang, Yong Chen, Luo Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Large Multi-Modal Models (LMMs) have made significant advancements in
multi-modal alignment by employing lightweight connection modules to facilitate
the representation and fusion of knowledge from existing pre-trained uni-modal
models. However, these methods still rely on modality-specific and
direction-specific connectors, leading to compartmentalized knowledge
representations and reduced computational efficiency, which limits the model's
ability to form unified multi-modal representations. To address these issues,
we introduce a novel training framework, Alt-MoE, which employs the Mixture of
Experts (MoE) as a unified multi-directional connector across modalities, and
employs a multi-step sequential alternating unidirectional alignment strategy,
which converges to bidirectional alignment over iterations. The extensive
empirical studies revealed the following key points: 1) Alt-MoE achieves
competitive results by integrating diverse knowledge representations from
uni-modal models. This approach seamlessly fuses the specialized expertise of
existing high-performance uni-modal models, effectively synthesizing their
domain-specific knowledge into a cohesive multi-modal representation. 2)
Alt-MoE efficiently scales to new tasks and modalities without altering its
model architecture or training strategy. Furthermore, Alt-MoE operates in
latent space, supporting vector pre-storage and real-time retrieval via
lightweight multi-directional MoE, thereby facilitating massive data
processing. Our methodology has been validated on several well-performing
uni-modal models (LLAMA3, Qwen2, and DINOv2), achieving competitive results on
a wide range of downstream tasks and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Curriculum Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02419v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02419v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quoc Hoan Tran, Yasuhiro Endo, Hirotaka Oshima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning (QML) requires significant quantum resources to
address practical real-world problems. When the underlying quantum information
exhibits hierarchical structures in the data, limitations persist in training
complexity and generalization. Research should prioritize both the efficient
design of quantum architectures and the development of learning strategies to
optimize resource usage. We propose a framework called quantum curriculum
learning (Q-CurL) for quantum data, where the curriculum introduces simpler
tasks or data to the learning model before progressing to more challenging
ones. Q-CurL exhibits robustness to noise and data limitations, which is
particularly relevant for current and near-term noisy intermediate-scale
quantum devices. We achieve this through a curriculum design based on quantum
data density ratios and a dynamic learning schedule that prioritizes the most
informative quantum data. Empirical evidence shows that Q-CurL significantly
enhances training convergence and generalization for unitary learning and
improves the robustness of quantum phase recognition tasks. Q-CurL is effective
with broad physical learning applications in condensed matter physics and
quantum chemistry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main 6 pages, supplementary materials 11 pages (update the
  supplementary materials with more explanation on data-based Q-CurL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Discretized Neural Networks under Ricci Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03390v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03390v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Ivor W. Tsang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study Discretized Neural Networks (DNNs) composed of
low-precision weights and activations, which suffer from either infinite or
zero gradients due to the non-differentiable discrete function during training.
Most training-based DNNs in such scenarios employ the standard Straight-Through
Estimator (STE) to approximate the gradient w.r.t. discrete values. However,
the use of STE introduces the problem of gradient mismatch, arising from
perturbations in the approximated gradient. To address this problem, this paper
reveals that this mismatch can be interpreted as a metric perturbation in a
Riemannian manifold, viewed through the lens of duality theory. Building on
information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold
for DNNs, providing a background for addressing perturbations. By introducing a
partial differential equation on metrics, i.e., the Ricci flow, we establish
the dynamical stability and convergence of the LNE metric with the $L^2$-norm
perturbation. In contrast to previous perturbation theories with convergence
rates in fractional powers, the metric perturbation under the Ricci flow
exhibits exponential decay in the LNE manifold. Experimental results across
various datasets demonstrate that our method achieves superior and more stable
performance for DNNs compared to other representative training-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective Uncertainty Propagation in Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00284v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00284v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanath Kumar Krishnamurthy, Tanmay Gangwani, Sumeet Katariya, Branislav Kveton, Shrey Modi, Anshuka Rangi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the finite-horizon offline reinforcement learning (RL) setting,
and are motivated by the challenge of learning the policy at any step h in
dynamic programming (DP) algorithms. To learn this, it is sufficient to
evaluate the treatment effect of deviating from the behavioral policy at step h
after having optimized the policy for all future steps. Since the policy at any
step can affect next-state distributions, the related distributional shift
challenges can make this problem far more statistically hard than estimating
such treatment effects in the stochastic contextual bandit setting. However,
the hardness of many real-world RL instances lies between the two regimes. We
develop a flexible and general method called selective uncertainty propagation
for confidence interval construction that adapts to the hardness of the
associated distribution shift challenges. We show benefits of our approach on
toy environments and demonstrate the benefits of these techniques for offline
policy learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAP: A General Algorithm for Online Selective Conformal Prediction with
  FCR Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07728v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07728v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of post-selection predictive inference in an online
fashion. To avoid devoting resources to unimportant units, a preliminary
selection of the current individual before reporting its prediction interval is
common and meaningful in online predictive tasks. Since the online selection
causes a temporal multiplicity in the selected prediction intervals, it is
important to control the real-time false coverage-statement rate (FCR) which
measures the overall miscoverage level. We develop a general framework named
CAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on
historical data to construct a calibration set if the current individual is
selected and then outputs a conformal prediction interval for the unobserved
label. We provide tractable procedures for constructing the calibration set for
popular online selection rules. We proved that CAP can achieve an exact
selection-conditional coverage guarantee in the finite-sample and
distribution-free regimes. To account for the distribution shift in online
data, we also embed CAP into some recent dynamic conformal prediction
algorithms and show that the proposed method can deliver long-run FCR control.
Numerical results on both synthetic and real data corroborate that CAP can
effectively control FCR around the target level and yield more narrowed
prediction intervals over existing baselines across various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Infinitesimal Generators of Continuous Symmetries from Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeonghoon Ko, Hyunsu Kim, Juho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploiting symmetry inherent in data can significantly improve the sample
efficiency of a learning procedure and the generalization of learned models.
When data clearly reveals underlying symmetry, leveraging this symmetry can
naturally inform the design of model architectures or learning strategies. Yet,
in numerous real-world scenarios, identifying the specific symmetry within a
given data distribution often proves ambiguous. To tackle this, some existing
works learn symmetry in a data-driven manner, parameterizing and learning
expected symmetry through data. However, these methods often rely on explicit
knowledge, such as pre-defined Lie groups, which are typically restricted to
linear or affine transformations. In this paper, we propose a novel symmetry
learning algorithm based on transformations defined with one-parameter groups,
continuously parameterized transformations flowing along the directions of
vector fields called infinitesimal generators. Our method is built upon minimal
inductive biases, encompassing not only commonly utilized symmetries rooted in
Lie groups but also extending to symmetries derived from nonlinear generators.
To learn these symmetries, we introduce a notion of a validity score that
examine whether the transformed data is still valid for the given task. The
validity score is designed to be fully differentiable and easily computable,
enabling effective searches for transformations that achieve symmetries innate
to the data. We apply our method mainly in two domains: image data and partial
differential equations, and demonstrate its advantages. Our codes are available
at \url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does the Smoothness Approximation Method Facilitate Generalization
  for Federated Adversarial Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Ding, Ying An, Lixing Chen, Shichao Kan, Fan Wu, Zhe Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Adversarial Learning (FAL) is a robust framework for resisting
adversarial attacks on federated learning. Although some FAL studies have
developed efficient algorithms, they primarily focus on convergence performance
and overlook generalization. Generalization is crucial for evaluating algorithm
performance on unseen data. However, generalization analysis is more
challenging due to non-smooth adversarial loss functions. A common approach to
addressing this issue is to leverage smoothness approximation. In this paper,
we develop algorithm stability measures to evaluate the generalization
performance of two popular FAL algorithms: \textit{Vanilla FAL (VFAL)} and {\it
Slack FAL (SFAL)}, using three different smooth approximation methods: 1)
\textit{Surrogate Smoothness Approximation (SSA)}, (2) \textit{Randomized
Smoothness Approximation (RSA)}, and (3) \textit{Over-Parameterized Smoothness
Approximation (OPSA)}. Based on our in-depth analysis, we answer the question
of how to properly set the smoothness approximation method to mitigate
generalization error in FAL. Moreover, we identify RSA as the most effective
method for reducing generalization error. In highly data-heterogeneous
scenarios, we also recommend employing SFAL to mitigate the deterioration of
generalization performance caused by heterogeneity. Based on our theoretical
results, we provide insights to help develop more efficient FAL algorithms,
such as designing new metrics and dynamic aggregation rules to mitigate
heterogeneity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Forget-free Winning Subnetworks for Video
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11973v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11973v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Jaehong Yoon, Sung Ju Hwang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) and Task-agnostic
Incremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning
(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is
designed to prevent overfitting when the data samples are scarce. Furthermore,
the sparse reuse of WSN weights is considered for Video Incremental Learning
(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It
enables compact encoding of videos and identifies reusable subnetworks across
varying bandwidths. We have integrated FSO into different architectural
frameworks for continual learning, including VIL, TIL, and FSCIL. Our
comprehensive experiments demonstrate FSO's effectiveness, significantly
improving task performance at various convolutional representational levels.
Specifically, FSO enhances higher-layer performance in TIL and FSCIL and
lower-layer performance in VIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Pattern Analysis and Machine Intelligence
  (T-PAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time
  Adaptation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world vision models in dynamic environments face rapid shifts in domain
distributions, leading to decreased recognition performance. Using unlabeled
test data, continuous test-time adaptation (CTTA) directly adjusts a
pre-trained source discriminative model to these changing domains. A highly
effective CTTA method involves applying layer-wise adaptive learning rates for
selectively adapting pre-trained layers. However, it suffers from the poor
estimation of domain shift and the inaccuracies arising from the pseudo-labels.
This work aims to overcome these limitations by identifying layers for
adaptation via quantifying model prediction uncertainty without relying on
pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by
backpropagating the KL divergence between the softmax output and a uniform
distribution, to select layers for further adaptation. Subsequently, for the
parameters exclusively belonging to these selected layers, with the remaining
ones frozen, we evaluate their sensitivity to approximate the domain shift and
adjust their learning rates accordingly. We conduct extensive image
classification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,
demonstrating the superior efficacy of our method compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an unsupervised model for instruction-based image editing that
eliminates the need for ground-truth edited images during training. Existing
supervised methods depend on datasets containing triplets of input image,
edited image, and edit instruction. These are generated by either existing
editing methods or human-annotations, which introduce biases and limit their
generalization ability. Our method addresses these challenges by introducing a
novel editing mechanism called Cycle Edit Consistency (CEC), which applies
forward and backward edits in one training step and enforces consistency in
image and attention spaces. This allows us to bypass the need for ground-truth
edited images and unlock training for the first time on datasets comprising
either real image-caption pairs or image-caption-edit triplets. We empirically
show that our unsupervised technique performs better across a broader range of
edits with high fidelity and precision. By eliminating the need for
pre-existing datasets of triplets, reducing biases associated with supervised
methods, and proposing CEC, our work represents a significant advancement in
unblocking scaling of instruction-based image editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://enis.dev/uip2p/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnvGS: Modeling View-Dependent Appearance with Environment Gaussian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing complex reflections in real-world scenes from 2D images is
essential for achieving photorealistic novel view synthesis. Existing methods
that utilize environment maps to model reflections from distant lighting often
struggle with high-frequency reflection details and fail to account for
near-field reflections. In this work, we introduce EnvGS, a novel approach that
employs a set of Gaussian primitives as an explicit 3D representation for
capturing reflections of environments. These environment Gaussian primitives
are incorporated with base Gaussian primitives to model the appearance of the
whole scene. To efficiently render these environment Gaussian primitives, we
developed a ray-tracing-based renderer that leverages the GPU's RT core for
fast rendering. This allows us to jointly optimize our model for high-quality
reconstruction while maintaining real-time rendering speeds. Results from
multiple real-world and synthetic datasets demonstrate that our method produces
significantly more detailed reflections, achieving the best rendering quality
in real-time novel view synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/envgs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flowing from Words to Pixels: A Framework for Cross-Modality Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, and their generalization, flow matching, have had a
remarkable impact on the field of media generation. Here, the conventional
approach is to learn the complex mapping from a simple source distribution of
Gaussian noise to the target media distribution. For cross-modal tasks such as
text-to-image generation, this same mapping from noise to image is learnt
whilst including a conditioning mechanism in the model. One key and thus far
relatively unexplored feature of flow matching is that, unlike Diffusion
models, they are not constrained for the source distribution to be noise.
Hence, in this paper, we propose a paradigm shift, and ask the question of
whether we can instead train flow matching models to learn a direct mapping
from the distribution of one modality to the distribution of another, thus
obviating the need for both the noise distribution and conditioning mechanism.
We present a general and simple framework, CrossFlow, for cross-modal flow
matching. We show the importance of applying Variational Encoders to the input
data, and introduce a method to enable Classifier-free guidance. Surprisingly,
for text-to-image, CrossFlow with a vanilla transformer without cross attention
slightly outperforms standard flow matching, and we show that it scales better
with training steps and model size, while also allowing for interesting latent
arithmetic which results in semantically meaningful edits in the output space.
To demonstrate the generalizability of our approach, we also show that
CrossFlow is on par with or outperforms the state-of-the-art for various
cross-modal / intra-modal mapping tasks, viz. image captioning, depth
estimation, and image super-resolution. We hope this paper contributes to
accelerating progress in cross-modal media generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://cross-flow.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intuitive nature of drag-based interaction has led to its growing
adoption for controlling object trajectories in image-to-video synthesis.
Still, existing methods that perform dragging in the 2D space usually face
ambiguity when handling out-of-plane movements. In this work, we augment the
interaction with a new dimension, i.e., the depth dimension, such that users
are allowed to assign a relative depth for each point on the trajectory. That
way, our new interaction paradigm not only inherits the convenience from 2D
dragging, but facilitates trajectory control in the 3D space, broadening the
scope of creativity. We propose a pioneering method for 3D trajectory control
in image-to-video synthesis by abstracting object masks into a few cluster
points. These points, accompanied by the depth information and the instance
information, are finally fed into a video diffusion model as the control
signal. Extensive experiments validate the effectiveness of our approach,
dubbed LeviTor, in precisely manipulating the object movements when producing
photo-realistic videos from static images. Project page:
https://ppetrichor.github.io/levitor.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page available at
  https://ppetrichor.github.io/levitor.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multiview Relighting for 3D Reconstruction under Extreme
  Illumination Variation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the geometry and appearance of objects from photographs taken
in different environments is difficult as the illumination and therefore the
object appearance vary across captured images. This is particularly challenging
for more specular objects whose appearance strongly depends on the viewing
direction. Some prior approaches model appearance variation across images using
a per-image embedding vector, while others use physically-based rendering to
recover the materials and per-image illumination. Such approaches fail at
faithfully recovering view-dependent appearance given significant variation in
input illumination and tend to produce mostly diffuse results. We present an
approach that reconstructs objects from images taken under different
illuminations by first relighting the images under a single reference
illumination with a multiview relighting diffusion model and then
reconstructing the object's geometry and appearance with a radiance field
architecture that is robust to the small remaining inconsistencies among the
relit images. We validate our proposed approach on both synthetic and real
datasets and demonstrate that it greatly outperforms existing techniques at
reconstructing high-fidelity appearance from images taken under extreme
illumination variation. Moreover, our approach is particularly effective at
recovering view-dependent "shiny" appearance which cannot be reconstructed by
prior methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://relight-to-reconstruct.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling 4D Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Vélez, Luisa Polanía, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Pătrăucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling has not yet been convincingly demonstrated for pure self-supervised
learning from video. However, prior work has focused evaluations on
semantic-related tasks $\unicode{x2013}$ action classification, ImageNet
classification, etc. In this paper we focus on evaluating self-supervised
learning on non-semantic vision tasks that are more spatial (3D) and temporal
(+1D = 4D), such as camera pose estimation, point and object tracking, and
depth estimation. We show that by learning from very large video datasets,
masked auto-encoding (MAE) with transformer video models actually scales,
consistently improving performance on these 4D tasks, as model size increases
from 20M all the way to the largest by far reported self-supervised video model
$\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with
many recent image and video models demonstrates the benefits of scaling 4D
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in Large Vision-Language Models (LVLMs),
existing pixel-grounding models operate on single-image settings, limiting
their ability to perform detailed, fine-grained comparisons across multiple
images. Conversely, current multi-image understanding models lack pixel-level
grounding. Our work addresses this gap by introducing the task of multi-image
pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates
pixel-level grounding with robust multi-image reasoning capabilities to produce
contextually rich, pixel-grounded explanations. Central to PRIMA is an
efficient vision module that queries fine-grained visual representations across
multiple images, reducing TFLOPs by $25.3\%$. To support training and
evaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark
consisting of $\sim$224K question-answer pairs that require fine-grained visual
understanding across multiple images. Experimental results demonstrate PRIMA
outperforms state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://plan-lab.github.io/prima</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the advent of Multimodal Large Language Models (MLLMs), they have made
a significant impact across a wide range of real-world applications,
particularly in Autonomous Driving (AD). Their ability to process complex
visual data and reason about intricate driving scenarios has paved the way for
a new paradigm in end-to-end AD systems. However, the progress of developing
end-to-end models for AD has been slow, as existing fine-tuning methods demand
substantial resources, including extensive computational power, large-scale
datasets, and significant funding. Drawing inspiration from recent advancements
in inference computing, we propose OpenEMMA, an open-source end-to-end
framework based on MLLMs. By incorporating the Chain-of-Thought reasoning
process, OpenEMMA achieves significant improvements compared to the baseline
when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates
effectiveness, generalizability, and robustness across a variety of challenging
driving scenarios, offering a more efficient and effective approach to
autonomous driving. We release all the codes in
https://github.com/taco-group/OpenEMMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large vision language models (VLMs) tailored for
autonomous driving (AD) have shown strong scene understanding and reasoning
capabilities, making them undeniable candidates for end-to-end driving systems.
However, limited work exists on studying the trustworthiness of DriveVLMs -- a
critical factor that directly impacts public transportation safety. In this
paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for
large vision-language models in autonomous driving (DriveVLMs), considering
diverse perspectives -- including trustfulness, safety, robustness, privacy,
and fairness. We constructed the largest visual question-answering dataset for
investigating trustworthiness issues in driving scenarios, comprising over 10k
unique scenes and 18k queries. We evaluated six publicly available VLMs,
spanning from generalist to specialist, from open-source to commercial models.
Our exhaustive evaluations have unveiled previously undiscovered
vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found
that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform
specialized models fine-tuned for driving in terms of overall trustworthiness.
DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing
sensitive information. Additionally, both generalist and specialist VLMs remain
susceptible to adversarial attacks and struggle to ensure unbiased
decision-making across diverse environments and populations. Our findings call
for immediate and decisive action to address the trustworthiness of DriveVLMs
-- an issue of critical importance to public safety and the welfare of all
citizens relying on autonomous transportation systems. Our benchmark is
publicly available at \url{https://github.com/taco-group/AutoTrust}, and the
leaderboard is released at \url{https://taco-group.github.io/AutoTrust/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) modeling has achieved remarkable success in natural
language processing by enabling models to generate text with coherence and
contextual understanding through next token prediction. Recently, in image
generation, VAR proposes scale-wise autoregressive modeling, which extends the
next token prediction to the next scale prediction, preserving the 2D structure
of images. However, VAR encounters two primary challenges: (1) its complex and
rigid scale design limits generalization in next scale prediction, and (2) the
generator's dependence on a discrete tokenizer with the same complex scale
structure restricts modularity and flexibility in updating the tokenizer. To
address these limitations, we introduce FlowAR, a general next scale prediction
method featuring a streamlined scale design, where each subsequent scale is
simply double the previous one. This eliminates the need for VAR's intricate
multi-scale residual tokenizer and enables the use of any off-the-shelf
Variational AutoEncoder (VAE). Our simplified design enhances generalization in
next scale prediction and facilitates the integration of Flow Matching for
high-quality image synthesis. We validate the effectiveness of FlowAR on the
challenging ImageNet-256 benchmark, demonstrating superior generation
performance compared to previous methods. Codes will be available at
\url{https://github.com/OliverRensu/FlowAR}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation
  for High-quality 3D Asset Creation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural Content Generation (PCG) is powerful in creating high-quality 3D
contents, yet controlling it to produce desired shapes is difficult and often
requires extensive parameter tuning. Inverse Procedural Content Generation aims
to automatically find the best parameters under the input condition. However,
existing sampling-based and neural network-based methods still suffer from
numerous sample iterations or limited controllability. In this work, we present
DI-PCG, a novel and efficient method for Inverse PCG from general image
conditions. At its core is a lightweight diffusion transformer model, where PCG
parameters are directly treated as the denoising target and the observed images
as conditions to control parameter generation. DI-PCG is efficient and
effective. With only 7.6M network parameters and 30 GPU hours to train, it
demonstrates superior performance in recovering parameters accurately, and
generalizing well to in-the-wild images. Quantitative and qualitative
experiment results validate the effectiveness of DI-PCG in inverse PCG and
image-to-3D generation tasks. DI-PCG offers a promising approach for efficient
inverse PCG and represents a valuable exploration step towards a 3D generation
path that models how to construct a 3D asset using parametric models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://thuzhaowang.github.io/projects/DI-PCG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper targets the challenge of real-time LiDAR re-simulation in dynamic
driving scenarios. Recent approaches utilize neural radiance fields combined
with the physical modeling of LiDAR sensors to achieve high-fidelity
re-simulation results. Unfortunately, these methods face limitations due to
high computational demands in large-scale scenes and cannot perform real-time
LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel
framework that supports real-time, physically accurate LiDAR re-simulation for
driving scenes. Our primary contribution is the development of an efficient and
effective rendering pipeline, which integrates Gaussian primitives and
hardware-accelerated ray tracing technology. Specifically, we model the
physical properties of LiDAR sensors using Gaussian primitives with learnable
parameters and incorporate scene graphs to handle scene dynamics. Building upon
this scene representation, our framework first constructs a bounding volume
hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views
through a differentiable rendering algorithm. Importantly, our framework
supports realistic rendering with flexible scene editing operations and various
sensor configurations. Extensive experiments across multiple public benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
rendering quality and efficiency. Our project page is at
https://zju3dv.github.io/lidar-rt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/lidar-rt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preventing Local Pitfalls in Vector Quantization via Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-quantized networks (VQNs) have exhibited remarkable performance across
various tasks, yet they are prone to training instability, which complicates
the training process due to the necessity for techniques such as subtle
initialization and model distillation. In this study, we identify the local
minima issue as the primary cause of this instability. To address this, we
integrate an optimal transport method in place of the nearest neighbor search
to achieve a more globally informed assignment. We introduce OptVQ, a novel
vector quantization method that employs the Sinkhorn algorithm to optimize the
optimal transport problem, thereby enhancing the stability and efficiency of
the training process. To mitigate the influence of diverse data distributions
on the Sinkhorn algorithm, we implement a straightforward yet effective
normalization strategy. Our comprehensive experiments on image reconstruction
tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses
current state-of-the-art VQNs in reconstruction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/zbr17/OptVQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal
  Audio-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video
generation that leverages the activations of frozen video and audio diffusion
models for temporally-aligned cross-modal conditioning. The key to our
framework is a Fusion Block that enables bidirectional information exchange
between our backbone video and audio diffusion models through a
temporally-aligned self attention operation. Unlike prior work that uses
feature extractors pretrained for other tasks for the conditioning signal,
AV-Link can directly leverage features obtained by the complementary modality
in a single framework i.e. video features to generate audio, or audio features
to generate video. We extensively evaluate our design choices and demonstrate
the ability of our method to achieve synchronized and high-quality audiovisual
content, showcasing its potential for applications in immersive media
generation. Project Page: snap-research.github.io/AVLink/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: snap-research.github.io/AVLink/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EarthDial: Turning Multi-sensory Earth Observations to Interactive
  Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated analysis of vast Earth observation data via interactive
Vision-Language Models (VLMs) can unlock new opportunities for environmental
monitoring, disaster response, and resource management. Existing generic VLMs
do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs
remain restricted to a fixed resolution and few sensor modalities. In this
paper, we introduce EarthDial, a conversational assistant specifically designed
for Earth Observation (EO) data, transforming complex, multi-sensory Earth
observations into interactive, natural language dialogues. EarthDial supports
multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide
range of remote sensing tasks, including classification, detection, captioning,
question answering, visual reasoning, and visual grounding. To achieve this, we
introduce an extensive instruction tuning dataset comprising over 11.11M
instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and
multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,
EarthDial handles bi-temporal and multi-temporal sequence analysis for
applications like change detection. Our extensive experimental results on 37
downstream applications demonstrate that EarthDial outperforms existing generic
and domain-specific models, achieving better generalization across various EO
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LlamaFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LlamaFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LlamaFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LlamaFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LlamaFusion improves image understanding by 20% and image generation by 3.6%
using only 50% of the FLOPs while maintaining Llama-3's language capabilities.
We also demonstrate that this framework can adapt existing vision-language
models with multimodal generation ability. Overall, this framework not only
leverages existing computational investments in text-only LLMs but also enables
the parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tiled Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Madar, Ohad Fried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image tiling -- the seamless connection of disparate images to create a
coherent visual field -- is crucial for applications such as texture creation,
video game asset development, and digital art. Traditionally, tiles have been
constructed manually, a method that poses significant limitations in
scalability and flexibility. Recent research has attempted to automate this
process using generative models. However, current approaches primarily focus on
tiling textures and manipulating models for single-image generation, without
inherently supporting the creation of multiple interconnected tiles across
diverse domains. This paper presents Tiled Diffusion, a novel approach that
extends the capabilities of diffusion models to accommodate the generation of
cohesive tiling patterns across various domains of image synthesis that require
tiling. Our method supports a wide range of tiling scenarios, from self-tiling
to complex many-to-many connections, enabling seamless integration of multiple
images. Tiled Diffusion automates the tiling process, eliminating the need for
manual intervention and enhancing creative possibilities in various
applications, such as seamlessly tiling of existing images, tiled texture
creation, and 360{\deg} synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SqueezeMe: Efficient Gaussian Avatars for VR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting has enabled real-time 3D human avatars with unprecedented
levels of visual quality. While previous methods require a desktop GPU for
real-time inference of a single avatar, we aim to squeeze multiple Gaussian
avatars onto a portable virtual reality headset with real-time drivable
inference. We begin by training a previous work, Animatable Gaussians, on a
high quality dataset captured with 512 cameras. The Gaussians are animated by
controlling base set of Gaussians with linear blend skinning (LBS) motion and
then further adjusting the Gaussians with a neural network decoder to correct
their appearance. When deploying the model on a Meta Quest 3 VR headset, we
find two major computational bottlenecks: the decoder and the rendering. To
accelerate the decoder, we train the Gaussians in UV-space instead of
pixel-space, and we distill the decoder to a single neural network layer.
Further, we discover that neighborhoods of Gaussians can share a single
corrective from the decoder, which provides an additional speedup. To
accelerate the rendering, we develop a custom pipeline in Vulkan that runs on
the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently
at 72 FPS on a VR headset. Demo videos are at
https://forresti.github.io/squeezeme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OnlineVPO: Align Video Diffusion Model with Online Video-Centric
  Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, Kai Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the field of text-to-video (T2V) generation has made
significant strides. Despite this progress, there is still a gap between
theoretical advancements and practical application, amplified by issues like
degraded image quality and flickering artifacts. Recent advancements in
enhancing the video diffusion model (VDM) through feedback learning have shown
promising results. However, these methods still exhibit notable limitations,
such as misaligned feedback and inferior scalability. To tackle these issues,
we introduce OnlineVPO, a more efficient preference learning approach tailored
specifically for video diffusion models. Our method features two novel designs,
firstly, instead of directly using image-based reward feedback, we leverage the
video quality assessment (VQA) model trained on synthetic data as the reward
model to provide distribution and modality-aligned feedback on the video
diffusion model. Additionally, we introduce an online DPO algorithm to address
the off-policy optimization and scalability issue in existing video preference
learning frameworks. By employing the video reward model to offer concise video
feedback on the fly, OnlineVPO offers effective and efficient preference
guidance. Extensive experiments on the open-source video-diffusion model
demonstrate OnlineVPO as a simple yet effective and more importantly scalable
preference learning algorithm for video diffusion models, offering valuable
insights for future advancements in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-A-Video: <span class="highlight-title">Prompt</span> Your Video Diffusion Model via Preference-Aligned
  LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, Weilin Huang, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video models have made remarkable advancements through optimization
on high-quality text-video pairs, where the textual prompts play a pivotal role
in determining quality of output videos. However, achieving the desired output
often entails multiple revisions and iterative inference to refine
user-provided prompts. Current automatic methods for refining prompts encounter
challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware
when applied to text-to-video diffusion models. To address these problem, we
introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,
which excels in crafting Video-Centric, Labor-Free and Preference-Aligned
prompts tailored to specific video diffusion model. Our approach involves a
meticulously crafted two-stage optimization and alignment system. Initially, we
conduct a reward-guided prompt evolution pipeline to automatically create
optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the
LLM. Then multi-dimensional rewards are employed to generate pairwise data for
the SFT model, followed by the direct preference optimization (DPO) algorithm
to further facilitate preference alignment. Through extensive experimentation
and comparative analyses, we validate the effectiveness of Prompt-A-Video
across diverse generation models, highlighting its potential to push the
boundaries of video generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Color Channel Independence for Improved Unsupervised Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Jäckl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric architectures can learn to extract distinct object
representations from visual scenes, enabling downstream applications on the
object level. Similarly to autoencoder-based image models, object-centric
approaches have been trained on the unsupervised reconstruction loss of images
encoded by RGB color spaces. In our work, we challenge the common assumption
that RGB images are the optimal color space for unsupervised learning in
computer vision. We discuss conceptually and empirically that other color
spaces, such as HSV, bear essential characteristics for object-centric
representation learning, like robustness to lighting conditions. We further
show that models improve when requiring them to predict additional color
channels. Specifically, we propose to transform the predicted targets to the
RGB-S space, which extends RGB with HSV's saturation component and leads to
markedly better reconstruction and disentanglement for five common evaluation
datasets. The use of composite color spaces can be implemented with basically
no computational overhead, is agnostic of the models' architecture, and is
universally applicable across a wide range of visual computing tasks and
training types. The findings of our approach encourage additional
investigations in computer vision tasks beyond object-centric learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages incl. references, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jet: A Modern <span class="highlight-title">Transformer</span>-Based Normalizing Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kolesnikov, André Susano Pinto, Michael Tschannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past, normalizing generative flows have emerged as a promising class
of generative models for natural images. This type of model has many modeling
advantages: the ability to efficiently compute log-likelihood of the input
data, fast generation and simple overall structure. Normalizing flows remained
a topic of active research but later fell out of favor, as visual quality of
the samples was not competitive with other model classes, such as GANs,
VQ-VAE-based approaches or diffusion models. In this paper we revisit the
design of the coupling-based normalizing flow models by carefully ablating
prior design choices and using computational blocks based on the Vision
Transformer architecture, not convolutional neural networks. As a result, we
achieve state-of-the-art quantitative and qualitative performance with a much
simpler architecture. While the overall visual quality is still behind the
current state-of-the-art models, we argue that strong normalizing flow models
can help advancing research frontier by serving as building components of more
powerful generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallelized Autoregressive Visual Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive models have emerged as a powerful approach for visual
generation but suffer from slow inference speed due to their sequential
token-by-token prediction process. In this paper, we propose a simple yet
effective approach for parallelized autoregressive visual generation that
improves generation efficiency while preserving the advantages of
autoregressive modeling. Our key insight is that parallel generation depends on
visual token dependencies-tokens with weak dependencies can be generated in
parallel, while strongly dependent adjacent tokens are difficult to generate
together, as their independent sampling may lead to inconsistencies. Based on
this observation, we develop a parallel generation strategy that generates
distant tokens with weak dependencies in parallel while maintaining sequential
generation for strongly dependent local tokens. Our approach can be seamlessly
integrated into standard autoregressive models without modifying the
architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that
our method achieves a 3.6x speedup with comparable quality and up to 9.5x
speedup with minimal quality degradation across both image and video generation
tasks. We hope this work will inspire future research in efficient visual
generation and unified autoregressive modeling. Project page:
https://epiphqny.github.io/PAR-project.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://epiphqny.github.io/PAR-project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowing Where to Focus: Attention-Guided Alignment for Text-based Person
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Tan, Weihao Li, Pingyang Dai, Jie Chen, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of Text-Based Person Search (TBPS), mainstream methods aim to
explore more efficient interaction frameworks between text descriptions and
visual data. However, recent approaches encounter two principal challenges.
Firstly, the widely used random-based Masked Language Modeling (MLM) considers
all the words in the text equally during training. However, massive
semantically vacuous words ('with', 'the', etc.) be masked fail to contribute
efficient interaction in the cross-modal MLM and hampers the representation
alignment. Secondly, manual descriptions in TBPS datasets are tedious and
inevitably contain several inaccuracies. To address these issues, we introduce
an Attention-Guided Alignment (AGA) framework featuring two innovative
components: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module
(TEM). AGM dynamically masks semantically meaningful words by aggregating the
attention weight derived from the text encoding process, thereby cross-modal
MLM can capture information related to the masked word from text context and
images and align their representations. Meanwhile, TEM alleviates low-quality
representations caused by repetitive and erroneous text descriptions by
replacing those semantically meaningful words with MLM's prediction. It not
only enriches text descriptions but also prevents overfitting. Extensive
experiments across three challenging benchmarks demonstrate the effectiveness
of our AGA, achieving new state-of-the-art results with Rank-1 accuracy
reaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Full <span class="highlight-title">Transformer</span>-based Framework for Automatic Pain Estimation using
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefanos Gkikas, Manolis Tsiknakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic estimation of pain is essential in designing an optimal pain
management system offering reliable assessment and reducing the suffering of
patients. In this study, we present a novel full transformer-based framework
consisting of a Transformer in Transformer (TNT) model and a Transformer
leveraging cross-attention and self-attention blocks. Elaborating on videos
from the BioVid database, we demonstrate state-of-the-art performances, showing
the efficacy, efficiency, and generalization capability across all the primary
pain estimation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Till the Layers Collapse: Compressing a Deep Neural Network through the
  Lenses of Batch Normalization Layers <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Liao, Nour Hezbri, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, deep neural networks are widely used since they can handle a variety
of complex tasks. Their generality makes them very powerful tools in modern
technology. However, deep neural networks are often overparameterized. The
usage of these large models consumes a lot of computation resources. In this
paper, we introduce a method called \textbf{T}ill the \textbf{L}ayers
\textbf{C}ollapse (TLC), which compresses deep neural networks through the
lenses of batch normalization layers. By reducing the depth of these networks,
our method decreases deep neural networks' computational requirements and
overall latency. We validate our method on popular models such as Swin-T,
MobileNet-V2, and RoBERTa, across both image classification and natural
language processing (NLP) tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging
  <span class="highlight-title">Dataset</span>s with In-Context Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical researchers and clinicians often need to perform novel segmentation
tasks on a set of related images. Existing methods for segmenting a new dataset
are either interactive, requiring substantial human effort for each image, or
require an existing set of manually labeled images. We introduce a system,
MultiverSeg, that enables practitioners to rapidly segment an entire new
dataset without requiring access to any existing labeled data from that task or
domain. Along with the image to segment, the model takes user interactions such
as clicks, bounding boxes or scribbles as input, and predicts a segmentation.
As the user segments more images, those images and segmentations become
additional inputs to the model, providing context. As the context set of
labeled images grows, the number of interactions required to segment each new
image decreases. We demonstrate that MultiverSeg enables users to interactively
segment new datasets efficiently, by amortizing the number of interactions per
image to achieve an accurate segmentation. Compared to using a state-of-the-art
interactive segmentation method, using MultiverSeg reduced the total number of
scribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images
from unseen tasks. We release code and model weights at
https://multiverseg.csail.mit.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://multiverseg.csail.mit.edu Keywords:
  interactive segmentation, in-context learning, medical image analysis,
  biomedical imaging, image annotation, visual prompting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GIRAFE: Glottal Imaging <span class="highlight-title">Dataset</span> for Advanced Segmentation, Analysis, and
  Facilitative Playbacks Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Andrade-Miranda, K. Chatzipapas, J. D. Arias-Londoño, J. I. Godino-Llorente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advances in the development of Facilitative Playbacks extracted from
High-Speed videoendoscopic sequences of the vocal folds are hindered by a
notable lack of publicly available datasets annotated with the semantic
segmentations corresponding to the area of the glottal gap. This fact also
limits the reproducibility and further exploration of existing research in this
field.
  To address this gap, GIRAFE is a data repository designed to facilitate the
development of advanced techniques for the semantic segmentation, analysis, and
fast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The
repository includes 65 high-speed videoendoscopic recordings from a cohort of
50 patients (30 female, 20 male). The dataset comprises 15 recordings from
healthy controls, 26 from patients with diagnosed voice disorders, and 24 with
an unknown health condition. All of them were manually annotated by an expert,
including the masks corresponding to the semantic segmentation of the glottal
gap. The repository is also complemented with the automatic segmentation of the
glottal area using different state-of-the-art approaches.
  This data set has already supported several studies, which demonstrates its
usefulness for the development of new glottal gap segmentation algorithms from
High-Speed-Videoendoscopic sequences to improve or create new Facilitative
Playbacks. Despite these advances and others in the field, the broader
challenge of performing an accurate and completely automatic semantic
segmentation method of the glottal area remains open.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifei Chen, Tianshuo Xu, Wenhang Ge, Leyi Wu, Dongyu Yan, Jing He, Luozhou Wang, Lu Zeng, Shunsi Zhang, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rendering and inverse rendering are pivotal tasks in both computer vision and
graphics. The rendering equation is the core of the two tasks, as an ideal
conditional distribution transfer function from intrinsic properties to RGB
images. Despite achieving promising results of existing rendering methods, they
merely approximate the ideal estimation for a specific scene and come with a
high computational cost. Additionally, the inverse conditional distribution
transfer is intractable due to the inherent ambiguity. To address these
challenges, we propose a data-driven method that jointly models rendering and
inverse rendering as two conditional generation tasks within a single diffusion
framework. Inspired by UniDiffuser, we utilize two distinct time schedules to
model both tasks, and with a tailored dual streaming module, we achieve
cross-conditioning of two pre-trained diffusion models. This unified approach,
named Uni-Renderer, allows the two processes to facilitate each other through a
cycle-consistent constrain, mitigating ambiguity by enforcing consistency
between intrinsic properties and rendered images. Combined with a meticulously
prepared dataset, our method effectively decomposition of intrinsic properties
and demonstrates a strong capability to recognize changes during rendering. We
will open-source our training and inference code to the public, fostering
further research and development in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Albert Ali Salah, Itir Onal Ertugrul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores image modeling from the frequency space and introduces
DCTdiff, an end-to-end diffusion generative paradigm that efficiently models
images in the discrete cosine transform (DCT) space. We investigate the design
space of DCTdiff and reveal the key design factors. Experiments on different
frameworks (UViT, DiT), generation tasks, and various diffusion samplers
demonstrate that DCTdiff outperforms pixel-based diffusion models regarding
generative quality and training efficiency. Remarkably, DCTdiff can seamlessly
scale up to high-resolution generation without using the latent diffusion
paradigm. Finally, we illustrate several intriguing properties of DCT image
modeling. For example, we provide a theoretical proof of why `image diffusion
can be seen as spectral autoregression', bridging the gap between diffusion and
autoregressive models. The effectiveness of DCTdiff and the introduced
properties suggest a promising direction for image modeling in the frequency
space. The code is at \url{https://github.com/forever208/DCTdiff}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and
  Semantic Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunità, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound designers and Foley artists usually sonorize a scene, such as from a
movie or video game, by manually annotating and sonorizing each action of
interest in the video. In our case, the intent is to leave full creative
control to sound designers with a tool that allows them to bypass the more
repetitive parts of their work, thus being able to focus on the creative
aspects of sound production. We achieve this presenting Stable-V2A, a two-stage
model consisting of: an RMS-Mapper that estimates an envelope representative of
the audio characteristics associated with the input video; and Stable-Foley, a
diffusion model based on Stable Audio Open that generates audio semantically
and temporally aligned with the target video. Temporal alignment is guaranteed
by the use of the envelope as a ControlNet input, while semantic alignment is
achieved through the use of sound representations chosen by the designer as
cross-attention conditioning of the diffusion process. We train and test our
model on Greatest Hits, a dataset commonly used to evaluate V2A models. In
addition, to test our model on a case study of interest, we introduce Walking
The Maps, a dataset of videos extracted from video games depicting animated
characters walking in different locations. Samples and code available on our
demo page at https://ispamm.github.io/Stable-V2A.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Federated Learning in the Face of Covariate Shift: A Magnitude
  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozgu Goksu, Nicolas Pugeault
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of highly sophisticated neural networks has allowed for fast
progress in every field of computer vision, however, applications where
annotated data is prohibited due to privacy or security concerns remain
challenging. Federated Learning (FL) offers a promising framework for
individuals aiming to collaboratively develop a shared model while preserving
data privacy. Nevertheless, our findings reveal that variations in data
distribution among clients can profoundly affect FL methodologies, primarily
due to instabilities in the aggregation process. We also propose a novel FL
framework to mitigate the adverse effects of covariate shifts among federated
clients by combining individual parameter pruning and regularization techniques
to improve the robustness of individual clients' models to aggregate. Each
client's model is optimized through magnitude-based pruning and the addition of
dropout and noise injection layers to build more resilient decision pathways in
the networks and improve the robustness of the model's parameter aggregation
step. The proposed framework is capable of extracting robust representations
even in the presence of very large covariate shifts among client data
distributions and in the federation of a small number of clients. Empirical
findings substantiate the effectiveness of our proposed methodology across
common benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.
Furthermore, we introduce the CelebA-Gender dataset, specifically designed to
evaluate performance on a more realistic domain. The proposed method is capable
of extracting robust representations even in the presence of both high and low
covariate shifts among client data distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stitch Contrast and Segment_Learning a Human Action Segmentation Model
  Using Trimmed Skeleton Videos <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Tian, Pierre Payeur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing skeleton-based human action classification models rely on
well-trimmed action-specific skeleton videos for both training and testing,
precluding their scalability to real-world applications where untrimmed videos
exhibiting concatenated actions are predominant. To overcome this limitation,
recently introduced skeleton action segmentation models involve un-trimmed
skeleton videos into end-to-end training. The model is optimized to provide
frame-wise predictions for any length of testing videos, simultaneously
realizing action localization and classification. Yet, achieving such an
improvement im-poses frame-wise annotated skeleton videos, which remains
time-consuming in practice. This paper features a novel framework for
skeleton-based action segmentation trained on short trimmed skeleton videos,
but that can run on longer un-trimmed videos. The approach is implemented in
three steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral
skeleton stitching scheme that treats trimmed skeleton videos as elementary
human motions that compose a semantic space and can be sampled to generate
multi-action stitched se-quences. Contrast learns contrastive representations
from stitched sequences with a novel discrimination pretext task that enables a
skeleton encoder to learn meaningful action-temporal contexts to improve action
segmentation. Finally, Segment relates the proposed method to action
segmentation by learning a segmentation layer while handling particular da-ta
availability. Experiments involve a trimmed source dataset and an untrimmed
target dataset in an adaptation formulation for real-world skeleton-based human
action segmentation to evaluate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse
  Articulated Objects with Rich Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhua Sun, Yuxuan Li, Jiude Wei, Longfei Xu, Nange Wang, Yining Zhang, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The acquisition of substantial volumes of 3D articulated object data is
expensive and time-consuming, and consequently the scarcity of 3D articulated
object data becomes an obstacle for deep learning methods to achieve remarkable
performance in various articulated object understanding tasks. Meanwhile,
pairing these object data with detailed annotations to enable training for
various tasks is also difficult and labor-intensive to achieve. In order to
expeditiously gather a significant number of 3D articulated objects with
comprehensive and detailed annotations for training, we propose Articulated
Object Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox
consists of i) descriptions of articulated objects by means of a generalized
structure program along with their analytic correspondence to the objects'
point cloud, ii) procedural rules about manipulations on the structure program
to synthesize large-scale and diverse new articulated objects, and iii)
mathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to
provide annotations to the synthesized object. Arti-PG has two appealing
properties for providing training data for articulated object understanding
tasks: i) objects are created with unlimited variations in shape through
program-oriented structure manipulation, ii) Arti-PG is widely applicable to
diverse tasks by easily providing comprehensive and detailed annotations.
Arti-PG now supports the procedural generation of 26 categories of articulate
objects and provides annotations across a wide range of both vision and
manipulation tasks, and we provide exhaustive experiments which fully
demonstrate its advantages. We will make Arti-PG toolbox publicly available for
the community to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhotoHolmes: a Python library for forgery detection in digital images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julián O'Flaherty, Rodrigo Paganini, Juan Pablo Sotelo, Julieta Umpiérrez, Marina Gardella, Matías Tailanian, Pablo Musé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce PhotoHolmes, an open-source Python library
designed to easily run and benchmark forgery detection methods on digital
images. The library includes implementations of popular and state-of-the-art
methods, dataset integration tools, and evaluation metrics. Utilizing the
Benchmark tool in PhotoHolmes, users can effortlessly compare various methods.
This facilitates an accurate and reproducible comparison between their own
methods and those in the existing literature. Furthermore, PhotoHolmes includes
a command-line interface (CLI) to easily run the methods implemented in the
library on any suspicious image. As such, image forgery methods become more
accessible to the community. The library has been built with extensibility and
modularity in mind, which makes adding new methods, datasets and metrics to the
library a straightforward process. The source code is available at
https://github.com/photoholmes/photoholmes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movie2Story: A framework for understanding videos and telling stories in
  the form of novel text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Li, Zheyang Jia, Anyu Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal video-to-text models have made considerable progress, primarily in
generating brief descriptions of video content. However, there is still a
deficiency in generating rich long-form text descriptions that integrate both
video and audio. In this paper, we introduce a framework called M2S, designed
to generate novel-length text by combining audio, video, and character
recognition. M2S includes modules for video long-form text description and
comprehension, audio-based analysis of emotion, speech rate, and character
alignment, and visual-based character recognition alignment. By integrating
multimodal information using the large language model GPT4o, M2S stands out in
the field of multimodal text generation. We demonstrate the effectiveness and
accuracy of M2S through comparative experiments and human evaluation.
Additionally, the model framework has good scalability and significant
potential for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDOL: Instant Photorealistic 3D Human Creation from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating a high-fidelity, animatable 3D full-body avatar from a single image
is a challenging task due to the diverse appearance and poses of humans and the
limited availability of high-quality training data. To achieve fast and
high-quality human reconstruction, this work rethinks the task from the
perspectives of dataset, model, and representation. First, we introduce a
large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K
diverse, photorealistic sets of human images. Each set contains 24-view frames
in specific human poses, generated using a pose-controllable
image-to-multi-view model. Next, leveraging the diversity in views, poses, and
appearances within HuGe100K, we develop a scalable feed-forward transformer
model to predict a 3D human Gaussian representation in a uniform space from a
given human image. This model is trained to disentangle human pose, body shape,
clothing geometry, and texture. The estimated Gaussians can be animated without
post-processing. We conduct comprehensive experiments to validate the
effectiveness of the proposed dataset and method. Our model demonstrates the
ability to efficiently reconstruct photorealistic humans at 1K resolution from
a single input image using a single GPU instantly. Additionally, it seamlessly
supports various applications, as well as shape and texture editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures, includes main content, supplementary materials,
  and references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TDCNet: Transparent Objects Depth Completion with CNN-<span class="highlight-title">Transformer</span>
  Dual-Branch Parallel Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghui Fan, Chao Ye, Anping Deng, Xiaotian Wu, Mengyang Pan, Hang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sensing and manipulation of transparent objects present a critical
challenge in industrial and laboratory robotics. Conventional sensors face
challenges in obtaining the full depth of transparent objects due to the
refraction and reflection of light on their surfaces and their lack of visible
texture. Previous research has attempted to obtain complete depth maps of
transparent objects from RGB and damaged depth maps (collected by depth sensor)
using deep learning models. However, existing methods fail to fully utilize the
original depth map, resulting in limited accuracy for deep completion. To solve
this problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel
network for transparent object depth completion. The proposed framework
consists of two different branches: one extracts features from partial depth
maps, while the other processes RGB-D images. Experimental results demonstrate
that our model achieves state-of-the-art performance across multiple public
datasets. Our code and the pre-trained model are publicly available at
https://github.com/XianghuiFan/TDCNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dream to Manipulate: Compositional World Models Empowering Robot
  Imitation Learning with Imagination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A world model provides an agent with a representation of its environment,
enabling it to predict the causal consequences of its actions. Current world
models typically cannot directly and explicitly imitate the actual environment
in front of a robot, often resulting in unrealistic behaviors and
hallucinations that make them unsuitable for real-world applications. In this
paper, we introduce a new paradigm for constructing world models that are
explicit representations of the real world and its dynamics. By integrating
cutting-edge advances in real-time photorealism with Gaussian Splatting and
physics simulators, we propose the first compositional manipulation world
model, which we call DreMa. DreMa replicates the observed world and its
dynamics, allowing it to imagine novel configurations of objects and predict
the future consequences of robot actions. We leverage this capability to
generate new data for imitation learning by applying equivariant
transformations to a small set of demonstrations. Our evaluations across
various settings demonstrate significant improvements in both accuracy and
robustness by incrementing actions and object distributions, reducing the data
needed to learn a policy and improving the generalization of the agents. As a
highlight, we show that a real Franka Emika Panda robot, powered by DreMa's
imagination, can successfully learn novel physical tasks from just a single
example per task variation (one-shot policy learning). Our project page and
source code can be found in https://leobarcellona.github.io/DreamToManipulate/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corn Ear Detection and Orientation Estimation Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Sprague, John Evans, Michael Mardikes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring growth behavior of maize plants such as the development of ears
can give key insights into the plant's health and development. Traditionally,
the measurement of the angle of ears is performed manually, which can be
time-consuming and prone to human error. To address these challenges, this
paper presents a computer vision-based system for detecting and tracking ears
of corn in an image sequence. The proposed system could accurately detect,
track, and predict the ear's orientation, which can be useful in monitoring
their growth behavior. This can significantly save time compared to manual
measurement and enables additional areas of ear orientation research and
potential increase in efficiencies for maize production. Using an object
detector with keypoint detection, the algorithm proposed could detect 90
percent of all ears. The cardinal estimation had a mean absolute error (MAE) of
18 degrees, compared to a mean 15 degree difference between two people
measuring by hand. These results demonstrate the feasibility of using computer
vision techniques for monitoring maize growth and can lead to further research
in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages;15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface
  Reconstruction <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesong Yang, Ru Zhang, Jiale Shi, Zixiang Ai, Boming Zhao, Hujun Bao, Luwei Yang, Zhaopeng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surface representation has demonstrated remarkable success in the
areas of novel view synthesis and 3D reconstruction. However, assessing the
geometric quality of 3D reconstructions in the absence of ground truth mesh
remains a significant challenge, due to its rendering-based optimization
process and entangled learning of appearance and geometry with photometric
losses. In this paper, we present a novel framework, i.e, GURecon, which
establishes a geometric uncertainty field for the neural surface based on
geometric consistency. Different from existing methods that rely on
rendering-based measurement, GURecon models a continuous 3D uncertainty field
for the reconstructed surface, and is learned by an online distillation
approach without introducing real geometric information for supervision.
Moreover, in order to mitigate the interference of illumination on geometric
consistency, a decoupled field is learned and exploited to finetune the
uncertainty field. Experiments on various datasets demonstrate the superiority
of GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play
extension to various neural surface representations and improvement on
downstream tasks such as incremental reconstruction. The code and supplementary
material are available on the project website:
https://zju3dv.github.io/GURecon/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025. Project page:
  https://zju3dv.github.io/gurecon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Spectral Calibration of Hyperspectral Images:Method, <span class="highlight-title">Dataset</span>
  and Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Du, Shaodi You, Cheng Cheng, Shikui Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) densely samples the world in both the space and
frequency domain and therefore is more distinctive than RGB images. Usually,
HSI needs to be calibrated to minimize the impact of various illumination
conditions. The traditional way to calibrate HSI utilizes a physical reference,
which involves manual operations, occlusions, and/or limits camera mobility.
These limitations inspire this paper to automatically calibrate HSIs using a
learning-based method. Towards this goal, a large-scale HSI calibration dataset
is created, which has 765 high-quality HSI pairs covering diversified natural
scenes and illuminations. The dataset is further expanded to 7650 pairs by
combining with 10 different physically measured illuminations. A spectral
illumination transformer (SIT) together with an illumination attention module
is proposed. Extensive benchmarks demonstrate the SoTA performance of the
proposed SIT. The benchmarks also indicate that low-light conditions are more
challenging than normal conditions. The dataset and codes are available
online:https://github.com/duranze/Automatic-spectral-calibration-of-HSI
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicNaming: Consistent Identity Generation by Finding a "Name Space" in
  T2I Diffusion Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Hunag, Yuhua Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable
of generating famous persons by simply referring to their names. Is it possible
to make such models generate generic identities as simple as the famous ones,
e.g., just use a name? In this paper, we explore the existence of a "Name
Space", where any point in the space corresponds to a specific identity.
Fortunately, we find some clues in the feature space spanned by text embedding
of celebrities' names. Specifically, we first extract the embeddings of
celebrities' names in the Laion5B dataset with the text encoder of diffusion
models. Such embeddings are used as supervision to learn an encoder that can
predict the name (actually an embedding) of a given face image. We
experimentally find that such name embeddings work well in promising the
generated image with good identity consistency. Note that like the names of
celebrities, our predicted name embeddings are disentangled from the semantics
of text inputs, making the original generation capability of text-to-image
models well-preserved. Moreover, by simply plugging such name embeddings, all
variants (e.g., from Civitai) derived from the same base model (i.e., SDXL)
readily become identity-aware text-to-image models. Project homepage:
\url{https://magicfusion.github.io/MagicNaming/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Hypothetical Summary for Retrieval-based Multi-image Question
  Answering <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peize Li, Qingyi Si, Peng Fu, Zheng Lin, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-based multi-image question answering (QA) task involves retrieving
multiple question-related images and synthesizing these images to generate an
answer. Conventional "retrieve-then-answer" pipelines often suffer from
cascading errors because the training objective of QA fails to optimize the
retrieval stage. To address this issue, we propose a novel method to
effectively introduce and reference retrieved information into the QA. Given
the image set to be retrieved, we employ a multimodal large language model
(visual perspective) and a large language model (textual perspective) to obtain
multimodal hypothetical summary in question-form and description-form. By
combining visual and textual perspectives, MHyS captures image content more
specifically and replaces real images in retrieval, which eliminates the
modality gap by transforming into text-to-text retrieval and helps improve
retrieval. To more advantageously introduce retrieval with QA, we employ
contrastive learning to align queries (questions) with MHyS. Moreover, we
propose a coarse-to-fine strategy for calculating both sentence-level and
word-level similarity scores, to further enhance retrieval and filter out
irrelevant details. Our approach achieves a 3.7% absolute improvement over
state-of-the-art methods on RETVQA and a 14.5% improvement over CLIP.
Comprehensive experiments and detailed ablation studies demonstrate the
superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Artifact2Artifact: Self-incentive artifact removal for
  photoacoustic imaging without any data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Li, Qian Chen, Chulhong Kim, Seongwook Choi, Yibing Wang, Yu Zhang, Changhui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoacoustic imaging (PAI) uniquely combines optical contrast with the
penetration depth of ultrasound, making it critical for clinical applications.
However, the quality of 3D PAI is often degraded due to reconstruction
artifacts caused by the sparse and angle-limited configuration of detector
arrays. Existing iterative or deep learning-based methods are either
time-consuming or require large training datasets, significantly limiting their
practical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a
zero-shot self-supervised artifact removal method based on a super-lightweight
network, which leverages the fact that reconstruction artifacts are sensitive
to irregularities caused by data loss. By introducing random perturbations to
the acquired PA data, it spontaneously generates subset data, which in turn
stimulates the network to learn the artifact patterns in the reconstruction
results, thus enabling zero-shot artifact removal. This approach requires
neither training data nor prior knowledge of the artifacts, and is capable of
artifact removal for 3D PAI. For maximum amplitude projection (MAP) images or
slice images in 3D PAI acquired with arbitrarily sparse or angle-limited
detector arrays, ZS-A2A employs a self-incentive strategy to complete artifact
removal and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in
both simulation study and $ in\ vivo $ animal experiments. Results demonstrate
that ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing
zero-shot methods, and for the $ in\ vivo $ rat liver, ZS-A2A improves CNR from
17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in
the following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-scale School Mapping using Weakly Supervised Deep Learning for
  Universal School Connectivity <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Tingzon, Utku Can Ozturk, Ivan Dotu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving global school connectivity is critical for ensuring inclusive and
equitable quality education. To reliably estimate the cost of connecting
schools, governments and connectivity providers require complete and accurate
school location data - a resource that is often scarce in many low- and
middle-income countries. To address this challenge, we propose a
cost-effective, scalable approach to locating schools in high-resolution
satellite images using weakly supervised deep learning techniques. Our best
models, which combine vision transformers and convolutional neural networks,
achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging
explainable AI techniques, our approach can approximate the precise
geographical coordinates of the school locations using only low-cost,
classification-level annotations. To demonstrate the scalability of our method,
we generate nationwide maps of school location predictions in African countries
and present a detailed analysis of our results, using Senegal as our case
study. Finally, we demonstrate the immediate usability of our work by
introducing an interactive web mapping tool to streamline human-in-the-loop
model validation efforts by government partners. This work successfully
showcases the real-world utility of deep learning and satellite images for
planning regional infrastructure and accelerating universal school
connectivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI-25 Special Track on AI for Social Impact (AISI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional
  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature
  Screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood
within the skull, which occurs due to the rupture of blood vessels in or around
the brain. If this condition is not diagnosed in a timely manner and
appropriately treated, it can lead to serious complications such as decreased
consciousness, permanent neurological disabilities, or even death.The primary
aim of this study is to detect the occurrence or non-occurrence of ICH,
followed by determining the type of subdural hemorrhage (SDH). These tasks are
framed as two separate binary classification problems. By adding two layers to
the co-scale convolutional attention (CCA) classifier architecture, we
introduce a novel approach for ICH detection. In the first layer, after
extracting features from different slices of computed tomography (CT) scan
images, we combine these features and select the 50 components that capture the
highest variance in the data, considering them as informative features. We then
assess the discriminative power of these features using the bootstrap forest
algorithm, discarding those that lack sufficient discriminative ability between
different classes. This algorithm explicitly determines the contribution of
each feature to the final prediction, assisting us in developing an explainable
AI model. The features feed into a boosting neural network as a latent feature
space. In the second layer, we introduce a novel uncertainty-based fuzzy
integral operator to fuse information from different CT scan slices. This
operator, by accounting for the dependencies between consecutive slices,
significantly improves detection accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy
  with <span class="highlight-title">Pre-train</span>ing, Data Augmentation and Dual Flow UNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Head and neck tumors and metastatic lymph nodes are crucial for treatment
planning and prognostic analysis. Accurate segmentation and quantitative
analysis of these structures require pixel-level annotation, making automated
segmentation techniques essential for the diagnosis and treatment of head and
neck cancer. In this study, we investigated the effects of multiple strategies
on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)
images. For the segmentation of pre-RT images, we utilized: 1) a fully
supervised learning approach, and 2) the same approach enhanced with
pre-trained weights and the MixUp data augmentation technique. For mid-RT
images, we introduced a novel computational-friendly network architecture that
features separate encoders for mid-RT images and registered pre-RT images with
their labels. The mid-RT encoder branch integrates information from pre-RT
images and labels progressively during the forward propagation. We selected the
highest-performing model from each fold and used their predictions to create an
ensemble average for inference. In the final test, our models achieved a
segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on
aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at
https://github.com/WltyBY/HNTS-MRG2024_train_code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging
  Scenes with Subtly Distinguished Objects <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihang Cao, Huangxun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D scene understanding is an important task, and there has been a recent
surge of research interest in aligning 3D representations of point clouds with
text to empower embodied AI. However, due to the lack of comprehensive 3D
benchmarks, the capabilities of 3D models in real-world scenes, particularly
those that are challenging with subtly distinguished objects, remain
insufficiently investigated. To facilitate a more thorough evaluation of 3D
models' capabilities, we propose a scheme, ObjVariantEnsemble, to
systematically introduce more scenes with specified object classes, colors,
shapes, quantities, and spatial relationships to meet model evaluation needs.
More importantly, we intentionally construct scenes with similar objects to a
certain degree and design an LLM-VLM-cooperated annotator to capture key
distinctions as annotations. The resultant benchmark can better challenge 3D
models, reveal their shortcomings in understanding, and potentially aid in the
further development of 3D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Multimodal Reasoning via Active Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step multimodal reasoning tasks pose significant challenges for
multimodal large language models (MLLMs), and finding effective ways to enhance
their performance in such scenarios remains an unresolved issue. In this paper,
we propose AR-MCTS, a universal framework designed to progressively improve the
reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo
Tree Search (MCTS). Our approach begins with the development of a unified
retrieval module that retrieves key supporting insights for solving complex
reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in
automated multimodal reasoning verification, we employ the MCTS algorithm
combined with an active retrieval mechanism, which enables the automatic
generation of step-wise annotations. This strategy dynamically retrieves key
insights for each reasoning step, moving beyond traditional beam search
sampling to improve the diversity and reliability of the reasoning space.
Additionally, we introduce a process reward model that aligns progressively to
support the automatic verification of multimodal reasoning tasks. Experimental
results across three complex multimodal reasoning benchmarks confirm the
effectiveness of the AR-MCTS framework in enhancing the performance of various
multimodal models. Further analysis demonstrates that AR-MCTS can optimize
sampling diversity and accuracy, yielding reliable multimodal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Huang, Yujie Lin, Siyu Chen, Haiyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action recognition using GCNs has achieved remarkable
performance, but recognizing ambiguous actions, such as "waving" and
"saluting", remains a significant challenge. Existing methods typically rely on
a serial combination of GCNs and TCNs, where spatial and temporal features are
extracted independently, leading to an unbalanced spatial-temporal information,
which hinders accurate action recognition. Moreover, existing methods for
ambiguous actions often overemphasize local details, resulting in the loss of
crucial global context, which further complicates the task of differentiating
ambiguous actions. To address these challenges, we propose a lightweight
plug-and-play module called Synchronized and Fine-grained Head (SF-Head),
inserted between GCN and TCN layers. SF-Head first conducts Synchronized
Spatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL),
ensuring a balanced interaction between the two types of features. It then
performs Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature
Consistency Loss (F-CL), which aligns the aggregated feature with their
original spatial-temporal feature. This aggregation step effectively combines
both global context and local details. Experimental results on NTU RGB+D 60,
NTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in
distinguishing ambiguous actions. Our code will be made available at
https://github.com/HaoHuang2003/SFHead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR
  Semantic Segmentation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoumeng Qiu, Xinrun Li, XiangYang Xue, Jian Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although multiview fusion has demonstrated potential in LiDAR segmentation,
its dependence on computationally intensive point-based interactions, arising
from the lack of fixed correspondences between views such as range view and
Bird's-Eye View (BEV), hinders its practical deployment. This paper challenges
the prevailing notion that multiview fusion is essential for achieving high
performance. We demonstrate that significant gains can be realized by directly
fusing Polar and Cartesian partitioning strategies within the BEV space. Our
proposed BEV-only segmentation model leverages the inherent fixed grid
correspondences between these partitioning schemes, enabling a fusion process
that is orders of magnitude faster (170$\times$ speedup) than conventional
point-based methods. Furthermore, our approach facilitates dense feature
fusion, preserving richer contextual information compared to sparse point-based
alternatives. To enhance scene understanding while maintaining inference
efficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive
evaluation on the SemanticKITTI and nuScenes datasets provides compelling
evidence that our method outperforms previous multiview fusion approaches in
terms of both performance and inference speed, highlighting the potential of
BEV-based fusion for LiDAR segmentation. Code is available at
\url{https://github.com/skyshoumeng/PC-BEV.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Level Embedding and Alignment Network with Consistency and
  Invariance Learning for Cross-View Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-View Geo-Localization (CVGL) involves determining the localization of
drone images by retrieving the most similar GPS-tagged satellite images.
However, the imaging gaps between platforms are often significant and the
variations in viewpoints are substantial, which limits the ability of existing
methods to effectively associate cross-view features and extract consistent and
invariant characteristics. Moreover, existing methods often overlook the
problem of increased computational and storage requirements when improving
model performance. To handle these limitations, we propose a lightweight
enhanced alignment network, called the Multi-Level Embedding and Alignment
Network (MEAN). The MEAN network uses a progressive multi-level enhancement
strategy, global-to-local associations, and cross-domain alignment, enabling
feature communication across levels. This allows MEAN to effectively connect
features at different levels and learn robust cross-view consistent mappings
and modality-invariant features. Moreover, MEAN adopts a shallow backbone
network combined with a lightweight branch design, effectively reducing
parameter count and computational complexity. Experimental results on the
University-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter
count by 62.17% and computational complexity by 70.99% compared to
state-of-the-art models, while maintaining competitive or even superior
performance. The codes will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Tampered Text Detection via Multimodal Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenfan Qu, Jian Liu, Haoxing Chen, Baihan Yu, Jingjing Liu, Weiqiang Wang, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, tampered text detection has attracted increasing attention due to
its essential role in information security. Although existing methods can
detect the tampered text region, the interpretation of such detection remains
unclear, making the prediction unreliable. To address this black-box problem,
we propose to explain the basis of tampered text detection with natural
language via large multimodal models. To fill the data gap for this task, we
propose a large-scale, comprehensive dataset, ETTD, which contains both
pixel-level annotations indicating the tampered text region and natural
language annotations describing the anomaly of the tampered text. Multiple
methods are employed to improve the quality of the proposed data. For example,
a fused mask prompt is proposed to reduce confusion when querying GPT4o to
generate anomaly descriptions. By weighting the input image with the mask
annotation, the tampered region can be clearly indicated and the content in and
around the tampered region can also be preserved. We also propose prompting
GPT4o to recognize tampered texts and filtering out the responses with low OCR
accuracy, which can effectively improve annotation quality in an automatic
manner. To further improve explainable tampered text detection, we propose a
simple yet effective model called TTD, which benefits from improved
fine-grained perception by paying attention to the suspected region with
auxiliary reference grounding query. Extensive experiments on both the ETTD
dataset and the public dataset have verified the effectiveness of the proposed
methods. In-depth analysis is also provided to inspire further research. The
dataset and code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first work for explainable tampered text detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Prediction Policy: A Generalist Robot Policy with Predictive
  Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in robotics have focused on developing generalist
policies capable of performing multiple tasks. Typically, these policies
utilize pre-trained vision encoders to capture crucial information from current
observations. However, previous vision encoders, which trained on two-image
contrastive learning or single-image reconstruction, can not perfectly capture
the sequential information essential for embodied tasks. Recently, video
diffusion models (VDMs) have demonstrated the capability to accurately predict
future image sequences, exhibiting a good understanding of physical dynamics.
Motivated by the strong visual prediction capabilities of VDMs, we hypothesize
that they inherently possess visual representations that reflect the evolution
of the physical world, which we term predictive visual representations.
Building on this hypothesis, we propose the Video Prediction Policy (VPP), a
generalist robotic policy conditioned on the predictive visual representations
from VDMs. To further enhance these representations, we incorporate diverse
human or robotic manipulation datasets, employing unified video-generation
training objectives. VPP consistently outperforms existing methods across two
simulated and two real-world benchmarks. Notably, it achieves a 28.1\% relative
improvement in the Calvin ABC-D benchmark compared to the previous
state-of-the-art and delivers a 28.8\% increase in success rates for complex
real-world dexterous manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally. Project Page at
  https://video-prediction-policy.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLOv11 Optimization for Efficient Resource Utilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Areeg Fagad Rasheed, M. Zarkoosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this research is to optimize the eleventh iteration of You
Only Look Once (YOLOv11) by developing size-specific modified versions of the
architecture. These modifications involve pruning unnecessary layers and
reconfiguring the main architecture of YOLOv11. Each proposed version is
tailored to detect objects of specific size ranges, from small to large. To
ensure proper model selection based on dataset characteristics, we introduced
an object classifier program. This program identifies the most suitable
modified version for a given dataset. The proposed models were evaluated on
various datasets and compared with the original YOLOv11 and YOLOv8 models. The
experimental results highlight significant improvements in computational
resource efficiency, with the proposed models maintaining the accuracy of the
original YOLOv11. In some cases, the modified versions outperformed the
original model regarding detection performance. Furthermore, the proposed
models demonstrated reduced model sizes and faster inference times. Models
weights and the object size classifier can be found in this repository
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 13 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal
  Keypoint <span class="highlight-title">Transformer</span>s for Pedestrian Fall Detection in Smart Cities <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonghun Kim, Byeongjoon Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In smart cities, detecting pedestrian falls is a major challenge to ensure
the safety and quality of life of citizens. In this study, we propose a novel
fall detection system using FLAMe (Federated Learning with Attention
Mechanism), a federated learning (FL) based algorithm. FLAMe trains around
important keypoint information and only transmits the trained important weights
to the server, reducing communication costs and preserving data privacy.
Furthermore, the lightweight keypoint transformer model is integrated into the
FL framework to effectively learn spatio-temporal features. We validated the
experiment using 22,672 video samples from the "Fall Accident Risk Behavior
Video-Sensor Pair data" dataset from AI-Hub. As a result of the experiment, the
FLAMe-based system achieved an accuracy of 94.02% with about 190,000
transmission parameters, maintaining performance similar to that of existing
centralized learning while maximizing efficiency by reducing communication
costs by about 40% compared to the existing FL algorithm, FedAvg. Therefore,
the FLAMe algorithm has demonstrated that it provides robust performance in the
distributed environment of smart cities and is a practical and effective
solution for public safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, AAAI 2025 FLUID Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Li, Dan Guo, Guoliang Chen, Chunxiao Fan, Jingyuan Xu, Zhiliang Wu, Hehe Fan, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro-Action Recognition (MAR) has gained increasing attention due to its
crucial role as a form of non-verbal communication in social interactions, with
promising potential for applications in human communication and emotion
analysis. However, current approaches often overlook the inherent ambiguity in
micro-actions, which arises from the wide category range and subtle visual
differences between categories. This oversight hampers the accuracy of
micro-action recognition. In this paper, we propose a novel Prototypical
Calibrating Ambiguous Network (\textbf{PCAN}) to unleash and mitigate the
ambiguity of MAR. \textbf{Firstly}, we employ a hierarchical action-tree to
identify the ambiguous sample, categorizing them into distinct sets of
ambiguous samples of false negatives and false positives, considering both
body- and action-level categories. \textbf{Secondly}, we implement an ambiguous
contrastive refinement module to calibrate these ambiguous samples by
regulating the distance between ambiguous samples and their corresponding
prototypes. This calibration process aims to pull false negative
($\mathbb{FN}$) samples closer to their respective prototypes and push false
positive ($\mathbb{FP}$) samples apart from their affiliated prototypes. In
addition, we propose a new prototypical diversity amplification loss to
strengthen the model's capacity by amplifying the differences between different
prototypes. \textbf{Finally}, we propose a prototype-guided rectification to
rectify prediction by incorporating the representability of prototypes.
Extensive experiments conducted on the benchmark dataset demonstrate the
superior performance of our method compared to existing approaches. The code is
available at https://github.com/kunli-cs/PCAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnergyMoGen: Compositional Human Motion Generation with Energy-Based
  Diffusion Model in Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrong Zhang, Hehe Fan, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, particularly latent diffusion models, have demonstrated
remarkable success in text-driven human motion generation. However, it remains
challenging for latent diffusion models to effectively compose multiple
semantic concepts into a single, coherent motion sequence. To address this
issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based
Models: (1) We interpret the diffusion model as a latent-aware energy-based
model that generates motions by composing a set of diffusion models in latent
space; (2) We introduce a semantic-aware energy model based on cross-attention,
which enables semantic composition and adaptive gradient descent for text
embeddings. To overcome the challenges of semantic inconsistency and motion
distortion across these two spectrums, we introduce Synergistic Energy Fusion.
This design allows the motion latent diffusion model to synthesize
high-quality, complex motions by combining multiple energy terms corresponding
to textual descriptions. Experiments show that our approach outperforms
existing state-of-the-art models on various motion generation tasks, including
text-to-motion generation, compositional motion generation, and multi-concept
motion generation. Additionally, we demonstrate that our method can be used to
extend motion datasets and improve the text-to-motion task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jiro-zhang.github.io/EnergyMoGen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-assisted 12-stop HDR Imaging of Dynamic Scene 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Guo, Zixuan Chen, Ziran Zhang, Yutian Chen, Gangwei Xu, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High dynamic range (HDR) imaging is a crucial task in computational
photography, which captures details across diverse lighting conditions.
Traditional HDR fusion methods face limitations in dynamic scenes with extreme
exposure differences, as aligning low dynamic range (LDR) frames becomes
challenging due to motion and brightness variation. In this work, we propose a
novel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera
system with an event camera and an RGB camera. The event camera provides
temporally dense, high dynamic range signals that improve alignment between LDR
frames with large exposure differences, reducing ghosting artifacts caused by
motion. Also, a real-world finetuning strategy is proposed to increase the
generalization of alignment module on real-world events. Additionally, we
introduce a diffusion-based fusion module that incorporates image priors from
pre-trained diffusion models to address artifacts in high-contrast regions and
minimize errors from the alignment process. To support this work, we developed
the ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized
event signals, and validated our approach on both simulated and real-world
data. Extensive experiments demonstrate that our method achieves
state-of-the-art performance, successfully extending HDR imaging to 12 stops in
dynamic scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://openimaginglab.github.io/Event-Assisted-12stops-HDR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explicit Relational Reasoning Network for Scene Text Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Su, Zhineng Chen, Yongkun Du, Zhilong Ji, Kai Hu, Jinfeng Bai, Xieping Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected component (CC) is a proper text shape representation that aligns
with human reading intuition. However, CC-based text detection methods have
recently faced a developmental bottleneck that their time-consuming
post-processing is difficult to eliminate. To address this issue, we introduce
an explicit relational reasoning network (ERRNet) to elegantly model the
component relationships without post-processing. Concretely, we first represent
each text instance as multiple ordered text components, and then treat these
components as objects in sequential movement. In this way, scene text detection
can be innovatively viewed as a tracking problem. From this perspective, we
design an end-to-end tracking decoder to achieve a CC-based method dispensing
with post-processing entirely. Additionally, we observe that there is an
inconsistency between classification confidence and localization quality, so we
propose a Polygon Monte-Carlo method to quickly and accurately evaluate the
localization quality. Based on this, we introduce a position-supervised
classification loss to guide the task-aligned learning of ERRNet. Experiments
on challenging benchmarks demonstrate the effectiveness of our ERRNet. It
consistently achieves state-of-the-art accuracy while holding highly
competitive inference speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Light-Weight Framework for Open-Set Object Detection with Decoupled
  Feature Alignment in Joint Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao He, Hu Su, Haiyong Yu, Cong Yang, Wei Sui, Cong Wang, Song Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set object detection (OSOD) is highly desirable for robotic manipulation
in unstructured environments. However, existing OSOD methods often fail to meet
the requirements of robotic applications due to their high computational burden
and complex deployment. To address this issue, this paper proposes a
light-weight framework called Decoupled OSOD (DOSOD), which is a practical and
highly efficient solution to support real-time OSOD tasks in robotic systems.
Specifically, DOSOD builds upon the YOLO-World pipeline by integrating a
vision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)
adaptor is developed to transform text embeddings extracted by the VLM into a
joint space, within which the detector learns the region representations of
class-agnostic proposals. Cross-modality features are directly aligned in the
joint space, avoiding the complex feature interactions and thereby improving
computational efficiency. DOSOD operates like a traditional closed-set detector
during the testing phase, effectively bridging the gap between closed-set and
open-set detection. Compared to the baseline YOLO-World, the proposed DOSOD
significantly enhances real-time performance while maintaining comparable
accuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\%$, compared to
$26.2\%$ for YOLO-World-v1-S and $22.7\%$ for YOLO-World-v2-S, using similar
backbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is
$57.1\%$ higher than YOLO-World-v1-S and $29.6\%$ higher than YOLO-World-v2-S.
Meanwhile, we demonstrate that the DOSOD model facilitates the deployment of
edge devices. The codes and models are publicly available at
https://github.com/D-Robotics-AI-Lab/DOSOD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Few-Shot Neural Architecture Search by Counting the Number of
  Nonlinear Functions <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngmin Oh, Hyunju Lee, Bumsub Ham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural architecture search (NAS) enables finding the best-performing
architecture from a search space automatically. Most NAS methods exploit an
over-parameterized network (i.e., a supernet) containing all possible
architectures (i.e., subnets) in the search space. However, the subnets that
share the same set of parameters are likely to have different characteristics,
interfering with each other during training. To address this, few-shot NAS
methods have been proposed that divide the space into a few subspaces and
employ a separate supernet for each subspace to limit the extent of weight
sharing. They achieve state-of-the-art performance, but the computational cost
increases accordingly. We introduce in this paper a novel few-shot NAS method
that exploits the number of nonlinear functions to split the search space. To
be specific, our method divides the space such that each subspace consists of
subnets with the same number of nonlinear functions. Our splitting criterion is
efficient, since it does not require comparing gradients of a supernet to split
the space. In addition, we have found that dividing the space allows us to
reduce the channel dimensions required for each supernet, which enables
training multiple supernets in an efficient manner. We also introduce a
supernet-balanced sampling (SBS) technique, sampling several subnets at each
training step, to train different supernets evenly within a limited number of
training steps. Extensive experiments on standard NAS benchmarks demonstrate
the effectiveness of our approach. Our code is available at
https://cvlab.yonsei.ac.kr/projects/EFS-NAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FiVL: A Framework for Improved Vision-Language Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision Language Models (LVLMs) have achieved significant progress in
integrating visual and textual inputs for multimodal reasoning. However, a
recurring challenge is ensuring these models utilize visual information as
effectively as linguistic content when both modalities are necessary to
formulate an accurate answer. We hypothesize that hallucinations arise due to
the lack of effective visual grounding in current LVLMs. This issue extends to
vision-language benchmarks, where it is difficult to make the image
indispensable for accurate answer generation, particularly in vision
question-answering tasks. In this work, we introduce FiVL, a novel method for
constructing datasets designed to train LVLMs for enhanced visual grounding and
to evaluate their effectiveness in achieving it. These datasets can be utilized
for both training and assessing an LVLM's ability to use image content as
substantive evidence rather than relying solely on linguistic priors, providing
insights into the model's reliance on visual information. To demonstrate the
utility of our dataset, we introduce an innovative training task that
outperforms baselines alongside a validation method and application for
explainability. The code is available at https://github.com/IntelLabs/fivl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUSTER: Longitudinal Deformable Registration by Composition of
  Consecutive Deformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edvard O. S. Grødem, Donatas Sederevičius, Esten H. Leonardsen, Bradley J. MacIntosh, Atle Bjørnerud, Till Schellhorn, Øystein Sørensen, Inge Amlien, Pablo F. Garrido, Anders M. Fjell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Longitudinal imaging allows for the study of structural changes over time.
One approach to detecting such changes is by non-linear image registration.
This study introduces Multi-Session Temporal Registration (MUSTER), a novel
method that facilitates longitudinal analysis of changes in extended series of
medical images. MUSTER improves upon conventional pairwise registration by
incorporating more than two imaging sessions to recover longitudinal
deformations. Longitudinal analysis at a voxel-level is challenging due to
effects of a changing image contrast as well as instrumental and environmental
sources of bias between sessions. We show that local normalized
cross-correlation as an image similarity metric leads to biased results and
propose a robust alternative. We test the performance of MUSTER on a synthetic
multi-site, multi-session neuroimaging dataset and show that, in various
scenarios, using MUSTER significantly enhances the estimated deformations
relative to pairwise registration. Additionally, we apply MUSTER on a sample of
older adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.
The results show that MUSTER can effectively identify patterns of
neuro-degeneration from T1-weighted images and that these changes correlate
with changes in cognition, matching the performance of state of the art
segmentation methods. By leveraging GPU acceleration, MUSTER efficiently
handles large datasets, making it feasible also in situations with limited
computational resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Uncertainty: A Deep Dive into Calibration and Performance of
  Multimodal Large Language Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) combine visual and textual data for
tasks such as image captioning and visual question answering. Proper
uncertainty calibration is crucial, yet challenging, for reliable use in areas
like healthcare and autonomous driving. This paper investigates representative
MLLMs, focusing on their calibration across various scenarios, including before
and after visual fine-tuning, as well as before and after multimodal training
of the base LLMs. We observed miscalibration in their performance, and at the
same time, no significant differences in calibration across these scenarios. We
also highlight how uncertainty differs between text and images and how their
integration affects overall uncertainty. To better understand MLLMs'
miscalibration and their ability to self-assess uncertainty, we construct the
IDK (I don't know) dataset, which is key to evaluating how they handle
unknowns. Our findings reveal that MLLMs tend to give answers rather than admit
uncertainty, but this self-assessment improves with proper prompt adjustments.
Finally, to calibrate MLLMs and enhance model reliability, we propose
techniques such as temperature scaling and iterative prompt optimization. Our
results provide insights into improving MLLMs for effective and responsible
deployment in multimodal applications. Code and IDK dataset:
\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RefHCM: A Unified Model for Referring Perceptions in Human-Centric
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Huang, Ruibing Hou, Jiahe Zhao, Hong Chang, Shiguang Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-centric perceptions play a crucial role in real-world applications.
While recent human-centric works have achieved impressive progress, these
efforts are often constrained to the visual domain and lack interaction with
human instructions, limiting their applicability in broader scenarios such as
chatbots and sports analysis. This paper introduces Referring Human
Perceptions, where a referring prompt specifies the person of interest in an
image. To tackle the new task, we propose RefHCM (Referring Human-Centric
Model), a unified framework to integrate a wide range of human-centric
referring tasks. Specifically, RefHCM employs sequence mergers to convert raw
multimodal data -- including images, text, coordinates, and parsing maps --
into semantic tokens. This standardized representation enables RefHCM to
reformulate diverse human-centric referring tasks into a sequence-to-sequence
paradigm, solved using a plain encoder-decoder transformer architecture.
Benefiting from a unified learning strategy, RefHCM effectively facilitates
knowledge transfer across tasks and exhibits unforeseen capabilities in
handling complex reasoning. This work represents the first attempt to address
referring human perceptions with a general-purpose framework, while
simultaneously establishing a corresponding benchmark that sets new standards
for the field. Extensive experiments showcase RefHCM's competitive and even
superior performance across multiple human-centric referring tasks. The code
and data are publicly at https://github.com/JJJYmmm/RefHCM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Prompt</span> Tuning: Vision Guided <span class="highlight-title">Prompt</span> Tuning with Cross-Attention
  for Fine-Grained Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot, fine-grained classification in computer vision poses significant
challenges due to the need to differentiate subtle class distinctions with
limited data. This paper presents a novel method that enhances the Contrastive
Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided
by real-time visual inputs. Unlike existing techniques such as Context
Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by
static prompts or visual token reliance, the proposed approach leverages a
cross-attention mechanism to dynamically refine text prompts for the image at
hand. This enables an image-specific alignment of textual features with image
patches extracted from the Vision Transformer, making the model more effective
for datasets with high intra-class variance and low inter-class differences.
The method is evaluated on several datasets, including CUBirds, Oxford Flowers,
and FGVC Aircraft, showing significant performance gains over static prompt
tuning approaches. To ensure these performance gains translate into trustworthy
predictions, we integrate Monte-Carlo Dropout in our approach to improve the
reliability of the model predictions and uncertainty estimates. This
integration provides valuable insights into the model's predictive confidence,
helping to identify when predictions can be trusted and when additional
verification is necessary. This dynamic approach offers a robust solution,
advancing the state-of-the-art for few-shot fine-grained classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit
  Post-Training Quantization in Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Ding, Liang Yong, Sihuan Zhao, Jing Nie, Lihui Chen, Haijun Liu, Xichuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to its efficiency, Post-Training Quantization (PTQ) has been widely
adopted for compressing Vision Transformers (ViTs). However, when quantized
into low-bit representations, there is often a significant performance drop
compared to their full-precision counterparts. To address this issue,
reconstruction methods have been incorporated into the PTQ framework to improve
performance in low-bit quantization settings. Nevertheless, existing related
methods predefine the reconstruction granularity and seldom explore the
progressive relationships between different reconstruction granularities, which
leads to sub-optimal quantization results in ViTs. To this end, in this paper,
we propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for
accurate PTQ, which significantly improves the performance of low-bit quantized
vision transformers. Specifically, we define multi-head self-attention and
multi-layer perceptron modules along with their shortcuts as the finest
reconstruction units. After reconstructing these two fine-grained units, we
combine them to form coarser blocks and reconstruct them at a coarser
granularity level. We iteratively perform this combination and reconstruction
process, achieving progressive fine-to-coarse reconstruction. Additionally, we
introduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the
difficulty of training, thereby further enhancing model performance.
Experimental results on the ImageNet dataset demonstrate that our proposed
method achieves the best Top-1 accuracy among state-of-the-art methods,
particularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides,
quantization results on the COCO dataset reveal the effectiveness and
generalization of our proposed method on other computer vision tasks like
object detection and instance segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Review</span> of Fruit Tree Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Il-Seok Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fruit tree image segmentation is an essential problem in automating a variety
of agricultural tasks such as phenotyping, harvesting, spraying, and pruning.
Many research papers have proposed a diverse spectrum of solutions suitable to
specific tasks and environments. The review scope of this paper is confined to
the front views of fruit trees and based on 158 relevant papers collected using
a newly designed crawling review method. These papers are systematically
reviewed based on a taxonomy that sequentially considers the method, image,
task, and fruit. This taxonomy will assist readers to intuitively grasp the big
picture of these research activities. Our review reveals that the most
noticeable deficiency of the previous studies was the lack of a versatile
dataset and segmentation model that could be applied to a variety of tasks and
environments. Six important future research tasks are suggested, with the
expectation that these will pave the way to building a versatile tree
segmentation module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Image Restoration and Enhancement: Degradation Calibrated Cycle
  Reconstruction Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minglong Xue, Jinhong He, Shivakumara Palaiahnakote, Mingliang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image restoration and enhancement are pivotal for numerous computer vision
applications, yet unifying these tasks efficiently remains a significant
challenge. Inspired by the iterative refinement capabilities of diffusion
models, we propose CycleRDM, a novel framework designed to unify restoration
and enhancement tasks while achieving high-quality mapping. Specifically,
CycleRDM first learns the mapping relationships among the degraded domain, the
rough normal domain, and the normal domain through a two-stage diffusion
inference process. Subsequently, we transfer the final calibration process to
the wavelet low-frequency domain using discrete wavelet transform, performing
fine-grained calibration from a frequency domain perspective by leveraging
task-specific frequency spaces. To improve restoration quality, we design a
feature gain module for the decomposed wavelet high-frequency domain to
eliminate redundant features. Additionally, we employ multimodal textual
prompts and Fourier transform to drive stable denoising and reduce randomness
during the inference process. After extensive validation, CycleRDM can be
effectively generalized to a wide range of image restoration and enhancement
tasks while requiring only a small number of training samples to be
significantly superior on various benchmarks of reconstruction quality and
perceptual quality. The source code will be available at
https://github.com/hejh8/CycleRDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Li, You-wei Wen, Xu Xiao, Mingchao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Principal Component Analysis (RPCA) is a fundamental technique for
decomposing data into low-rank and sparse components, which plays a critical
role for applications such as image processing and anomaly detection.
Traditional RPCA methods commonly use $\ell_1$ norm regularization to enforce
sparsity, but this approach can introduce bias and result in suboptimal
estimates, particularly in the presence of significant noise or outliers.
Non-convex regularization methods have been proposed to mitigate these
challenges, but they tend to be complex to optimize and sensitive to initial
conditions, leading to potential instability in solutions. To overcome these
challenges, in this paper, we propose a novel RPCA model that integrates
adaptive weighted least squares (AWLS) and low-rank matrix factorization
(LRMF). The model employs a {self-attention-inspired} mechanism in its weight
update process, allowing the weight matrix to dynamically adjust and emphasize
significant components during each iteration. By employing a weighted F-norm
for the sparse component, our method effectively reduces bias while simplifying
the computational process compared to traditional $\ell_1$-norm-based methods.
We use an alternating minimization algorithm, where each subproblem has an
explicit solution, thereby improving computational efficiency. Despite its
simplicity, numerical experiments demonstrate that our method outperforms
existing non-convex regularization approaches, offering superior performance
and stability, as well as enhanced accuracy and robustness in practical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DM) have democratized AI image generation through an
iterative denoising process. Quantization is a major technique to alleviate the
inference cost and reduce the size of DM denoiser networks. However, as
denoisers evolve from variants of convolutional U-Nets toward newer Transformer
architectures, it is of growing importance to understand the quantization
sensitivity of different weight layers, operations and architecture types to
performance. In this work, we address this challenge with Qua$^2$SeDiMo, a
mixed-precision Post-Training Quantization framework that generates explainable
insights on the cost-effectiveness of various model weight quantization methods
for different denoiser operation types and block structures. We leverage these
insights to make high-quality mixed-precision quantization decisions for a
myriad of diffusion models ranging from foundational U-Nets to state-of-the-art
Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,
3.65-bit and 3.7-bit weight quantization on PixArt-${\alpha}$,
PixArt-${\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our
weight-quantization configurations with 6-bit activation quantization and
outperform existing approaches in terms of quantitative metrics and generative
image quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025; version includes supplementary material; 22 Pages, 18
  Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors
  Guided by Facial Recognizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younhun Kim, Myung-Joon Kwon, Wonjun Lee, Changick Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous Deepfake detection methods perform well within their training
domains, but their effectiveness diminishes significantly with new synthesis
techniques. Recent studies have revealed that detection models often create
decision boundaries based on facial identity rather than synthetic artifacts,
resulting in poor performance on cross-domain datasets. To address this
limitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a
novel training method that mitigates facial identity influence using a face
recognizer. Specifically, we first train a face recognizer using the same
backbone as the Deepfake detector. The recognizer is then frozen and employed
during the detector's training to reduce facial identity information. This is
achieved by feeding input images into both the recognizer and the detector, and
minimizing the similarity of their feature embeddings through our Facial
Identity Attenuating loss. This process encourages the detector to generate
embeddings distinct from the recognizer, effectively reducing the impact of
facial identity. Extensive experiments demonstrate that our approach
significantly enhances detection performance on both in-domain and cross-domain
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures. In 2024 IEEE International Conference on Visual
  Communications and Image Processing (VCIP) Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pitfalls of topology-aware image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Berger, Laurin Lux, Alexander Weers, Martin Menten, Daniel Rueckert, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological correctness, i.e., the preservation of structural integrity and
specific characteristics of shape, is a fundamental requirement for medical
imaging tasks, such as neuron or vessel segmentation. Despite the recent surge
in topology-aware methods addressing this challenge, their real-world
applicability is hindered by flawed benchmarking practices. In this paper, we
identify critical pitfalls in model evaluation that include inadequate
connectivity choices, overlooked topological artifacts in ground truth
annotations, and inappropriate use of evaluation metrics. Through detailed
empirical analysis, we uncover these issues' profound impact on the evaluation
and ranking of segmentation methods. Drawing from our findings, we propose a
set of actionable recommendations to establish fair and robust evaluation
standards for topology-aware medical image segmentation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/AlexanderHBerger/topo-pitfalls</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic
  Evaluation Using a Vision Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown impressive abilities in text and
image understanding. However, existing metrics for evaluating the text
generated by VLMs focus exclusively on overall quality, leading to two
limitations: 1) it is challenging to identify which aspects of the text need
improvement from the overall score; 2) metrics may overlook specific evaluation
criteria when predicting an overall score. To address these limitations, we
propose HarmonicEval, a reference-free evaluation metric that aggregates
criterion-wise scores to produce the overall score in a bottom-up manner.
Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)
dataset, which comprises 18,000 expert human judgments across four
vision-language tasks. Our experiments demonstrate that HarmonicEval achieves
higher correlations with human judgments than conventional metrics while
providing numerical scores for each criterion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Successive optimization of optics and post-processing with
  differentiable coherent PSF operator and field information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Ren, Jingwen Zhou, Wenguan Zhang, Jiapu Yan, Bingkun Chen, Huajun Feng, Shiqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the joint design of optical systems and downstream algorithms is
showing significant potential. However, existing rays-described methods are
limited to optimizing geometric degradation, making it difficult to fully
represent the optical characteristics of complex, miniaturized lenses
constrained by wavefront aberration or diffraction effects. In this work, we
introduce a precise optical simulation model, and every operation in pipeline
is differentiable. This model employs a novel initial value strategy to enhance
the reliability of intersection calculation on high aspherics. Moreover, it
utilizes a differential operator to reduce memory consumption during coherent
point spread function calculations. To efficiently address various degradation,
we design a joint optimization procedure that leverages field information.
Guided by a general restoration network, the proposed method not only enhances
the image quality, but also successively improves the optical performance
across multiple lenses that are already in professional level. This joint
optimization pipeline offers innovative insights into the practical design of
sophisticated optical systems and post-processing algorithms. The source code
will be made publicly available at
https://github.com/Zrr-ZJU/Successive-optimization
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Get Rid of Handcrafted Feature Extractors? SparseViT:
  Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization
  Through Spare-Coding <span class="highlight-title">Transformer</span> <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Su, Xiaochen Ma, Xuekang Zhu, Chaoqun Niu, Zeyu Lei, Ji-Zhe Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-semantic features or semantic-agnostic features, which are irrelevant to
image context but sensitive to image manipulations, are recognized as
evidential to Image Manipulation Localization (IML). Since manual labels are
impossible, existing works rely on handcrafted methods to extract non-semantic
features. Handcrafted non-semantic features jeopardize IML model's
generalization ability in unseen or complex scenarios. Therefore, for IML, the
elephant in the room is: How to adaptively extract non-semantic features?
Non-semantic features are context-irrelevant and manipulation-sensitive. That
is, within an image, they are consistent across patches unless manipulation
occurs. Then, spare and discrete interactions among image patches are
sufficient for extracting non-semantic features. However, image semantics vary
drastically on different patches, requiring dense and continuous interactions
among image patches for learning semantic representations. Hence, in this
paper, we propose a Sparse Vision Transformer (SparseViT), which reformulates
the dense, global self-attention in ViT into a sparse, discrete manner. Such
sparse self-attention breaks image semantics and forces SparseViT to adaptively
extract non-semantic features for images. Besides, compared with existing IML
models, the sparse self-attention mechanism largely reduced the model size (max
80% in FLOPs), achieving stunning parameter efficiency and computation
reduction. Extensive experiments demonstrate that, without any handcrafted
feature extractors, SparseViT is superior in both generalization and efficiency
across benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 page, 8 figures, published to AAAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDP: Generalizing to Multilingual Visual Information Extraction by
  Language Decoupled <span class="highlight-title">Pretrain</span>ing <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Information Extraction (VIE) plays a crucial role in the comprehension
of semi-structured documents, and several pre-trained models have been
developed to enhance performance. However, most of these works are monolingual
(usually English). Due to the extremely unbalanced quantity and quality of
pre-training corpora between English and other languages, few works can extend
to non-English scenarios. In this paper, we conduct systematic experiments to
show that vision and layout modality hold invariance among images with
different languages. If decoupling language bias from document images, a
vision-layout-based model can achieve impressive cross-lingual generalization.
Accordingly, we present a simple but effective multilingual training paradigm
LDP (Language Decoupled Pre-training) for better utilization of monolingual
pre-training data. Our proposed model LDM (Language Decoupled Model) is first
pre-trained on the language-independent data, where the language knowledge is
decoupled by a diffusion model, and then the LDM is fine-tuned on the
downstream languages. Extensive experiments show that the LDM outperformed all
SOTA multilingual pre-trained models, and also maintains competitiveness on
downstream monolingual/English benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,
  and Internal Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Li, Bozhong Zheng, Xiaohao Xu, Jinye Gan, Fading Lu, Xiang Li, Na Ni, Zheng Tian, Xiaonan Huang, Shenghua Gao, Yingna Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object anomaly detection is essential for industrial quality inspection, yet
traditional single-sensor methods face critical limitations. They fail to
capture the wide range of anomaly types, as single sensors are often
constrained to either external appearance, geometric structure, or internal
properties. To overcome these challenges, we introduce MulSen-AD, the first
high-resolution, multi-sensor anomaly detection dataset tailored for industrial
applications. MulSen-AD unifies data from RGB cameras, laser scanners, and
lock-in infrared thermography, effectively capturing external appearance,
geometric deformations, and internal defects. The dataset spans 15 industrial
products with diverse, real-world anomalies. We also present MulSen-AD Bench, a
benchmark designed to evaluate multi-sensor methods, and propose
MulSen-TripleAD, a decision-level fusion algorithm that integrates these three
modalities for robust, unsupervised object anomaly detection. Our experiments
demonstrate that multi-sensor fusion substantially outperforms single-sensor
approaches, achieving 96.1% AUROC in object-level detection accuracy. These
results highlight the importance of integrating multi-sensor data for
comprehensive industrial anomaly detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spike2Former: Efficient Spiking <span class="highlight-title">Transformer</span> for High-performance Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly
in image segmentation tasks. The reason is that directly converting neural
networks with complex architectural designs for segmentation tasks into spiking
versions leads to performance degradation and non-convergence. To address this
challenge, we first identify the modules in the architecture design that lead
to the severe reduction in spike firing, make targeted improvements, and
propose Spike2Former architecture. Second, we propose normalized integer
spiking neurons to solve the training stability problem of SNNs with complex
architectures. We set a new state-of-the-art for SNNs in various semantic
segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0
efficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU
and 6.6 efficiency on CityScapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted on Association for the Advancement of
  Artificial Intelligence 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video
  Captioning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing demand for solutions to real-world video challenges,
interest in dense video captioning (DVC) has been on the rise. DVC involves the
automatic captioning and localization of untrimmed videos. Several studies
highlight the challenges of DVC and introduce improved methods utilizing prior
knowledge, such as pre-training and external memory. In this research, we
propose a model that leverages the prior knowledge of human-oriented
hierarchical compact memory inspired by human memory hierarchy and cognition.
To mimic human-like memory recall, we construct a hierarchical memory and a
hierarchical memory reading module. We build an efficient hierarchical compact
memory by employing clustering of memory events and summarization using large
language models. Comparative experiments demonstrate that this hierarchical
memory recall process improves the performance of DVC by achieving
state-of-the-art performance on YouCook2 and ViTT datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffSim: Taming Diffusion Models for Evaluating Visual Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiren Song, Xiaokang Liu, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have fundamentally transformed the field of generative
models, making the assessment of similarity between customized model outputs
and reference inputs critically important. However, traditional perceptual
similarity metrics operate primarily at the pixel and patch levels, comparing
low-level colors and textures but failing to capture mid-level similarities and
differences in image layout, object pose, and semantic content. Contrastive
learning-based CLIP and self-supervised learning-based DINO are often used to
measure semantic similarity, but they highly compress image features,
inadequately assessing appearance details. This paper is the first to discover
that pretrained diffusion models can be utilized for measuring visual
similarity and introduces the DiffSim method, addressing the limitations of
traditional metrics in capturing perceptual consistency in custom generation
tasks. By aligning features in the attention layers of the denoising U-Net,
DiffSim evaluates both appearance and style similarity, showing superior
alignment with human visual preferences. Additionally, we introduce the Sref
and IP benchmarks to evaluate visual similarity at the level of style and
instance, respectively. Comprehensive evaluations across multiple benchmarks
demonstrate that DiffSim achieves state-of-the-art performance, providing a
robust tool for measuring visual coherence in generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D occupancy perception is gaining increasing attention due to its capability
to offer detailed and precise environment representations. Previous
weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU
varying by 5-10 points due to sampling count along camera rays. Recently,
real-time Gaussian splatting has gained widespread popularity in 3D
reconstruction, and the occupancy prediction task can also be viewed as a
reconstruction task. Consequently, we propose GSRender, which naturally employs
3D Gaussian Splatting for occupancy prediction, simplifying the sampling
process. In addition, the limitations of 2D supervision result in duplicate
predictions along the same camera ray. We implemented the Ray Compensation (RC)
module, which mitigates this issue by compensating for features from adjacent
frames. Finally, we redesigned the loss to eliminate the impact of dynamic
objects from adjacent frames. Extensive experiments demonstrate that our
approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while
narrowing the gap with 3D supervision methods. Our code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alignment-Free RGB-T Salient Object Detection: A Large-scale <span class="highlight-title">Dataset</span> and
  Progressive Correlation Network <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunpeng Wang, Keke Chen, Chenglong Li, Zhengzheng Tu, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment-free RGB-Thermal (RGB-T) salient object detection (SOD) aims to
achieve robust performance in complex scenes by directly leveraging the
complementary information from unaligned visible-thermal image pairs, without
requiring manual alignment. However, the labor-intensive process of collecting
and annotating image pairs limits the scale of existing benchmarks, hindering
the advancement of alignment-free RGB-T SOD. In this paper, we construct a
large-scale and high-diversity unaligned RGB-T SOD dataset named UVT20K,
comprising 20,000 image pairs, 407 scenes, and 1256 object categories. All
samples are collected from real-world scenarios with various challenges, such
as low illumination, image clutter, complex salient objects, and so on. To
support the exploration for further research, each sample in UVT20K is
annotated with a comprehensive set of ground truths, including saliency masks,
scribbles, boundaries, and challenge attributes. In addition, we propose a
Progressive Correlation Network (PCNet), which models inter- and intra-modal
correlations on the basis of explicit alignment to achieve accurate predictions
in unaligned image pairs. Extensive experiments conducted on unaligned and
aligned datasets demonstrate the effectiveness of our method.Code and dataset
are available at https://github.com/Angknpng/PCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar
  Object Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Xu, Zhiyu Xiang, Chenwei Zhang, Hanzhi Zhong, Xijun Zhao, Ruina Dang, Peng Xu, Tianyu Pu, Eryun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection is one of the fundamental perception tasks for autonomous
vehicles. Fulfilling such a task with a 4D millimeter-wave radar is very
attractive since the sensor is able to acquire 3D point clouds similar to Lidar
while maintaining robust measurements under adverse weather. However, due to
the high sparsity and noise associated with the radar point clouds, the
performance of the existing methods is still much lower than expected. In this
paper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation
(SCKD) method for 4D radar-based 3D object detection. It characterizes the
capability of learning the feature from a Lidar-radar-fused teacher network
with semi-supervised distillation. We first propose an adaptive fusion module
in the teacher network to boost its performance. Then, two feature distillation
modules are designed to facilitate the cross-modality knowledge transfer.
Finally, a semi-supervised output distillation is proposed to increase the
effectiveness and flexibility of the distillation framework. With the same
network structure, our radar-only student trained by SCKD boosts the mAP by
10.38% over the baseline and outperforms the state-of-the-art works on the VoD
dataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the
moderate difficulty level over the baseline when extra unlabeled data are
available. Code is available at https://github.com/Ruoyu-Xu/SCKD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF
  Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsung Kim, Minjun Park, Jooyoung Choi, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent learning-based Multi-View Stereo models have demonstrated
state-of-the-art performance in sparse-view 3D reconstruction. However,
directly applying 3D Gaussian Splatting (3DGS) as a refinement step following
these models presents challenges. We hypothesize that the excessive positional
degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting
color patterns at the cost of structural fidelity. To address this, we propose
reprojection-based DoF separation, a method distinguishing positional DoFs in
terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To
independently manage each DoF, we introduce a reprojection process along with
tailored constraints for each DoF. Through experiments across various datasets,
we confirm that separating the positional DoFs of Gaussians and applying
targeted constraints effectively suppresses geometric artifacts, producing
reconstruction results that are both visually and geometrically plausible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GBRIP: Granular Ball Representation for Imbalanced Partial Label
  Learning <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial label learning (PLL) is a complicated weakly supervised
multi-classification task compounded by class imbalance. Currently, existing
methods only rely on inter-class pseudo-labeling from inter-class features,
often overlooking the significant impact of the intra-class imbalanced features
combined with the inter-class. To address these limitations, we introduce
Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for
imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and
multi-center loss to construct a granular ball-based nfeature space through
unsupervised learning, effectively capturing the feature distribution within
each class. GBRIP mitigates the impact of confusing features by systematically
refining label disambiguation and estimating imbalance distributions. The novel
multi-center loss function enhances learning by emphasizing the relationships
between samples and their respective centers within the granular balls.
Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms
existing state-of-the-art methods, offering a robust solution to the challenges
of imbalanced PLL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, Ruimao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling law has been validated in various domains, such as natural
language processing (NLP) and massive computer vision tasks; however, its
application to motion generation remains largely unexplored. In this paper, we
introduce a scalable motion generation framework that includes the motion
tokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through
comprehensive experiments, we observe the scaling behavior of this system. For
the first time, we confirm the existence of scaling laws within the context of
motion generation. Specifically, our results demonstrate that the normalized
test loss of our prefix autoregressive models adheres to a logarithmic law in
relation to compute budgets. Furthermore, we also confirm the power law between
Non-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect
to compute budgets respectively. Leveraging the scaling law, we predict the
optimal transformer size, vocabulary size, and data requirements for a compute
budget of $1e18$. The test loss of the system, when trained with the optimal
model size, vocabulary size, and required data, aligns precisely with the
predicted test loss, thereby validating the scaling law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bright-NeRF:Brightening Neural Radiance Field with Color Restoration
  from Low-light Raw Images <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Wang, Xin Huang, Guoqing Zhou, Qifeng Guo, Qing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have demonstrated prominent performance in
novel view synthesis. However, their input heavily relies on image acquisition
under normal light conditions, making it challenging to learn accurate scene
representation in low-light environments where images typically exhibit
significant noise and severe color distortion. To address these challenges, we
propose a novel approach, Bright-NeRF, which learns enhanced and high-quality
radiance fields from multi-view low-light raw images in an unsupervised manner.
Our method simultaneously achieves color restoration, denoising, and enhanced
novel view synthesis. Specifically, we leverage a physically-inspired model of
the sensor's response to illumination and introduce a chromatic adaptation loss
to constrain the learning of response, enabling consistent color perception of
objects regardless of lighting conditions. We further utilize the raw data's
properties to expose the scene's intensity automatically. Additionally, we have
collected a multi-view low-light raw image dataset to advance research in this
field. Experimental results demonstrate that our proposed method significantly
outperforms existing 2D and 3D approaches. Our code and dataset will be made
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ {S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gui Wang, Yuexiang Li, Wenting Chen, Meidan Ding, Wooi Ping Cheah, Rong Qu, Jianfeng Ren, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small lesions play a critical role in early disease diagnosis and
intervention of severe infections. Popular models often face challenges in
segmenting small lesions, as it occupies only a minor portion of an image,
while down\_sampling operations may inevitably lose focus on local features of
small lesions. To tackle the challenges, we propose a {\bf S}mall-{\bf
S}ize-{\bf S}ensitive {\bf Mamba} ({\bf S$^3$-Mamba}), which promotes the
sensitivity to small lesions across three dimensions: channel, spatial, and
training strategy. Specifically, an Enhanced Visual State Space block is
designed to focus on small lesions through multiple residual connections to
preserve local features, and selectively amplify important details while
suppressing irrelevant ones through channel-wise attention. A Tensor-based
Cross-feature Multi-scale Attention is designed to integrate input image
features and intermediate-layer features with edge features and exploit the
attentive support of features across multiple scales, thereby retaining spatial
details of small lesions at various granularities. Finally, we introduce a
novel regularized curriculum learning to automatically assess lesion size and
sample difficulty, and gradually focus from easy samples to hard ones like
small lesions. Extensive experiments on three medical image segmentation
datasets show the superiority of our S$^3$-Mamba, especially in segmenting
small lesions. Our code is available at
https://github.com/ErinWang2023/S3-Mamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summary of Point <span class="highlight-title">Transformer</span> with Federated Learning for Predicting
  Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamorudeen A. Amuda, Almustapha A. Wakili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a federated learning-based approach to predict HER2
status from hematoxylin and eosin (HE)-stained whole slide images (WSIs),
reducing costs and speeding up treatment decisions. To address label imbalance
and feature representation challenges in multisite datasets, a point
transformer is proposed, incorporating dynamic label distribution, an auxiliary
classifier, and farthest cosine sampling. Extensive experiments demonstrate
state-of-the-art performance across four sites (2687 WSIs) and strong
generalization to two unseen sites (229 WSIs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracing the Roots: Leveraging Temporal Dynamics in Diffusion
  Trajectories for Origin Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have revolutionized image synthesis, garnering significant
research interest in recent years. Diffusion is an iterative algorithm in which
samples are generated step-by-step, starting from pure noise. This process
introduces the notion of diffusion trajectories, i.e., paths from the standard
Gaussian distribution to the target image distribution. In this context, we
study discriminative algorithms operating on these trajectories. Specifically,
given a pre-trained diffusion model, we consider the problem of classifying
images as part of the training dataset, generated by the model or originating
from an external source. Our approach demonstrates the presence of patterns
across steps that can be leveraged for classification. We also conduct ablation
studies, which reveal that using higher-order gradient features to characterize
the trajectories leads to significant performance gains and more robust
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does VLM Classification Benefit from LLM Description Semantics? <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately describing images with text is a foundation of explainable AI.
Vision-Language Models (VLMs) like CLIP have recently addressed this by
aligning images and texts in a shared embedding space, expressing semantic
similarities between vision and language embeddings. VLM classification can be
improved with descriptions generated by Large Language Models (LLMs). However,
it is difficult to determine the contribution of actual description semantics,
as the performance gain may also stem from a semantic-agnostic ensembling
effect, where multiple modified text prompts act as a noisy test-time
augmentation for the original one. We propose an alternative evaluation
scenario to decide if a performance boost of LLM-generated descriptions is
caused by such a noise augmentation effect or rather by genuine description
semantics. The proposed scenario avoids noisy test-time augmentation and
ensures that genuine, distinctive descriptions cause the performance boost.
Furthermore, we propose a training-free method for selecting discriminative
descriptions that work independently of classname-ensembling effects. Our
approach identifies descriptions that effectively differentiate classes within
a local CLIP label neighborhood, improving classification accuracy across seven
datasets. Additionally, we provide insights into the explainability of
description-based image classification with VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI-25 (extended version), Code: https://github.com/CompVis/DisCLIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DepthFM: Fast Monocular Depth Estimation with Flow Matching <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current discriminative depth estimation methods often produce blurry
artifacts, while generative approaches suffer from slow sampling due to
curvatures in the noise-to-depth transport. Our method addresses these
challenges by framing depth estimation as a direct transport between image and
depth distributions. We are the first to explore flow matching in this field,
and we demonstrate that its interpolation trajectories enhance both training
and sampling efficiency while preserving high performance. While generative
models typically require extensive training data, we mitigate this dependency
by integrating external knowledge from a pre-trained image diffusion model,
enabling effective transfer even across differing objectives. To further boost
our model performance, we employ synthetic data and utilize image-depth pairs
generated by a discriminative model on an in-the-wild image dataset. As a
generative model, our model can reliably estimate depth confidence, which
provides an additional advantage. Our approach achieves competitive zero-shot
performance on standard benchmarks of complex natural scenes while improving
sampling efficiency and only requiring minimal synthetic data for training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025, Project Page: https://github.com/CompVis/depth-fm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Compatible Training for Online Backfilling in Large-Scale
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backfilling is the process of re-extracting all gallery embeddings from
upgraded models in image retrieval systems. It inevitably requires a
prohibitively large amount of computational cost and even entails the downtime
of the service. Although backward-compatible learning sidesteps this challenge
by tackling query-side representations, this leads to suboptimal solutions in
principle because gallery embeddings cannot benefit from model upgrades. We
address this dilemma by introducing an online backfilling algorithm, which
enables us to achieve a progressive performance improvement during the
backfilling process while not sacrificing the final performance of new model
after the completion of backfilling. To this end, we first propose a simple
distance rank merge technique for online backfilling. Then, we incorporate a
reverse transformation module for more effective and efficient merging, which
is further enhanced by adopting a metric-compatible contrastive learning
approach. These two components help to make the distances of old and new models
compatible, resulting in desirable merge results during backfilling with no
extra computational overhead. Extensive experiments show the effectiveness of
our framework on four standard benchmarks in various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning-Based and Fully Automated Pipeline for Regurgitant
  Mitral Valve Anatomy Analysis from 3D Echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Munafò, Simone Saitta, Giacomo Ingallina, Paolo Denti, Francesco Maisano, Eustachio Agricola, Alberto Redaelli, Emiliano Votta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D transesophageal echocardiography (3DTEE), is the recommended method for
diagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of
the mitral valve (MV), allowing for precise segmentation and measurement of the
regurgitant valve anatomy. However, manual TEE segmentations are time-consuming
and prone to intra-operator variability, affecting the reliability of the
measurements. To address this, we developed a fully automated pipeline using a
3D convolutional neural network (CNN) to segment MV substructures (annulus,
anterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN,
based on a multi-decoder residual U-Net architecture, was trained and tested on
a dataset comprising 100 3DTEE images with corresponding segmentations. Within
the pipeline, a custom algorithm refines the CNN-based segmentations and
extracts MV models, from which anatomical landmarks and features are
quantified. The accuracy of the proposed method was assessed using Dice score
and mean surface distance (MSD) against ground truth segmentations, and the
extracted anatomical parameters were compared against a semiautomated
commercial software TomTec Image Arena. The trained 3D CNN achieved an average
Dice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the
annulus, anterior and posterior leaflet. The proposed CNN architecture
outperformed a baseline residual U-Net architecture in MV substructure
segmentation, and the refinement of the predicted annulus segmentation improved
MSD by 8.36%. The annular and leaflet linear measurements differed by less than
7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained
with TomTec Image Arena. The proposed pipeline was faster than the commercial
software, with a modeling time of 12.54 s and a quantification time of 54.42 s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Gradient Clipping for Noisy Label Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08941v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08941v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xichen Ye, Yifan Wu, Weizhong Zhang, Xiaoqiang Li, Yifan Chen, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has shown that constraining the gradient of loss function
with respect to model-predicted probabilities can enhance the model robustness
against noisy labels. These methods typically specify a fixed optimal threshold
for gradient clipping through validation data to obtain the desired robustness
against noise. However, this common practice overlooks the dynamic distribution
of gradients from both clean and noisy-labeled samples at different stages of
training, significantly limiting the model capability to adapt to the variable
nature of gradients throughout the training process. To address this issue, we
propose a simple yet effective approach called Optimized Gradient Clipping
(OGC), which dynamically adjusts the clipping threshold based on the ratio of
noise gradients to clean gradients after clipping, estimated by modeling the
distributions of clean and noisy samples. This approach allows us to modify the
clipping threshold at each training step, effectively controlling the influence
of noise gradients. Additionally, we provide statistical analysis to certify
the noise-tolerance ability of OGC. Our extensive experiments across various
types of label noise, including symmetric, asymmetric, instance-dependent, and
real-world noise, demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G-VEval: A Versatile Metric for Evaluating Image and Video Captions
  Using <span class="highlight-title">GPT</span>-4o 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation metric of visual captioning is important yet not thoroughly
explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss
semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are
limited in zero-shot scenarios. Advanced Language Model-based metrics also
struggle with aligning to nuanced human preferences. To address these issues,
we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new
GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and
supports three modes: reference-free, reference-only, and combined,
accommodating both video and image inputs. We also propose MSVD-Eval, a new
dataset for video captioning evaluation, to establish a more transparent and
consistent framework for both human experts and evaluation metrics. It is
designed to address the lack of clear criteria in existing datasets by
introducing distinct dimensions of Accuracy, Completeness, Conciseness, and
Relevance (ACCR). Extensive results show that G-VEval outperforms existing
methods in correlation with human annotations, as measured by Kendall tau-b and
Kendall tau-c. This provides a flexible solution for diverse captioning tasks
and suggests a straightforward yet effective approach for large language models
to understand video content, paving the way for advancements in automated
captioning. Codes are available at https://github.com/ztangaj/gveval
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait
  Image <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent works have achieved great success on image-to-3D object
generation, high quality and fidelity 3D head generation from a single image
remains a great challenge. Previous text-based methods for generating 3D heads
were limited by text descriptions and image-based methods struggled to produce
high-quality head geometry. To handle this challenging problem, we propose a
novel framework, ID-Sculpt, to generate high-quality 3D heads while preserving
their identities. Our work incorporates the identity information of the
portrait image into three parts: 1) geometry initialization, 2) geometry
sculpting, and 3) texture generation stages. Given a reference portrait image,
we first align the identity features with text features to realize ID-aware
guidance enhancement, which contains the control signals representing the face
information. We then use the canny map, ID features of the portrait image, and
a pre-trained text-to-normal/depth diffusion model to generate ID-aware
geometry supervision, and 3D-GAN inversion is employed to generate ID-aware
geometry initialization. Furthermore, with the ability to inject identity
information into 3D head generation, we use ID-aware guidance to calculate
ID-aware Score Distillation (ISD) for geometry sculpting. For texture
generation, we adopt the ID Consistent Texture Inpainting and Refinement which
progressively expands the view for texture inpainting to obtain an
initialization UV texture map. We then use the ID-aware guidance to provide
image-level supervision for noisy multi-view images to obtain a refined texture
map. Extensive experiments demonstrate that we can generate high-quality 3D
heads with accurate geometry and texture from a single in-the-wild portrait
image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025; Project page:
  https://jinkun-hao.github.io/ID-Sculpt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SageAttention2: Efficient Attention with Thorough Outlier Smoothing and
  Per-thread INT4 Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although quantization for linear layers has been widely used, its application
to accelerate the attention process remains limited. To further enhance the
efficiency of attention computation compared to SageAttention while maintaining
precision, we propose SageAttention2, which utilizes significantly faster 4-bit
matrix multiplication (Matmul) alongside additional precision-enhancing
techniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a
hardware-friendly thread-level granularity and quantize matrixes $(\widetilde
P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the
accuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$
to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS)
of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on
RTX4090, respectively. Comprehensive experiments confirm that our approach
incurs negligible end-to-end metrics loss across diverse models, including
those for large language processing, image generation, and video generation.
The codes are available at https://github.com/thu-ml/SageAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Jalonen, Mohammad Al-Sa'd, Roope Mellanen, Serkan Kiranyaz, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The health and safety hazards posed by worn crane lifting ropes mandate
periodic inspection for damage. This task is time-consuming, prone to human
error, halts operation, and may result in the premature disposal of ropes.
Therefore, we propose using efficient deep learning and computer vision methods
to automate the process of detecting damaged ropes. Specifically, we present a
vision-based system for detecting damage in synthetic fiber rope images using
lightweight convolutional neural networks. We develop a camera-based apparatus
to photograph the lifting rope's surface, while in operation, and capture the
progressive wear-and-tear as well as the more significant degradation in the
rope's health state. Experts from Konecranes annotate the collected images in
accordance with the rope's condition; normal or damaged. Then, we pre-process
the images, systematically design a deep learning model, evaluate its detection
and prediction performance, analyze its computational complexity, and compare
it with various other models. Experimental results show the proposed model
outperforms other similar techniques with 96.5% accuracy, 94.8% precision,
98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the
model's real-time operation, low memory footprint, robustness to various
environmental and operational conditions, and adequacy for deployment in
industrial applications such as lifting, mooring, towing, climbing, and
sailing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cycle Pixel Difference Network for Crisp Edge Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsong Liu, Wei Zhang, Yanyan Liu, Mingyang Li, Wenlin Li, Yimeng Fan, Xiangnan Bai, Liang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge detection, as a fundamental task in computer vision, has garnered
increasing attention. The advent of deep learning has significantly advanced
this field. However, recent deep learning-based methods generally face two
significant issues: 1) reliance on large-scale pre-trained weights, and 2)
generation of thick edges. We construct a U-shape encoder-decoder model named
CPD-Net that successfully addresses these two issues simultaneously. In
response to issue 1), we propose a novel cycle pixel difference convolution
(CPDC), which effectively integrates edge prior knowledge with modern
convolution operations, consequently successfully eliminating the dependence on
large-scale pre-trained weights. As for issue 2), we construct a multi-scale
information enhancement module (MSEM) and a dual residual connection-based
(DRC) decoder to enhance the edge location ability of the model, thereby
generating crisp and clean contour maps. Comprehensive experiments conducted on
four standard benchmarks demonstrate that our method achieves competitive
performance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760
and AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our
approach provides a novel perspective for addressing these challenges in edge
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth
  Estimation of Endoscopic Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16571v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16571v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Wang, Ying Zhou, Shiquan He, Ting Li, Fan Huang, Qiang Ding, Xinxia Feng, Mei Liu, Qiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photometric constraint is indispensable for self-supervised monocular depth
estimation. It involves warping a source image onto a target view using
estimated depth&pose, and then minimizing the difference between the warped and
target images. However, the endoscopic built-in light causes significant
brightness fluctuations, and thus makes the photometric constraint unreliable.
Previous efforts only mitigate this relying on extra models to calibrate image
brightness. In this paper, we propose MonoPCC to address the brightness
inconsistency radically by reshaping the photometric constraint into a cycle
form. Instead of only warping the source image, MonoPCC constructs a closed
loop consisting of two opposite forward-backward warping paths: from target to
source and then back to target. Thus, the target image finally receives an
image cycle-warped from itself, which naturally makes the constraint invariant
to brightness changes. Moreover, MonoPCC transplants the source image's
phase-frequency into the intermediate warped image to avoid structure lost, and
also stabilizes the training via an exponential moving average (EMA) strategy
to avoid frequent changes in the forward warping. The comprehensive and
extensive experimental results on four endoscopic datasets demonstrate that our
proposed MonoPCC shows a great robustness to the brightness inconsistency, and
exceeds other state-of-the-arts by reducing the absolute relative error by at
least 7.27%, 9.38%, 9.90% and 3.17%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Union-over-Intersections: Object Detection beyond Winner-Takes-All 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Bhowmik, Pascal Mettes, Martin R. Oswald, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the problem of predicting box locations in object
detection architectures. Typically, each box proposal or box query aims to
directly maximize the intersection-over-union score with the ground truth,
followed by a winner-takes-all non-maximum suppression where only the highest
scoring box in each region is retained. We observe that both steps are
sub-optimal: the first involves regressing proposals to the entire ground
truth, which is a difficult task even with large receptive fields, and the
second neglects valuable information from boxes other than the top candidate.
Instead of regressing proposals to the whole ground truth, we propose a simpler
approach: regress only to the area of intersection between the proposal and the
ground truth. This avoids the need for proposals to extrapolate beyond their
visual scope, improving localization accuracy. Rather than adopting a
winner-takes-all strategy, we take the union over the regressed intersections
of all boxes in a region to generate the final box outputs. Our plug-and-play
method integrates seamlessly into proposal-based, grid-based, and query-based
detection architectures with minimal modifications, consistently improving
object localization and instance segmentation. We demonstrate its broad
applicability and versatility across various detection and segmentation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point Cloud Semantic Segmentation with Sparse and Inhomogeneous
  Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyi Pan, Nan Zhang, Wei Gao, Shan Liu, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing uniformly distributed sparse annotations, weakly supervised
learning alleviates the heavy reliance on fine-grained annotations in point
cloud semantic segmentation tasks. However, few works discuss the inhomogeneity
of sparse annotations, albeit it is common in real-world scenarios. Therefore,
this work introduces the probability density function into the gradient
sampling approximation method to qualitatively analyze the impact of annotation
sparsity and inhomogeneity under weakly supervised learning. Based on our
analysis, we propose an Adaptive Annotation Distribution Network (AADNet)
capable of robust learning on arbitrarily distributed sparse annotations.
Specifically, we propose a label-aware point cloud downsampling strategy to
increase the proportion of annotations involved in the training stage.
Furthermore, we design the multiplicative dynamic entropy as the gradient
calibration function to mitigate the gradient bias caused by non-uniformly
distributed sparse annotations and explicitly reduce the epistemic uncertainty.
Without any prior restrictions and additional information, our proposed method
achieves comprehensive performance improvements at multiple label rates and
different annotation distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accuracy Limits as a Barrier to Biometric System Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Durbet, Paul-Marie Grollemund, Pascal Lafourcade, Kevin Thiry-Atighehchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biometric systems are widely used for identity verification and
identification, including authentication (i.e., one-to-one matching to verify a
claimed identity) and identification (i.e., one-to-many matching to find a
subject in a database). The matching process relies on measuring similarities
or dissimilarities between a fresh biometric template and enrolled templates.
The False Match Rate FMR is a key metric for assessing the accuracy and
reliability of such systems. This paper analyzes biometric systems based on
their FMR, with two main contributions. First, we explore untargeted attacks,
where an adversary aims to impersonate any user within a database. We determine
the number of trials required for an attacker to successfully impersonate a
user and derive the critical population size (i.e., the maximum number of users
in the database) required to maintain a given level of security. Furthermore,
we compute the critical FMR value needed to ensure resistance against
untargeted attacks as the database size increases. Second, we revisit the
biometric birthday problem to evaluate the approximate and exact probabilities
that two users in a database collide (i.e., can impersonate each other). Based
on this analysis, we derive both the approximate critical population size and
the critical FMR value needed to bound the likelihood of such collisions
occurring with a given probability. These thresholds offer insights for
designing systems that mitigate the risk of impersonation and collisions,
particularly in large-scale biometric databases. Our findings indicate that
current biometric systems fail to deliver sufficient accuracy to achieve an
adequate security level against untargeted attacks, even in small-scale
databases. Moreover, state-of-the-art systems face significant challenges in
addressing the biometric birthday problem, especially as database sizes grow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for
  Efficient Multi-Frame Interpolation in DSA Images <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Huangxuan Zhao, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid and accurate direct multi-frame interpolation method for Digital
Subtraction Angiography (DSA) images is crucial for reducing radiation and
providing real-time assistance to physicians for precise diagnostics and
treatment. DSA images contain complex vascular structures and various motions.
Applying natural scene Video Frame Interpolation (VFI) methods results in
motion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA
has specifically addressed these issues for the first time and achieved SOTA
results. However, MoSt-DSA's focus on real-time performance leads to
insufficient suppression of high-frequency noise and incomplete filtering of
low-frequency noise in the generated images. To address these issues within the
same computational time scale, we propose GaraMoSt. Specifically, we optimize
the network pipeline with a parallel design and propose a module named MG-MSFE.
MG-MSFE extracts frame-relative motion and structural features at various
granularities in a fully convolutional parallel manner and supports
independent, flexible adjustment of context-aware granularity at different
scales, thus enhancing computational efficiency and accuracy. Extensive
experiments demonstrate that GaraMoSt achieves the SOTA performance in
accuracy, robustness, visual effects, and noise suppression, comprehensively
surpassing MoSt-DSA and other natural scene VFI methods. The code and models
are available at https://github.com/ZyoungXu/GaraMoSt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large video-language models (LVLMs) struggle to comprehend long
videos correctly due to limited context. To address this problem, fine-tuning
long-context LVLMs and employing GPT-based agents have emerged as promising
solutions. However, fine-tuning LVLMs would require extensive high-quality data
and substantial GPU resources, while GPT-based agents would rely on proprietary
models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented
Generation (Video-RAG), a training-free and cost-effective pipeline that
employs visually-aligned auxiliary texts to help facilitate cross-modality
alignment while providing additional information beyond the visual content.
Specifically, we leverage open-source external tools to extract
visually-aligned information from pure video data (e.g., audio, optical
character, and object detection), and incorporate the extracted information
into an existing LVLM as auxiliary texts, alongside video frames and queries,
in a plug-and-play manner. Our Video-RAG offers several key advantages: (i)
lightweight with low computing overhead due to single-turn retrieval; (ii) easy
implementation and compatibility with any LVLM; and (iii) significant,
consistent performance gains across long video understanding benchmarks,
including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates
superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o
when utilized with a 72B model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Adversarial Networks for Image Super-Resolution: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunwei Tian, Xuanyu Zhang, Qi Zhu, Bob Zhang, Jerry Chun-Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single image super-resolution (SISR) has played an important role in the
field of image processing. Recent generative adversarial networks (GANs) can
achieve excellent results on low-resolution images with small samples. However,
there are little literatures summarizing different GANs in SISR. In this paper,
we conduct a comparative study of GANs from different perspectives. We first
take a look at developments of GANs. Second, we present popular architectures
for GANs in big and small samples for image applications. Then, we analyze
motivations, implementations and differences of GANs based optimization methods
and discriminative learning for image super-resolution in terms of supervised,
semi-supervised and unsupervised manners, where these GANs are analyzed via
integrating different network architectures, prior knowledge, loss functions
and multiple tasks. Next, we compare performance of these popular GANs on
public datasets via quantitative and qualitative analysis in SISR. Finally, we
highlight challenges of GANs and potential research points for SISR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Scalability of Self-Training for Open-Vocabulary Temporal
  Action Localization <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vocabulary size in temporal action localization (TAL) is limited by the
scarcity of large-scale annotated datasets. To overcome this, recent works
integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL
(OV-TAL). However, despite the success of VLMs trained on extensive datasets,
existing OV-TAL methods still rely on human-labeled TAL datasets of limited
size to train action localizers, limiting their generalizability. In this
paper, we explore the scalability of self-training with unlabeled YouTube
videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic
action localizer is trained on a human-labeled TAL dataset to generate
pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled
dataset is then used to train the localizer. Extensive experiments demonstrate
that leveraging web-scale videos in self-training significantly enhances the
generalizability of an action localizer. Additionally, we identify limitations
in existing OV-TAL evaluation schemes and propose a new benchmark for thorough
assessment. Finally, we showcase the TAL performance of the large multimodal
model Gemini-1.5 on our new benchmark. Code is released at
https://github.com/HYUNJS/STOV-TAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VHM: Versatile and Honest Vision Language Model for Remote Sensing Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20213v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20213v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a Versatile and Honest vision language Model (VHM) for
remote sensing image analysis. VHM is built on a large-scale remote sensing
image-text dataset with rich-content captions (VersaD), and an honest
instruction dataset comprising both factual and deceptive questions (HnstD).
Unlike prevailing remote sensing image-text datasets, in which image captions
focus on a few prominent objects and their relationships, VersaD captions
provide detailed information about image properties, object attributes, and the
overall scene. This comprehensive captioning enables VHM to thoroughly
understand remote sensing images and perform diverse remote sensing tasks.
Moreover, different from existing remote sensing instruction datasets that only
include factual questions, HnstD contains additional deceptive questions
stemming from the non-existence of objects. This feature prevents VHM from
producing affirmative answers to nonsense queries, thereby ensuring its
honesty. In our experiments, VHM significantly outperforms various vision
language models on common tasks of scene classification, visual question
answering, and visual grounding. Additionally, VHM achieves competent
performance on several unexplored tasks, such as building vectorizing,
multi-label classification and honest question answering. We will release the
code, data and model weights at https://github.com/opendatalab/VHM .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding
  author: Gui-Song Xia, Conghui He</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZAHA: Introducing the Level of Facade Generalization and the Large-Scale
  Point Cloud Facade Semantic Segmentation Benchmark <span class="highlight-title">Dataset</span> <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04865v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04865v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facade semantic segmentation is a long-standing challenge in photogrammetry
and computer vision. Although the last decades have witnessed the influx of
facade segmentation methods, there is a lack of comprehensive facade classes
and data covering the architectural variability. In ZAHA, we introduce Level of
Facade Generalization (LoFG), novel hierarchical facade classes designed based
on international urban modeling standards, ensuring compatibility with
real-world challenging classes and uniform methods' comparison. Realizing the
LoFG, we present to date the largest semantic 3D facade segmentation dataset,
providing 601 million annotated points at five and 15 classes of LoFG2 and
LoFG3, respectively. Moreover, we analyze the performance of baseline semantic
segmentation methods on our introduced LoFG classes and data, complementing it
with a discussion on the unresolved challenges for facade segmentation. We
firmly believe that ZAHA shall facilitate further development of 3D facade
semantic segmentation methods, enabling robust segmentation indispensable in
creating urban digital twins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye
  View Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Wang, Yanghao Zhang, Xiangyu Yin, Guangliang Cheng, Zeyu Fu, Xiaowei Huang, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera-based Bird's Eye View (BEV) perception models receive increasing
attention for their crucial role in autonomous driving, a domain where concerns
about the robustness and reliability of deep learning have been raised. While
only a few works have investigated the effects of randomly generated semantic
perturbations, aka natural corruptions, on the multi-view BEV detection task,
we develop a black-box robustness evaluation framework that adversarially
optimises three common semantic perturbations: geometric transformation, colour
shifting, and motion blur, to deceive BEV models, serving as the first approach
in this emerging field. To address the challenge posed by optimising the
semantic perturbation, we design a smoothed, distance-based surrogate function
to replace the mAP metric and introduce SimpleDIRECT, a deterministic
optimisation algorithm that utilises observed slopes to guide the optimisation
process. By comparing with randomised perturbation and two optimisation
baselines, we demonstrate the effectiveness of the proposed framework.
Additionally, we provide a benchmark on the semantic robustness of ten recent
BEV models. The results reveal that PolarFormer, which emphasises geometric
information from multi-view images, exhibits the highest robustness, whereas
BEVDet is fully compromised, with its precision reduced to zero.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASTM :Autonomous Smart Traffic Management System Using Artificial
  Intelligence CNN and LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10929v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10929v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christofel Rio Goenawan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the modern world, the development of Artificial Intelligence (AI) has
contributed to improvements in various areas, including automation, computer
vision, fraud detection, and more. AI can be leveraged to enhance the
efficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce
traffic congestion rates. This paper presents an Autonomous Smart Traffic
Management (STM) system that uses AI to improve traffic flow rates. The system
employs the YOLO V5 Convolutional Neural Network to detect vehicles in traffic
management images. Additionally, it predicts the number of vehicles for the
next 12 hours using a Recurrent Neural Network with Long Short-Term Memory
(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the
traffic cycle length based on these vehicle predictions, aided by AI. From the
results of the RNN-LSTM model for predicting vehicle numbers over the next 12
hours, we observe that the model predicts traffic with a Mean Squared Error
(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.
After simulating the STM system in the CARLA simulation environment, we found
that the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per
minute) is 50\% higher than the rate without STM (around 15 vehicles per
minute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5
seconds per vehicle) is 70\% lower than without STM (around 12 seconds per
vehicle). These results demonstrate that the STM system using AI can increase
traffic flow by 50\% and reduce vehicle pass delays by 70\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In process to IEEE Intelligent Vehicle Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCB-<span class="highlight-title">dataset</span>: A <span class="highlight-title">Dataset</span> for Detecting Student Classroom Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02488v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02488v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of deep learning methods for automatic detection of students'
classroom behavior is a promising approach to analyze their class performance
and enhance teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose a Student Classroom Behavior dataset
(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248
labels and 4,003 images, with a focus on hand-raising behavior. We evaluated
the dataset using the YOLOv7 algorithm, achieving a mean average precision
(map) of up to 85.3%. We believe that our dataset can serve as a robust
foundation for future research in the field of student behavior detection and
promote further advancements in this area.Our SCB-dataset can be downloaded
from: https://github.com/Whiffe/SCB-dataset
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Registration in 30 Years: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yang, Chu'ai Zhang, Zhengbao Wang, Xinyue Cao, Xuan Ouyang, Xiyu Zhang, Zhenxuan Zeng, Zhao Zeng, Borui Lu, Zhiyi Xia, Qian Zhang, Yulan Guo, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud registration is a fundamental problem in computer vision,
computer graphics, robotics, remote sensing, and etc. Over the last thirty
years, we have witnessed the amazing advancement in this area with numerous
kinds of solutions. Although a handful of relevant surveys have been conducted,
their coverage is still limited. In this work, we present a comprehensive
survey on 3D point cloud registration, covering a set of sub-areas such as
pairwise coarse registration, pairwise fine registration, multi-view
registration, cross-scale registration, and multi-instance registration. The
datasets, evaluation metrics, method taxonomy, discussions of the merits and
demerits, insightful thoughts of future directions are comprehensively
presented in this survey. The regularly updated project page of the survey is
available at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Diffusion <span class="highlight-title">Transformer</span>s with Token-wise Feature Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion transformers have shown significant effectiveness in both image and
video synthesis at the expense of huge computation costs. To address this
problem, feature caching methods have been introduced to accelerate diffusion
transformers by caching the features in previous timesteps and reusing them in
the following timesteps. However, previous caching methods ignore that
different tokens exhibit different sensitivities to feature caching, and
feature caching on some tokens may lead to 10$\times$ more destruction to the
overall generation quality compared with other tokens. In this paper, we
introduce token-wise feature caching, allowing us to adaptively select the most
suitable tokens for caching, and further enable us to apply different caching
ratios to neural layers in different types and depths. Extensive experiments on
PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image
and video generation with no requirements for training. For instance,
2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and
PixArt-$\alpha$ with almost no drop in generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In this version, we achieved a nearly lossless acceleration of 1.51
  times for ToCa on FLUX in the appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Jiaxin Li, Liming Tan, Yejie Guo, Junxuan Liang, Cewu Lu, Yong-Lu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent robots need to interact with diverse objects across various
environments. The appearance and state of objects frequently undergo complex
transformations depending on the object properties, e.g., phase transitions.
However, in the vision community, segmenting dynamic objects with phase
transitions is overlooked. In light of this, we introduce the concept of phase
in segmentation, which categorizes real-world objects based on their visual
characteristics and potential morphological and appearance changes. Then, we
present a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video
Object Segmentation (M$^3$-VOS), to verify the ability of models to understand
object phases, which consists of 479 high-resolution videos spanning over 10
distinct everyday scenarios. It provides dense instance mask annotations that
capture both object phases and their transitions. We evaluate state-of-the-art
methods on M$^3$-VOS, yielding several key insights. Notably, current
appearancebased approaches show significant room for improvement when handling
objects with phase transitions. The inherent changes in disorder suggest that
the predictive performance of the forward entropy-increasing process can be
improved through a reverse entropy-reducing process. These findings lead us to
propose ReVOS, a new plug-andplay model that improves its performance by
reversal refinement. Our data and code will be publicly available at
https://zixuan-chen.github.io/M-cubeVOS.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM
system for real-time and high-quality dense 3D reconstruction. SLAM3R provides
an end-to-end solution by seamlessly integrating local 3D reconstruction and
global coordinate registration through feed-forward neural networks. Given an
input video, the system first converts it into overlapping clips using a
sliding window mechanism. Unlike traditional pose optimization-based methods,
SLAM3R directly regresses 3D pointmaps from RGB images in each window and
progressively aligns and deforms these local pointmaps to create a globally
consistent scene reconstruction - all without explicitly solving any camera
parameters. Experiments across datasets consistently show that SLAM3R achieves
state-of-the-art reconstruction accuracy and completeness while maintaining
real-time performance at 20+ FPS. Code and weights at:
https://github.com/PKU-VCL-3DV/SLAM3R.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepClean: Integrated Distortion Identification and Algorithm Selection
  for Rectifying Image Corruptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kapoor, Harshad Khadilkar, Jayvardhana Gubbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distortion identification and rectification in images and videos is vital for
achieving good performance in downstream vision applications. Instead of
relying on fixed trial-and-error based image processing pipelines, we propose a
two-level sequential planning approach for automated image distortion
classification and rectification. At the higher level it detects the class of
corruptions present in the input image, if any. The lower level selects a
specific algorithm to be applied, from a set of externally provided candidate
algorithms. The entire two-level setup runs in the form of a single forward
pass during inference and it is to be queried iteratively until the retrieval
of the original image. We demonstrate improvements compared to three baselines
on the object detection task on COCO image dataset with rich set of
distortions. The advantage of our approach is its dynamic reconfiguration,
conditioned on the input image and generalisability to unseen candidate
algorithms at inference time, since it relies only on the comparison of their
output of the image embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FashionComposer: Compositional Fashion Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FashionComposer for compositional fashion image generation. Unlike
previous methods, FashionComposer is highly flexible. It takes multi-modal
input (i.e., text prompt, parametric human model, garment image, and face
image) and supports personalizing the appearance, pose, and figure of the human
and assigning multiple garments in one pass. To achieve this, we first develop
a universal framework capable of handling diverse input modalities. We
construct scaled training data to enhance the model's robust compositional
capabilities. To accommodate multiple reference images (garments and faces)
seamlessly, we organize these references in a single image as an "asset
library" and employ a reference UNet to extract appearance features. To inject
the appearance features into the correct pixels in the generated result, we
propose subject-binding attention. It binds the appearance features from
different "assets" with the corresponding text features. In this way, the model
could understand each asset according to their semantics, supporting arbitrary
numbers and types of reference images. As a comprehensive solution,
FashionComposer also supports many other applications like human album
generation, diverse virtual try-on tasks, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://sihuiji.github.io/FashionComposer-Page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and
  BEV Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01812v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01812v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ground-to-aerial image synthesis focuses on generating realistic aerial
images from corresponding ground street view images while maintaining
consistent content layout, simulating a top-down view. The significant
viewpoint difference leads to domain gaps between views, and dense urban scenes
limit the visible range of street views, making this cross-view generation task
particularly challenging. In this paper, we introduce SkyDiffusion, a novel
cross-view generation method for synthesizing aerial images from street view
images, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The
Curved-BEV method in SkyDiffusion converts street-view images into a BEV
perspective, effectively bridging the domain gap, and employs a "multi-to-one"
mapping strategy to address occlusion issues in dense urban scenes. Next,
SkyDiffusion designed a BEV-guided diffusion model to generate
content-consistent and realistic aerial images. Additionally, we introduce a
novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image
synthesis applications, including disaster scene aerial synthesis, historical
high-resolution satellite image synthesis, and low-altitude UAV image synthesis
tasks. Experimental results demonstrate that SkyDiffusion outperforms
state-of-the-art methods on cross-view datasets across natural (CVUSA),
suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios
(G2A-3), achieving realistic and content-consistent aerial image generation.
More result and dataset information can be found at
https://opendatalab.github.io/skydiffusion/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Training-Free to Adaptive: Empirical Insights into MLLMs'
  Understanding of Detection Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17981v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17981v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive capabilities of Multimodal Large Language Models
(MLLMs) in integrating text and image modalities, challenges remain in
accurately interpreting detailed visual elements. Vision detection models excel
at recognizing fine-grained image details, prompting researchers to use them to
enhance MLLMs. One effective strategy is to infuse detection information in
text format, which has proven simple and effective. However, most studies
utilize this method without training, leaving the potential of adaptive
training largely unexplored. Adaptive training could significantly enhance
MLLMs' comprehension of unique inputs while filtering out irrelevant
information. This paper addresses the crucial question: How does training
impact MLLMs' understanding of infused textual detection information? We
systematically experiment with various representative models to evaluate the
effects of training-free, retraining, and fine-tuning strategies. We also
examine the influence of training on MLLMs' original abilities and the
interchangeability of detection models. Our findings indicate that fine-tuning
a pre-trained MLLM to incorporate textual detection information delivers
superior results compared to training-free and retraining methods, improving
performance by 6.71% across 10 widely recognized benchmarks. Furthermore,
fine-tuning enables MLLMs to retain performance enhancements even when
detection models are swapped, indicating improved understanding of formatted
textual data. We release our codes to support further exploration of fusion
strategies for vision detection models and the enhancement of MLLMs'
fine-grained multimodal capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 22 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for
  Adversarial Defense <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23091v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23091v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite ongoing efforts to defend neural classifiers from adversarial
attacks, they remain vulnerable, especially to unseen attacks. In contrast,
humans are difficult to be cheated by subtle manipulations, since we make
judgments only based on essential factors. Inspired by this observation, we
attempt to model label generation with essential label-causative factors and
incorporate label-non-causative factors to assist data generation. For an
adversarial example, we aim to discriminate the perturbations as non-causative
factors and make predictions only based on the label-causative factors.
Concretely, we propose a casual diffusion model (CausalDiff) that adapts
diffusion models for conditional data generation and disentangles the two types
of casual factors by learning towards a novel casual information bottleneck
objective. Empirically, CausalDiff has significantly outperformed
state-of-the-art defense methods on various unseen attacks, achieving an
average robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on
CIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition
Benchmark). The code is available at
https://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training <span class="highlight-title">Dataset</span>s Generation for Machine Learning: Application to Vision
  Based Navigation <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Hans Krüger, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Based Navigation consists in utilizing cameras as precision sensors
for GNC after extracting information from images. To enable the adoption of
machine learning for space applications, one of obstacles is the demonstration
that available training datasets are adequate to validate the algorithms. The
objective of the study is to generate datasets of images and metadata suitable
for training machine learning algorithms. Two use cases were selected and a
robust methodology was developed to validate the datasets including the ground
truth. The first use case is in-orbit rendezvous with a man-made object: a
mockup of satellite ENVISAT. The second use case is a Lunar landing scenario.
Datasets were produced from archival datasets (Chang'e 3), from the laboratory
at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software
high fidelity image simulator using Model Capture and from Generative
Adversarial Networks. The use case definition included the selection of
algorithms as benchmark: an AI-based pose estimation algorithm and a dense
optical flow algorithm were selected. Eventually it is demonstrated that
datasets produced with SurRender and selected laboratory facilities are
adequate to train machine learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, preprint of the proceedings of ESA SPAICE
  conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Img-Diff: Contrastive Data Synthesis for Multimodal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04594v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04594v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qirui Jiao, Daoyuan Chen, Yilun Huang, Bolin Ding, Yaliang Li, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-performance Multimodal Large Language Models (MLLMs) are heavily
dependent on data quality. To advance fine-grained image recognition within
MLLMs, we introduce a novel data synthesis method inspired by contrastive
learning and image difference captioning. Our key idea involves challenging the
model to discern both matching and distinct elements by scrutinizing object
differences in detailed regions across similar images. We begin by generating
pairs of similar images that emphasize object variations. Following this, we
employ a Difference Area Generator to pinpoint object differences, and
subsequently, a Difference Captions Generator to articulate these differences.
This process results in a high-quality dataset of "object replacement" samples,
termed Img-Diff, which can be scaled as needed due to its automated nature. We
leverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs,
such as InternVL2, achieving substantial improvements across various image
difference and Visual Question Answering tasks. Notably, the trained models
significantly outperform existing SOTA models like GPT-4V and Gemini on the
MMVP benchmark. Additionally, we conduct comprehensive evaluations to validate
the dataset's diversity, quality, and robustness, offering several insights
into the synthesis of such contrastive datasets. We release our codes and
dataset to encourage further research on multimodal data synthesis and MLLMs'
fundamental capabilities for image understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Pixel is All I Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deng Siqin, Zhou Xiaoyi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have achieved record-breaking performance in
various visual tasks. However, concerns about their robustness against backdoor
attacks have grown. Backdoor attacks involve associating a specific trigger
with a target label, causing the model to predict the attacker-specified label
when the trigger is present, while correctly identifying clean images.We found
that ViTs exhibit higher attack success rates for quasi-triggers(patterns
different from but similar to the original training triggers)compared to CNNs.
Moreover, some backdoor features in clean samples can suppress the original
trigger, making quasi-triggers more effective.To better understand and exploit
these vulnerabilities, we developed a tool called the Perturbation Sensitivity
Distribution Map (PSDM). PSDM computes and sums gradients over many inputs to
show how sensitive the model is to small changes in the input. In ViTs, PSDM
reveals a patch-like pattern where central pixels are more sensitive than
edges. We use PSDM to guide the creation of quasi-triggers.Based on these
findings, we designed "WorstVIT," a simple yet effective data poisoning
backdoor for ViT models. This attack requires an extremely low poisoning rate,
trains for just one epoch, and modifies a single pixel to successfully attack
all validation images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction-Feedback DETR for Temporal Action Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihwan Kim, Miso Lee, Cheol-Ho Cho, Jihyun Lee, Jae-Pil Heo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Action Detection (TAD) is fundamental yet challenging for real-world
video applications. Leveraging the unique benefits of transformers, various
DETR-based approaches have been adopted in TAD. However, it has recently been
identified that the attention collapse in self-attention causes the performance
degradation of DETR for TAD. Building upon previous research, this paper newly
addresses the attention collapse problem in cross-attention within DETR-based
TAD methods. Moreover, our findings reveal that cross-attention exhibits
patterns distinct from predictions, indicating a short-cut phenomenon. To
resolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),
which utilizes predictions to restore the collapse and align the cross- and
self-attention with predictions. Specifically, we devise novel
prediction-feedback objectives using guidance from the relations of the
predictions. As a result, Pred-DETR significantly alleviates the collapse and
achieves state-of-the-art performance among DETR-based methods on various
challenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and
FineAction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding a Diffusion Model with a Bad Version of Itself <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary axes of interest in image-generating diffusion models are image
quality, the amount of variation in the results, and how well the results align
with a given condition, e.g., a class label or a text prompt. The popular
classifier-free guidance approach uses an unconditional model to guide a
conditional model, leading to simultaneously better prompt alignment and
higher-quality images at the cost of reduced variation. These effects seem
inherently entangled, and thus hard to control. We make the surprising
observation that it is possible to obtain disentangled control over image
quality without compromising the amount of variation by guiding generation
using a smaller, less-trained version of the model itself rather than an
unconditional model. This leads to significant improvements in ImageNet
generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using
publicly available networks. Furthermore, the method is also applicable to
unconditional diffusion models, drastically improving their quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activity Recognition on Avatar-Anonymized <span class="highlight-title">Dataset</span>s with Masked
  Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Schneider, Sina Sajadmanesh, Vikash Sehwag, Saquib Sarfraz, Rainer Stiefelhagen, Lingjuan Lyu, Vivek Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy-preserving computer vision is an important emerging problem in
machine learning and artificial intelligence. Prevalent methods tackling this
problem use differential privacy (DP) or obfuscation techniques to protect the
privacy of individuals. In both cases, the utility of the trained model is
sacrificed heavily in this process. In this work, we present an anonymization
pipeline that replaces sensitive human subjects in video datasets with
synthetic avatars within context, employing a combined rendering and stable
diffusion-based strategy. Additionally we propose masked differential privacy
({MaskDP}) to protect non-anonymized but privacy sensitive background
information. MaskDP allows for controlling sensitive regions where differential
privacy is applied, in contrast to applying DP on the entire input. This
combined methodology provides strong privacy protection while minimizing the
usual performance penalty of privacy preserving methods. Experiments on
multiple challenging action recognition datasets demonstrate that our proposed
techniques result in better utility-privacy trade-offs compared to standard
differentially private training in the especially demanding $\epsilon<1$
regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding 3D scenes is a crucial challenge in computer vision research
with applications spanning multiple domains. Recent advancements in distilling
2D vision-language foundation models into neural fields, like NeRF and 3DGS,
enable open-vocabulary segmentation of 3D scenes from 2D multi-view images
without the need for precise 3D annotations. However, while effective, these
methods typically rely on the per-pixel distillation of high-dimensional CLIP
features, introducing ambiguity and necessitating complex regularization
strategies, which adds inefficiency during training. This paper presents
MaskField, which enables efficient 3D open-vocabulary segmentation with neural
fields from a novel perspective. Unlike previous methods, MaskField decomposes
the distillation of mask and semantic features from foundation models by
formulating a mask feature field and queries. MaskField overcomes ambiguous
object boundaries by naturally introducing SAM segmented object shapes without
extra regularization during training. By circumventing the direct handling of
dense high-dimensional CLIP features during training, MaskField is particularly
compatible with explicit scene representations like 3DGS. Our extensive
experiments show that MaskField not only surpasses prior state-of-the-art
methods but also achieves remarkably fast convergence. We hope that MaskField
will inspire further exploration into how neural fields can be trained to
comprehend 3D scenes from 2D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, Code:https://github.com/keloee/MaskField</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Classification with Rotation-Invariant Variational Quantum
  Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul San Sebastian, Mikel Cañizo, Román Orús
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational quantum algorithms are gaining attention as an early application
of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of
variational methods lies in the phenomenon of Barren Plateaus, present in the
optimization of variational parameters. Adding geometric inductive bias to the
quantum models has been proposed as a potential solution to mitigate this
problem, leading to a new field called Geometric Quantum Machine Learning. In
this work, an equivariant architecture for variational quantum classifiers is
introduced to create a label-invariant model for image classification with
$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against
two different architectures, and it is experimentally observed that the
geometric approach boosts the model's performance. Finally, a classical
equivariant convolution operation is proposed to extend the quantum model for
the processing of larger images, employing the resources available in NISQ
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversifying Query: Region-Guided <span class="highlight-title">Transformer</span> for Temporal Sentence
  Grounding <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Sun, Liushuai Shi, Le Wang, Sanping Zhou, Kun Xia, Yabing Wang, Gang Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding is a challenging task that aims to localize the
moment spans relevant to a language description. Although recent DETR-based
models have achieved notable progress by leveraging multiple learnable moment
queries, they suffer from overlapped and redundant proposals, leading to
inaccurate predictions. We attribute this limitation to the lack of
task-related guidance for the learnable queries to serve a specific mode.
Furthermore, the complex solution space generated by variable and
open-vocabulary language descriptions complicates optimization, making it
harder for learnable queries to distinguish each other adaptively. To tackle
this limitation, we present a Region-Guided TRansformer (RGTR) for temporal
sentence grounding, which diversifies moment queries to eliminate overlapped
and redundant predictions. Instead of using learnable queries, RGTR adopts a
set of anchor pairs as moment queries to introduce explicit regional guidance.
Each anchor pair takes charge of moment prediction for a specific temporal
region, which reduces the optimization difficulty and ensures the diversity of
the final predictions. In addition, we design an IoU-aware scoring head to
improve proposal quality. Extensive experiments demonstrate the effectiveness
of RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA
and TACoS datasets. Codes are available at https://github.com/TensorsSun/RGTR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-25. Code is available at
  https://github.com/TensorsSun/RGTR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable Breast Cancer Molecular Subtype Prediction based on
  uncertainty-aware Bayesian Deep Learning by Mammography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohaddeseh Chegini, Ali Mahloojifar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is a heterogeneous disease with different molecular subtypes,
clinical behavior, treatment responses as well as survival outcomes. The
development of a reliable, accurate, available and inexpensive method to
predict the molecular subtypes using medical images plays an important role in
the diagnosis and prognosis of breast cancer. Recently, deep learning methods
have shown good performance in the breast cancer classification tasks using
various medical images. Despite all that success, classical deep learning
cannot deliver the predictive uncertainty. The uncertainty represents the
validity of the predictions. Therefore, the high predicted uncertainty might
cause a negative effect in the accurate diagnosis of breast cancer molecular
subtypes. To overcome this, uncertainty quantification methods are used to
determine the predictive uncertainty. Accordingly, in this study, we proposed
an uncertainty-aware Bayesian deep learning model using the full mammogram
images. In addition, to increase the performance of the multi-class molecular
subtype classification task, we proposed a novel hierarchical classification
strategy, named the two-stage classification strategy. The separate AUC of the
proposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,
luminal and triple-negative classes, respectively. The proposed model not only
has a comparable performance to other studies in the field of breast cancer
molecular subtypes prediction, even using full mammography images, but it is
also more reliable, due to quantify the predictive uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in
  the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donggyun Kim, Seongwoong Cho, Semin Kim, Chong Luo, Seunghoon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have evolved data-efficient generalists, benefiting
from the universal language interface and large-scale pre-training. However,
constructing a data-efficient generalist for dense visual prediction presents a
distinct challenge due to the variation in label structures across different
tasks. Consequently, generalization to unseen dense prediction tasks in the
low-data regime is not straightforward and has received less attention from
previous vision generalists. In this study, we explore a universal model that
can flexibly adapt to unseen dense label structures with a few examples,
enabling it to serve as a data-efficient vision generalist in diverse
real-world scenarios. To this end, we base our method on a powerful
meta-learning framework and explore several axes to improve its performance and
versatility for real-world problems, such as flexible adaptation mechanisms and
scalability. We evaluate our model across a spectrum of unseen real-world
scenarios where low-shot learning is desirable, including video, 3D, medical,
biological, and user-interactive tasks. Equipped with a generic architecture
and an effective adaptation mechanism, our model flexibly adapts to all of
these tasks with at most 50 labeled images, showcasing a significant
advancement over existing data-efficient generalist approaches. Codes are
available at https://github.com/GitGyun/chameleon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential
  via Self-Attention Redirection Guidance <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion models have emerged as promising newcomers in the field
of generative models, shining brightly in image generation. However, when
employed for object removal tasks, they still encounter issues such as
generating random artifacts and the incapacity to repaint foreground object
areas with appropriate content after removal. To tackle these problems, we
propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion
models for stable and effective object removal. Firstly, in light of the
observation that the self-attention maps influence the structure and shape
details of the generated images, we propose Attention Activation and
Suppression (ASS), which re-engineers the self-attention mechanism within the
pre-trained diffusion models based on the given mask, thereby prioritizing the
background over the foreground object during the reverse generation process.
Moreover, we introduce Self-Attention Redirection Guidance (SARG), which
utilizes the self-attention redirected by ASS to guide the generation process,
effectively removing foreground objects within the mask while simultaneously
generating content that is both plausible and coherent. Experiments demonstrate
the stability and effectiveness of Attentive Eraser in object removal across a
variety of pre-trained diffusion models, outperforming even training-based
methods. Furthermore, Attentive Eraser can be implemented in various diffusion
model architectures and checkpoints, enabling excellent scalability. Code is
available at https://github.com/Anonym0u3/AttentiveEraser.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution-Consistency-Guided Multi-modal Hashing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Yu Liu, Xian-Ling Mao, Tian-Yi Che, Rong-Cheng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal hashing methods have gained popularity due to their fast speed
and low storage requirements. Among them, the supervised methods demonstrate
better performance by utilizing labels as supervisory signals compared with
unsupervised methods. Currently, for almost all supervised multi-modal hashing
methods, there is a hidden assumption that training sets have no noisy labels.
However, labels are often annotated incorrectly due to manual labeling in
real-world scenarios, which will greatly harm the retrieval performance. To
address this issue, we first discover a significant distribution consistency
pattern through experiments, i.e., the 1-0 distribution of the presence or
absence of each category in the label is consistent with the high-low
distribution of similarity scores of the hash codes relative to category
centers. Then, inspired by this pattern, we propose a novel
Distribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to
filter and reconstruct noisy labels to enhance retrieval performance.
Specifically, the proposed method first randomly initializes several category
centers, which are used to compute the high-low distribution of similarity
scores; Noisy and clean labels are then separately filtered out via the
discovered distribution consistency pattern to mitigate the impact of noisy
labels; Subsequently, a correction strategy, which is indirectly designed via
the distribution consistency pattern, is applied to the filtered noisy labels,
correcting high-confidence ones while treating low-confidence ones as unlabeled
for unsupervised learning, thereby further enhancing the model's performance.
Extensive experiments on three widely used datasets demonstrate the superiority
of the proposed method compared to state-of-the-art baselines in multi-modal
retrieval tasks. The code is available at
https://github.com/LiuJinyu1229/DCGMH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Anthropometric Measurements to Improve Human Mesh Estimation
  and Ensure Consistent Body Shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katja Ludwig, Julian Lorenz, Daniel Kienzle, Tuan Bui, Rainer Lienhart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The basic body shape (i.e., the body shape in T-pose) of a person does not
change within a single video. However, most SOTA human mesh estimation (HME)
models output a slightly different, thus inconsistent basic body shape for each
video frame. Furthermore, we find that SOTA 3D human pose estimation (HPE)
models outperform HME models regarding the precision of the estimated 3D
keypoint positions. We solve the problem of inconsistent body shapes by
leveraging anthropometric measurements like taken by tailors from humans. We
create a model called A2B that converts given anthropometric measurements to
basic body shape parameters of human mesh models. We obtain superior and
consistent human meshes by combining the A2B model results with the keypoints
of 3D HPE models using inverse kinematics. We evaluate our approach on
challenging datasets like ASPset or fit3D, where we can lower the MPJPE by over
30 mm compared to SOTA HME models. Further, replacing estimates of the body
shape parameters from existing HME models with A2B results not only increases
the performance of these HME models, but also guarantees consistent body
shapes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DocKylin: A Large Multimodal Model for Visual Document Understanding
  with Efficient Visual Slimming <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19101v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19101v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Zhang, Wentao Yang, Songxuan Lai, Zecheng Xie, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal large language models (MLLMs) face significant challenges
in visual document understanding (VDU) tasks due to the high resolution, dense
text, and complex layouts typical of document images. These characteristics
demand a high level of detail perception ability from MLLMs. While increasing
input resolution improves detail perception capability, it also leads to longer
sequences of visual tokens, increasing computational costs and straining the
models' ability to handle long contexts. To address these challenges, we
introduce DocKylin, a document-centric MLLM that performs visual content
slimming at both the pixel and token levels, thereby reducing token sequence
length in VDU scenarios. We introduce an Adaptive Pixel Slimming (APS)
preprocessing module to perform pixel-level slimming, increasing the proportion
of informative pixels. Moreover, we propose a novel Dynamic Token Slimming
(DTS) module to conduct token-level slimming, filtering essential tokens and
removing others to adaptively create a more compact visual sequence.
Experiments demonstrate DocKylin's promising performance across various VDU
benchmarks and the effectiveness of each component.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POPoS: Improving Efficient and Robust Facial Landmark Detection with
  Parallel Optimal Position Search <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09583v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09583v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong-Yang Xiang, Jun-Yan He, Zhi-Qi Cheng, Xiao Wu, Xian-Sheng Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving a balance between accuracy and efficiency is a critical challenge
in facial landmark detection (FLD). This paper introduces Parallel Optimal
Position Search (POPoS), a high-precision encoding-decoding framework designed
to address the limitations of traditional FLD methods. POPoS employs three key
contributions: (1) Pseudo-range multilateration is utilized to correct heatmap
errors, improving landmark localization accuracy. By integrating multiple
anchor points, it reduces the impact of individual heatmap inaccuracies,
leading to robust overall positioning. (2) To enhance the pseudo-range accuracy
of selected anchor points, a new loss function, named multilateration anchor
loss, is proposed. This loss function enhances the accuracy of the distance
map, mitigates the risk of local optima, and ensures optimal solutions. (3) A
single-step parallel computation algorithm is introduced, boosting
computational efficiency and reducing processing time. Extensive evaluations
across five benchmark datasets demonstrate that POPoS consistently outperforms
existing methods, particularly excelling in low-resolution heatmaps scenarios
with minimal computational overhead. These advantages make POPoS a highly
efficient and accurate tool for FLD, with broad applicability in real-world
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025, 9 pages, 6 figures. Code:
  https://github.com/teslatasy/POPoS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian
  Splatting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Xu, Zexin Fan, Jian Yang, Jin Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Gaussian splatting has received more and more attention in the
field of static scene rendering. Due to the low computational overhead and
inherent flexibility of explicit representations, plane-based explicit methods
are popular ways to predict deformations for Gaussian-based dynamic scene
rendering models. However, plane-based methods rely on the inappropriate
low-rank assumption and excessively decompose the space-time 4D encoding,
resulting in overmuch feature overlap and unsatisfactory rendering quality. To
tackle these problems, we propose Grid4D, a dynamic scene rendering model based
on Gaussian splatting and employing a novel explicit encoding method for the 4D
input through the hash encoding. Different from plane-based explicit
representations, we decompose the 4D encoding into one spatial and three
temporal 3D hash encodings without the low-rank assumption. Additionally, we
design a novel attention module that generates the attention scores in a
directional range to aggregate the spatial and temporal features. The
directional attention enables Grid4D to more accurately fit the diverse
deformations across distinct scene components based on the spatial encoded
features. Moreover, to mitigate the inherent lack of smoothness in explicit
representation methods, we introduce a smooth regularization term that keeps
our model from the chaos of deformation prediction. Our experiments demonstrate
that Grid4D significantly outperforms the state-of-the-art models in visual
quality and rendering speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language
  Generation with Knowledge Graph for Explaining Thoracic Pathologies <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameer Hamza,  Abdullah, Yong Hyun Ahn, Sungyoung Lee, Seong Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating Natural Language Explanations (NLEs) for model predictions on
medical images, particularly those depicting thoracic pathologies, remains a
critical and challenging task. Existing methodologies often struggle due to
general models' insufficient domain-specific medical knowledge and privacy
concerns associated with retrieval-based augmentation techniques. To address
these issues, we propose a novel Vision-Language framework augmented with a
Knowledge Graph (KG)-based datastore, which enhances the model's understanding
by incorporating additional domain-specific medical knowledge essential for
generating accurate and informative NLEs. Our framework employs a KG-based
retrieval mechanism that not only improves the precision of the generated
explanations but also preserves data privacy by avoiding direct data retrieval.
The KG datastore is designed as a plug-and-play module, allowing for seamless
integration with various model architectures. We introduce and evaluate three
distinct frameworks within this paradigm: KG-LLaVA, which integrates the
pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining
MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts
LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are
validated on the MIMIC-NLE dataset, where they achieve state-of-the-art
results, underscoring the effectiveness of KG augmentation in generating
high-quality NLEs for thoracic pathologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoMeO: Robust Metric Visual Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Cheng, Zhipeng Cai, Zhaoxing Zhang, Wei Yin, Matthias Muller, Michael Paulitsch, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a
fundamental building block for many applications such as VR/AR and robotics.
This work focuses on monocular RGB VO where the input is a monocular RGB video
without IMU or 3D sensors. Existing approaches lack robustness under this
challenging scenario and fail to generalize to unseen data (especially
outdoors); they also cannot recover metric-scale poses. We propose Robust
Metric Visual Odometry (RoMeO), a novel method that resolves these issues
leveraging priors from pre-trained depth models. RoMeO incorporates both
monocular metric depth and multi-view stereo (MVS) models to recover
metric-scale, simplify correspondence search, provide better initialization and
regularize optimization. Effective strategies are proposed to inject noise
during training and adaptively filter noisy depth priors, which ensure the
robustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the
state-of-the-art (SOTA) by a large margin across 6 diverse datasets covering
both indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO
reduces the relative (align the trajectory scale with GT) and absolute
trajectory errors both by >50%. The performance gain also transfers to the full
SLAM pipeline (with global BA & loop closure). Code will be released upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Forget-free Winning Subnetworks for Video
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11973v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11973v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Jaehong Yoon, Sung Ju Hwang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) and Task-agnostic
Incremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning
(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is
designed to prevent overfitting when the data samples are scarce. Furthermore,
the sparse reuse of WSN weights is considered for Video Incremental Learning
(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It
enables compact encoding of videos and identifies reusable subnetworks across
varying bandwidths. We have integrated FSO into different architectural
frameworks for continual learning, including VIL, TIL, and FSCIL. Our
comprehensive experiments demonstrate FSO's effectiveness, significantly
improving task performance at various convolutional representational levels.
Specifically, FSO enhances higher-layer performance in TIL and FSCIL and
lower-layer performance in VIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Pattern Analysis and Machine Intelligence
  (T-PAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recoverable Compression: A Multimodal Vision Token Recovery Mechanism
  Guided by Text Information <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01179v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01179v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of large-scale language modeling techniques, large
multimodal models combining visual encoders with large language models have
demonstrated exceptional performance in various visual tasks. Most of the
current large-scale multimodal models achieve this by mapping visual features
obtained from the visual encoder into a large language model and using them as
inputs alongside text for downstream tasks. Therefore, the number of visual
tokens directly affects the training and inference speed of the model. There
has been significant work on token pruning for visual transformers, but for
large multimodal models, only relying on visual information for token pruning
or compression may lead to significant loss of important information. On the
other hand, the textual input in the form of a question may contain valuable
information that can aid in answering the question, providing additional
knowledge to the model. To address the potential oversimplification and
excessive pruning that can occur with most purely visual token pruning methods,
we propose a text information-guided dynamic visual token recovery mechanism
that does not require training. This mechanism leverages the similarity between
the question text and visual tokens to recover visually meaningful tokens with
important text information while merging other less important tokens.
Experimental results demonstrate that our proposed method achieves comparable
performance to the original approach while compressing the visual tokens to an
average of 10% of the original quantity. Our source code will be made publicly
available following acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time
  Adaptation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world vision models in dynamic environments face rapid shifts in domain
distributions, leading to decreased recognition performance. Using unlabeled
test data, continuous test-time adaptation (CTTA) directly adjusts a
pre-trained source discriminative model to these changing domains. A highly
effective CTTA method involves applying layer-wise adaptive learning rates for
selectively adapting pre-trained layers. However, it suffers from the poor
estimation of domain shift and the inaccuracies arising from the pseudo-labels.
This work aims to overcome these limitations by identifying layers for
adaptation via quantifying model prediction uncertainty without relying on
pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by
backpropagating the KL divergence between the softmax output and a uniform
distribution, to select layers for further adaptation. Subsequently, for the
parameters exclusively belonging to these selected layers, with the remaining
ones frozen, we evaluate their sensitivity to approximate the domain shift and
adjust their learning rates accordingly. We conduct extensive image
classification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,
demonstrating the superior efficacy of our method compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust
  Out-of-Distribution Human Action Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20633v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20633v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu, Anqi Zhu, Jingyu Lin, Qiuhong Ke, Cunjian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human action recognition is crucial in computer vision systems. However, in
real-world scenarios, human actions often fall outside the distribution of
training data, requiring a model to both recognize in-distribution (ID) actions
and reject out-of-distribution (OOD) ones. Despite its importance, there has
been limited research on OOD detection in human actions. Existing works on OOD
detection mainly focus on image data with RGB structure, and many methods are
post-hoc in nature. While these methods are convenient and computationally
efficient, they often lack sufficient accuracy, fail to consider the exposure
of OOD samples, and ignore the application in skeleton structure data. To
address these challenges, we propose a novel end-to-end skeleton-based model
called Skeleton-OOD, which is committed to improving the effectiveness of OOD
tasks while ensuring the accuracy of ID recognition. Through extensive
experiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400
datasets, Skeleton-OOD demonstrates the superior performance of our proposed
approach compared to state-of-the-art methods. Our findings underscore the
effectiveness of classic OOD detection techniques in the context of
skeleton-based action recognition tasks, offering promising avenues for future
research in this field. Code is available at
https://github.com/YilliaJing/Skeleton-OOD.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurocomputing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-Shadow: Global-guided Diffusion Model for Shadow Removal <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinting Luo, Ru Li, Chengzhi Jiang, Xiaoming Zhang, Mingyan Han, Ting Jiang, Haoqiang Fan, Shuaicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Diff-Shadow, a global-guided diffusion model for shadow removal.
Previous transformer-based approaches can utilize global information to relate
shadow and non-shadow regions but are limited in their synthesis ability and
recover images with obvious boundaries. In contrast, diffusion-based methods
can generate better content but they are not exempt from issues related to
inconsistent illumination. In this work, we combine the advantages of diffusion
models and global guidance to achieve shadow-free restoration. Specifically, we
propose a parallel UNets architecture: 1) the local branch performs the
patch-based noise estimation in the diffusion process, and 2) the global branch
recovers the low-resolution shadow-free images. A Reweight Cross Attention
(RCA) module is designed to integrate global contextual information of
non-shadow regions into the local branch. We further design a Global-guided
Sampling Strategy (GSS) that mitigates patch boundary issues and ensures
consistent illumination across shaded and unshaded regions in the recovered
image. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have
demonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art
methods, our method achieves a significant improvement in terms of PSNR,
increasing from 32.33dB to 33.69dB on the ISTD dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 39th Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Multi-granular Alignments for Grounded Reasoning in Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang-Hung Le, Long Hoang Dang, Ngan Le, Truyen Tran, Thao Minh Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Large Vision-Language Models (LVLMs) excel at matching concepts
across multi-modal inputs but struggle with compositional concepts and
high-level relationships between entities. This paper introduces Progressive
multi-granular Vision-Language alignments (PromViL), a novel framework to
enhance LVLMs' ability in performing grounded compositional visual reasoning
tasks. Our approach constructs a hierarchical structure of multi-modal
alignments, ranging from simple to complex concepts. By progressively aligning
textual descriptions with corresponding visual regions, our model learns to
leverage contextual information from lower levels to inform higher-level
reasoning. To facilitate this learning process, we introduce a data generation
process that creates a novel dataset derived from Visual Genome, providing a
wide range of nested compositional vision-language pairs. Experimental results
demonstrate that our PromViL framework significantly outperforms baselines on
various visual grounding and compositional question answering tasks. The code
is available at: https://github.com/lqh52/PromViL.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit
  Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enis Simsar, Alessio Tonioni, Yongqin Xian, Thomas Hofmann, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an unsupervised model for instruction-based image editing that
eliminates the need for ground-truth edited images during training. Existing
supervised methods depend on datasets containing triplets of input image,
edited image, and edit instruction. These are generated by either existing
editing methods or human-annotations, which introduce biases and limit their
generalization ability. Our method addresses these challenges by introducing a
novel editing mechanism called Cycle Edit Consistency (CEC), which applies
forward and backward edits in one training step and enforces consistency in
image and attention spaces. This allows us to bypass the need for ground-truth
edited images and unlock training for the first time on datasets comprising
either real image-caption pairs or image-caption-edit triplets. We empirically
show that our unsupervised technique performs better across a broader range of
edits with high fidelity and precision. By eliminating the need for
pre-existing datasets of triplets, reducing biases associated with supervised
methods, and proposing CEC, our work represents a significant advancement in
unblocking scaling of instruction-based image editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://enis.dev/uip2p/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnvGS: Modeling View-Dependent Appearance with Environment Gaussian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, Sida Peng, Hujun Bao, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing complex reflections in real-world scenes from 2D images is
essential for achieving photorealistic novel view synthesis. Existing methods
that utilize environment maps to model reflections from distant lighting often
struggle with high-frequency reflection details and fail to account for
near-field reflections. In this work, we introduce EnvGS, a novel approach that
employs a set of Gaussian primitives as an explicit 3D representation for
capturing reflections of environments. These environment Gaussian primitives
are incorporated with base Gaussian primitives to model the appearance of the
whole scene. To efficiently render these environment Gaussian primitives, we
developed a ray-tracing-based renderer that leverages the GPU's RT core for
fast rendering. This allows us to jointly optimize our model for high-quality
reconstruction while maintaining real-time rendering speeds. Results from
multiple real-world and synthetic datasets demonstrate that our method produces
significantly more detailed reflections, achieving the best rendering quality
in real-time novel view synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/envgs/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flowing from Words to Pixels: A Framework for Cross-Modality Evolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihao Liu, Xi Yin, Alan Yuille, Andrew Brown, Mannat Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, and their generalization, flow matching, have had a
remarkable impact on the field of media generation. Here, the conventional
approach is to learn the complex mapping from a simple source distribution of
Gaussian noise to the target media distribution. For cross-modal tasks such as
text-to-image generation, this same mapping from noise to image is learnt
whilst including a conditioning mechanism in the model. One key and thus far
relatively unexplored feature of flow matching is that, unlike Diffusion
models, they are not constrained for the source distribution to be noise.
Hence, in this paper, we propose a paradigm shift, and ask the question of
whether we can instead train flow matching models to learn a direct mapping
from the distribution of one modality to the distribution of another, thus
obviating the need for both the noise distribution and conditioning mechanism.
We present a general and simple framework, CrossFlow, for cross-modal flow
matching. We show the importance of applying Variational Encoders to the input
data, and introduce a method to enable Classifier-free guidance. Surprisingly,
for text-to-image, CrossFlow with a vanilla transformer without cross attention
slightly outperforms standard flow matching, and we show that it scales better
with training steps and model size, while also allowing for interesting latent
arithmetic which results in semantically meaningful edits in the output space.
To demonstrate the generalizability of our approach, we also show that
CrossFlow is on par with or outperforms the state-of-the-art for various
cross-modal / intra-modal mapping tasks, viz. image captioning, depth
estimation, and image super-resolution. We hope this paper contributes to
accelerating progress in cross-modal media generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://cross-flow.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, Yujun Shen, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intuitive nature of drag-based interaction has led to its growing
adoption for controlling object trajectories in image-to-video synthesis.
Still, existing methods that perform dragging in the 2D space usually face
ambiguity when handling out-of-plane movements. In this work, we augment the
interaction with a new dimension, i.e., the depth dimension, such that users
are allowed to assign a relative depth for each point on the trajectory. That
way, our new interaction paradigm not only inherits the convenience from 2D
dragging, but facilitates trajectory control in the 3D space, broadening the
scope of creativity. We propose a pioneering method for 3D trajectory control
in image-to-video synthesis by abstracting object masks into a few cluster
points. These points, accompanied by the depth information and the instance
information, are finally fed into a video diffusion model as the control
signal. Extensive experiments validate the effectiveness of our approach,
dubbed LeviTor, in precisely manipulating the object movements when producing
photo-realistic videos from static images. Project page:
https://ppetrichor.github.io/levitor.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page available at
  https://ppetrichor.github.io/levitor.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multiview Relighting for 3D Reconstruction under Extreme
  Illumination Variation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadi Alzayer, Philipp Henzler, Jonathan T. Barron, Jia-Bin Huang, Pratul P. Srinivasan, Dor Verbin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing the geometry and appearance of objects from photographs taken
in different environments is difficult as the illumination and therefore the
object appearance vary across captured images. This is particularly challenging
for more specular objects whose appearance strongly depends on the viewing
direction. Some prior approaches model appearance variation across images using
a per-image embedding vector, while others use physically-based rendering to
recover the materials and per-image illumination. Such approaches fail at
faithfully recovering view-dependent appearance given significant variation in
input illumination and tend to produce mostly diffuse results. We present an
approach that reconstructs objects from images taken under different
illuminations by first relighting the images under a single reference
illumination with a multiview relighting diffusion model and then
reconstructing the object's geometry and appearance with a radiance field
architecture that is robust to the small remaining inconsistencies among the
relit images. We validate our proposed approach on both synthetic and real
datasets and demonstrate that it greatly outperforms existing techniques at
reconstructing high-fidelity appearance from images taken under extreme
illumination variation. Moreover, our approach is particularly effective at
recovering view-dependent "shiny" appearance which cannot be reconstructed by
prior methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://relight-to-reconstruct.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling 4D Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Vélez, Luisa Polanía, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Pătrăucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling has not yet been convincingly demonstrated for pure self-supervised
learning from video. However, prior work has focused evaluations on
semantic-related tasks $\unicode{x2013}$ action classification, ImageNet
classification, etc. In this paper we focus on evaluating self-supervised
learning on non-semantic vision tasks that are more spatial (3D) and temporal
(+1D = 4D), such as camera pose estimation, point and object tracking, and
depth estimation. We show that by learning from very large video datasets,
masked auto-encoding (MAE) with transformer video models actually scales,
consistently improving performance on these 4D tasks, as model size increases
from 20M all the way to the largest by far reported self-supervised video model
$\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with
many recent image and video models demonstrates the benefits of scaling 4D
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in Large Vision-Language Models (LVLMs),
existing pixel-grounding models operate on single-image settings, limiting
their ability to perform detailed, fine-grained comparisons across multiple
images. Conversely, current multi-image understanding models lack pixel-level
grounding. Our work addresses this gap by introducing the task of multi-image
pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates
pixel-level grounding with robust multi-image reasoning capabilities to produce
contextually rich, pixel-grounded explanations. Central to PRIMA is an
efficient vision module that queries fine-grained visual representations across
multiple images, reducing TFLOPs by $25.3\%$. To support training and
evaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark
consisting of $\sim$224K question-answer pairs that require fine-grained visual
understanding across multiple images. Experimental results demonstrate PRIMA
outperforms state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://plan-lab.github.io/prima</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the advent of Multimodal Large Language Models (MLLMs), they have made
a significant impact across a wide range of real-world applications,
particularly in Autonomous Driving (AD). Their ability to process complex
visual data and reason about intricate driving scenarios has paved the way for
a new paradigm in end-to-end AD systems. However, the progress of developing
end-to-end models for AD has been slow, as existing fine-tuning methods demand
substantial resources, including extensive computational power, large-scale
datasets, and significant funding. Drawing inspiration from recent advancements
in inference computing, we propose OpenEMMA, an open-source end-to-end
framework based on MLLMs. By incorporating the Chain-of-Thought reasoning
process, OpenEMMA achieves significant improvements compared to the baseline
when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates
effectiveness, generalizability, and robustness across a variety of challenging
driving scenarios, offering a more efficient and effective approach to
autonomous driving. We release all the codes in
https://github.com/taco-group/OpenEMMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large vision language models (VLMs) tailored for
autonomous driving (AD) have shown strong scene understanding and reasoning
capabilities, making them undeniable candidates for end-to-end driving systems.
However, limited work exists on studying the trustworthiness of DriveVLMs -- a
critical factor that directly impacts public transportation safety. In this
paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for
large vision-language models in autonomous driving (DriveVLMs), considering
diverse perspectives -- including trustfulness, safety, robustness, privacy,
and fairness. We constructed the largest visual question-answering dataset for
investigating trustworthiness issues in driving scenarios, comprising over 10k
unique scenes and 18k queries. We evaluated six publicly available VLMs,
spanning from generalist to specialist, from open-source to commercial models.
Our exhaustive evaluations have unveiled previously undiscovered
vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found
that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform
specialized models fine-tuned for driving in terms of overall trustworthiness.
DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing
sensitive information. Additionally, both generalist and specialist VLMs remain
susceptible to adversarial attacks and struggle to ensure unbiased
decision-making across diverse environments and populations. Our findings call
for immediate and decisive action to address the trustworthiness of DriveVLMs
-- an issue of critical importance to public safety and the welfare of all
citizens relying on autonomous transportation systems. Our benchmark is
publicly available at \url{https://github.com/taco-group/AutoTrust}, and the
leaderboard is released at \url{https://taco-group.github.io/AutoTrust/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, Liang-Chieh Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive (AR) modeling has achieved remarkable success in natural
language processing by enabling models to generate text with coherence and
contextual understanding through next token prediction. Recently, in image
generation, VAR proposes scale-wise autoregressive modeling, which extends the
next token prediction to the next scale prediction, preserving the 2D structure
of images. However, VAR encounters two primary challenges: (1) its complex and
rigid scale design limits generalization in next scale prediction, and (2) the
generator's dependence on a discrete tokenizer with the same complex scale
structure restricts modularity and flexibility in updating the tokenizer. To
address these limitations, we introduce FlowAR, a general next scale prediction
method featuring a streamlined scale design, where each subsequent scale is
simply double the previous one. This eliminates the need for VAR's intricate
multi-scale residual tokenizer and enables the use of any off-the-shelf
Variational AutoEncoder (VAE). Our simplified design enhances generalization in
next scale prediction and facilitates the integration of Flow Matching for
high-quality image synthesis. We validate the effectiveness of FlowAR on the
challenging ImageNet-256 benchmark, demonstrating superior generation
performance compared to previous methods. Codes will be available at
\url{https://github.com/OliverRensu/FlowAR}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation
  for High-quality 3D Asset Creation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Zhao, Yan-Pei Cao, Jiale Xu, Yuejiang Dong, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedural Content Generation (PCG) is powerful in creating high-quality 3D
contents, yet controlling it to produce desired shapes is difficult and often
requires extensive parameter tuning. Inverse Procedural Content Generation aims
to automatically find the best parameters under the input condition. However,
existing sampling-based and neural network-based methods still suffer from
numerous sample iterations or limited controllability. In this work, we present
DI-PCG, a novel and efficient method for Inverse PCG from general image
conditions. At its core is a lightweight diffusion transformer model, where PCG
parameters are directly treated as the denoising target and the observed images
as conditions to control parameter generation. DI-PCG is efficient and
effective. With only 7.6M network parameters and 30 GPU hours to train, it
demonstrates superior performance in recovering parameters accurately, and
generalizing well to in-the-wild images. Quantitative and qualitative
experiment results validate the effectiveness of DI-PCG in inverse PCG and
image-to-3D generation tasks. DI-PCG offers a promising approach for efficient
inverse PCG and represents a valuable exploration step towards a 3D generation
path that models how to construct a 3D asset using parametric models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://thuzhaowang.github.io/projects/DI-PCG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper targets the challenge of real-time LiDAR re-simulation in dynamic
driving scenarios. Recent approaches utilize neural radiance fields combined
with the physical modeling of LiDAR sensors to achieve high-fidelity
re-simulation results. Unfortunately, these methods face limitations due to
high computational demands in large-scale scenes and cannot perform real-time
LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel
framework that supports real-time, physically accurate LiDAR re-simulation for
driving scenes. Our primary contribution is the development of an efficient and
effective rendering pipeline, which integrates Gaussian primitives and
hardware-accelerated ray tracing technology. Specifically, we model the
physical properties of LiDAR sensors using Gaussian primitives with learnable
parameters and incorporate scene graphs to handle scene dynamics. Building upon
this scene representation, our framework first constructs a bounding volume
hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views
through a differentiable rendering algorithm. Importantly, our framework
supports realistic rendering with flexible scene editing operations and various
sensor configurations. Extensive experiments across multiple public benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
rendering quality and efficiency. Our project page is at
https://zju3dv.github.io/lidar-rt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/lidar-rt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preventing Local Pitfalls in Vector Quantization via Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-quantized networks (VQNs) have exhibited remarkable performance across
various tasks, yet they are prone to training instability, which complicates
the training process due to the necessity for techniques such as subtle
initialization and model distillation. In this study, we identify the local
minima issue as the primary cause of this instability. To address this, we
integrate an optimal transport method in place of the nearest neighbor search
to achieve a more globally informed assignment. We introduce OptVQ, a novel
vector quantization method that employs the Sinkhorn algorithm to optimize the
optimal transport problem, thereby enhancing the stability and efficiency of
the training process. To mitigate the influence of diverse data distributions
on the Sinkhorn algorithm, we implement a straightforward yet effective
normalization strategy. Our comprehensive experiments on image reconstruction
tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses
current state-of-the-art VQNs in reconstruction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/zbr17/OptVQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal
  Audio-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video
generation that leverages the activations of frozen video and audio diffusion
models for temporally-aligned cross-modal conditioning. The key to our
framework is a Fusion Block that enables bidirectional information exchange
between our backbone video and audio diffusion models through a
temporally-aligned self attention operation. Unlike prior work that uses
feature extractors pretrained for other tasks for the conditioning signal,
AV-Link can directly leverage features obtained by the complementary modality
in a single framework i.e. video features to generate audio, or audio features
to generate video. We extensively evaluate our design choices and demonstrate
the ability of our method to achieve synchronized and high-quality audiovisual
content, showcasing its potential for applications in immersive media
generation. Project Page: snap-research.github.io/AVLink/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: snap-research.github.io/AVLink/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EarthDial: Turning Multi-sensory Earth Observations to Interactive
  Dialogues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Soni, Akshay Dudhane, Hiyam Debary, Mustansar Fiaz, Muhammad Akhtar Munir, Muhammad Sohail Danish, Paolo Fraccaro, Campbell D Watson, Levente J Klein, Fahad Shahbaz Khan, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated analysis of vast Earth observation data via interactive
Vision-Language Models (VLMs) can unlock new opportunities for environmental
monitoring, disaster response, and resource management. Existing generic VLMs
do not perform well on Remote Sensing data, while the recent Geo-spatial VLMs
remain restricted to a fixed resolution and few sensor modalities. In this
paper, we introduce EarthDial, a conversational assistant specifically designed
for Earth Observation (EO) data, transforming complex, multi-sensory Earth
observations into interactive, natural language dialogues. EarthDial supports
multi-spectral, multi-temporal, and multi-resolution imagery, enabling a wide
range of remote sensing tasks, including classification, detection, captioning,
question answering, visual reasoning, and visual grounding. To achieve this, we
introduce an extensive instruction tuning dataset comprising over 11.11M
instruction pairs covering RGB, Synthetic Aperture Radar (SAR), and
multispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,
EarthDial handles bi-temporal and multi-temporal sequence analysis for
applications like change detection. Our extensive experimental results on 37
downstream applications demonstrate that EarthDial outperforms existing generic
and domain-specific models, achieving better generalization across various EO
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LlamaFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LlamaFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LlamaFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LlamaFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LlamaFusion improves image understanding by 20% and image generation by 3.6%
using only 50% of the FLOPs while maintaining Llama-3's language capabilities.
We also demonstrate that this framework can adapt existing vision-language
models with multimodal generation ability. Overall, this framework not only
leverages existing computational investments in text-only LLMs but also enables
the parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tiled Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Madar, Ohad Fried
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image tiling -- the seamless connection of disparate images to create a
coherent visual field -- is crucial for applications such as texture creation,
video game asset development, and digital art. Traditionally, tiles have been
constructed manually, a method that poses significant limitations in
scalability and flexibility. Recent research has attempted to automate this
process using generative models. However, current approaches primarily focus on
tiling textures and manipulating models for single-image generation, without
inherently supporting the creation of multiple interconnected tiles across
diverse domains. This paper presents Tiled Diffusion, a novel approach that
extends the capabilities of diffusion models to accommodate the generation of
cohesive tiling patterns across various domains of image synthesis that require
tiling. Our method supports a wide range of tiling scenarios, from self-tiling
to complex many-to-many connections, enabling seamless integration of multiple
images. Tiled Diffusion automates the tiling process, eliminating the need for
manual intervention and enhancing creative possibilities in various
applications, such as seamlessly tiling of existing images, tiled texture
creation, and 360{\deg} synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SqueezeMe: Efficient Gaussian Avatars for VR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15171v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15171v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunsuke Saito, Stanislav Pidhorskyi, Igor Santesteban, Forrest Iandola, Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Frank Yu, Emanuel Garbin, Tomas Simon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Splatting has enabled real-time 3D human avatars with unprecedented
levels of visual quality. While previous methods require a desktop GPU for
real-time inference of a single avatar, we aim to squeeze multiple Gaussian
avatars onto a portable virtual reality headset with real-time drivable
inference. We begin by training a previous work, Animatable Gaussians, on a
high quality dataset captured with 512 cameras. The Gaussians are animated by
controlling base set of Gaussians with linear blend skinning (LBS) motion and
then further adjusting the Gaussians with a neural network decoder to correct
their appearance. When deploying the model on a Meta Quest 3 VR headset, we
find two major computational bottlenecks: the decoder and the rendering. To
accelerate the decoder, we train the Gaussians in UV-space instead of
pixel-space, and we distill the decoder to a single neural network layer.
Further, we discover that neighborhoods of Gaussians can share a single
corrective from the decoder, which provides an additional speedup. To
accelerate the rendering, we develop a custom pipeline in Vulkan that runs on
the mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently
at 72 FPS on a VR headset. Demo videos are at
https://forresti.github.io/squeezeme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Initial version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OnlineVPO: Align Video Diffusion Model with Online Video-Centric
  Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zhang, Jie Wu, Weifeng Chen, Yatai Ji, Xuefeng Xiao, Weilin Huang, Kai Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the field of text-to-video (T2V) generation has made
significant strides. Despite this progress, there is still a gap between
theoretical advancements and practical application, amplified by issues like
degraded image quality and flickering artifacts. Recent advancements in
enhancing the video diffusion model (VDM) through feedback learning have shown
promising results. However, these methods still exhibit notable limitations,
such as misaligned feedback and inferior scalability. To tackle these issues,
we introduce OnlineVPO, a more efficient preference learning approach tailored
specifically for video diffusion models. Our method features two novel designs,
firstly, instead of directly using image-based reward feedback, we leverage the
video quality assessment (VQA) model trained on synthetic data as the reward
model to provide distribution and modality-aligned feedback on the video
diffusion model. Additionally, we introduce an online DPO algorithm to address
the off-policy optimization and scalability issue in existing video preference
learning frameworks. By employing the video reward model to offer concise video
feedback on the fly, OnlineVPO offers effective and efficient preference
guidance. Extensive experiments on the open-source video-diffusion model
demonstrate OnlineVPO as a simple yet effective and more importantly scalable
preference learning algorithm for video diffusion models, offering valuable
insights for future advancements in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-A-Video: <span class="highlight-title">Prompt</span> Your Video Diffusion Model via Preference-Aligned
  LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, Weilin Huang, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video models have made remarkable advancements through optimization
on high-quality text-video pairs, where the textual prompts play a pivotal role
in determining quality of output videos. However, achieving the desired output
often entails multiple revisions and iterative inference to refine
user-provided prompts. Current automatic methods for refining prompts encounter
challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware
when applied to text-to-video diffusion models. To address these problem, we
introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,
which excels in crafting Video-Centric, Labor-Free and Preference-Aligned
prompts tailored to specific video diffusion model. Our approach involves a
meticulously crafted two-stage optimization and alignment system. Initially, we
conduct a reward-guided prompt evolution pipeline to automatically create
optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the
LLM. Then multi-dimensional rewards are employed to generate pairwise data for
the SFT model, followed by the direct preference optimization (DPO) algorithm
to further facilitate preference alignment. Through extensive experimentation
and comparative analyses, we validate the effectiveness of Prompt-A-Video
across diverse generation models, highlighting its potential to push the
boundaries of video generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Color Channel Independence for Improved Unsupervised Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Jäckl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric architectures can learn to extract distinct object
representations from visual scenes, enabling downstream applications on the
object level. Similarly to autoencoder-based image models, object-centric
approaches have been trained on the unsupervised reconstruction loss of images
encoded by RGB color spaces. In our work, we challenge the common assumption
that RGB images are the optimal color space for unsupervised learning in
computer vision. We discuss conceptually and empirically that other color
spaces, such as HSV, bear essential characteristics for object-centric
representation learning, like robustness to lighting conditions. We further
show that models improve when requiring them to predict additional color
channels. Specifically, we propose to transform the predicted targets to the
RGB-S space, which extends RGB with HSV's saturation component and leads to
markedly better reconstruction and disentanglement for five common evaluation
datasets. The use of composite color spaces can be implemented with basically
no computational overhead, is agnostic of the models' architecture, and is
universally applicable across a wide range of visual computing tasks and
training types. The findings of our approach encourage additional
investigations in computer vision tasks beyond object-centric learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages incl. references, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jet: A Modern <span class="highlight-title">Transformer</span>-Based Normalizing Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kolesnikov, André Susano Pinto, Michael Tschannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past, normalizing generative flows have emerged as a promising class
of generative models for natural images. This type of model has many modeling
advantages: the ability to efficiently compute log-likelihood of the input
data, fast generation and simple overall structure. Normalizing flows remained
a topic of active research but later fell out of favor, as visual quality of
the samples was not competitive with other model classes, such as GANs,
VQ-VAE-based approaches or diffusion models. In this paper we revisit the
design of the coupling-based normalizing flow models by carefully ablating
prior design choices and using computational blocks based on the Vision
Transformer architecture, not convolutional neural networks. As a result, we
achieve state-of-the-art quantitative and qualitative performance with a much
simpler architecture. While the overall visual quality is still behind the
current state-of-the-art models, we argue that strong normalizing flow models
can help advancing research frontier by serving as building components of more
powerful generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallelized Autoregressive Visual Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive models have emerged as a powerful approach for visual
generation but suffer from slow inference speed due to their sequential
token-by-token prediction process. In this paper, we propose a simple yet
effective approach for parallelized autoregressive visual generation that
improves generation efficiency while preserving the advantages of
autoregressive modeling. Our key insight is that parallel generation depends on
visual token dependencies-tokens with weak dependencies can be generated in
parallel, while strongly dependent adjacent tokens are difficult to generate
together, as their independent sampling may lead to inconsistencies. Based on
this observation, we develop a parallel generation strategy that generates
distant tokens with weak dependencies in parallel while maintaining sequential
generation for strongly dependent local tokens. Our approach can be seamlessly
integrated into standard autoregressive models without modifying the
architecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that
our method achieves a 3.6x speedup with comparable quality and up to 9.5x
speedup with minimal quality degradation across both image and video generation
tasks. We hope this work will inspire future research in efficient visual
generation and unified autoregressive modeling. Project page:
https://epiphqny.github.io/PAR-project.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://epiphqny.github.io/PAR-project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowing Where to Focus: Attention-Guided Alignment for Text-based Person
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Tan, Weihao Li, Pingyang Dai, Jie Chen, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of Text-Based Person Search (TBPS), mainstream methods aim to
explore more efficient interaction frameworks between text descriptions and
visual data. However, recent approaches encounter two principal challenges.
Firstly, the widely used random-based Masked Language Modeling (MLM) considers
all the words in the text equally during training. However, massive
semantically vacuous words ('with', 'the', etc.) be masked fail to contribute
efficient interaction in the cross-modal MLM and hampers the representation
alignment. Secondly, manual descriptions in TBPS datasets are tedious and
inevitably contain several inaccuracies. To address these issues, we introduce
an Attention-Guided Alignment (AGA) framework featuring two innovative
components: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module
(TEM). AGM dynamically masks semantically meaningful words by aggregating the
attention weight derived from the text encoding process, thereby cross-modal
MLM can capture information related to the masked word from text context and
images and align their representations. Meanwhile, TEM alleviates low-quality
representations caused by repetitive and erroneous text descriptions by
replacing those semantically meaningful words with MLM's prediction. It not
only enriches text descriptions but also prevents overfitting. Extensive
experiments across three challenging benchmarks demonstrate the effectiveness
of our AGA, achieving new state-of-the-art results with Rank-1 accuracy
reaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid,
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Full <span class="highlight-title">Transformer</span>-based Framework for Automatic Pain Estimation using
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefanos Gkikas, Manolis Tsiknakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic estimation of pain is essential in designing an optimal pain
management system offering reliable assessment and reducing the suffering of
patients. In this study, we present a novel full transformer-based framework
consisting of a Transformer in Transformer (TNT) model and a Transformer
leveraging cross-attention and self-attention blocks. Elaborating on videos
from the BioVid database, we demonstrate state-of-the-art performances, showing
the efficacy, efficiency, and generalization capability across all the primary
pain estimation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Till the Layers Collapse: Compressing a Deep Neural Network through the
  Lenses of Batch Normalization Layers <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Liao, Nour Hezbri, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, deep neural networks are widely used since they can handle a variety
of complex tasks. Their generality makes them very powerful tools in modern
technology. However, deep neural networks are often overparameterized. The
usage of these large models consumes a lot of computation resources. In this
paper, we introduce a method called \textbf{T}ill the \textbf{L}ayers
\textbf{C}ollapse (TLC), which compresses deep neural networks through the
lenses of batch normalization layers. By reducing the depth of these networks,
our method decreases deep neural networks' computational requirements and
overall latency. We validate our method on popular models such as Swin-T,
MobileNet-V2, and RoBERTa, across both image classification and natural
language processing (NLP) tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging
  <span class="highlight-title">Dataset</span>s with In-Context Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical researchers and clinicians often need to perform novel segmentation
tasks on a set of related images. Existing methods for segmenting a new dataset
are either interactive, requiring substantial human effort for each image, or
require an existing set of manually labeled images. We introduce a system,
MultiverSeg, that enables practitioners to rapidly segment an entire new
dataset without requiring access to any existing labeled data from that task or
domain. Along with the image to segment, the model takes user interactions such
as clicks, bounding boxes or scribbles as input, and predicts a segmentation.
As the user segments more images, those images and segmentations become
additional inputs to the model, providing context. As the context set of
labeled images grows, the number of interactions required to segment each new
image decreases. We demonstrate that MultiverSeg enables users to interactively
segment new datasets efficiently, by amortizing the number of interactions per
image to achieve an accurate segmentation. Compared to using a state-of-the-art
interactive segmentation method, using MultiverSeg reduced the total number of
scribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images
from unseen tasks. We release code and model weights at
https://multiverseg.csail.mit.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://multiverseg.csail.mit.edu Keywords:
  interactive segmentation, in-context learning, medical image analysis,
  biomedical imaging, image annotation, visual prompting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GIRAFE: Glottal Imaging <span class="highlight-title">Dataset</span> for Advanced Segmentation, Analysis, and
  Facilitative Playbacks Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. Andrade-Miranda, K. Chatzipapas, J. D. Arias-Londoño, J. I. Godino-Llorente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advances in the development of Facilitative Playbacks extracted from
High-Speed videoendoscopic sequences of the vocal folds are hindered by a
notable lack of publicly available datasets annotated with the semantic
segmentations corresponding to the area of the glottal gap. This fact also
limits the reproducibility and further exploration of existing research in this
field.
  To address this gap, GIRAFE is a data repository designed to facilitate the
development of advanced techniques for the semantic segmentation, analysis, and
fast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The
repository includes 65 high-speed videoendoscopic recordings from a cohort of
50 patients (30 female, 20 male). The dataset comprises 15 recordings from
healthy controls, 26 from patients with diagnosed voice disorders, and 24 with
an unknown health condition. All of them were manually annotated by an expert,
including the masks corresponding to the semantic segmentation of the glottal
gap. The repository is also complemented with the automatic segmentation of the
glottal area using different state-of-the-art approaches.
  This data set has already supported several studies, which demonstrates its
usefulness for the development of new glottal gap segmentation algorithms from
High-Speed-Videoendoscopic sequences to improve or create new Facilitative
Playbacks. Despite these advances and others in the field, the broader
challenge of performing an accurate and completely automatic semantic
segmentation method of the glottal area remains open.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifei Chen, Tianshuo Xu, Wenhang Ge, Leyi Wu, Dongyu Yan, Jing He, Luozhou Wang, Lu Zeng, Shunsi Zhang, Yingcong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rendering and inverse rendering are pivotal tasks in both computer vision and
graphics. The rendering equation is the core of the two tasks, as an ideal
conditional distribution transfer function from intrinsic properties to RGB
images. Despite achieving promising results of existing rendering methods, they
merely approximate the ideal estimation for a specific scene and come with a
high computational cost. Additionally, the inverse conditional distribution
transfer is intractable due to the inherent ambiguity. To address these
challenges, we propose a data-driven method that jointly models rendering and
inverse rendering as two conditional generation tasks within a single diffusion
framework. Inspired by UniDiffuser, we utilize two distinct time schedules to
model both tasks, and with a tailored dual streaming module, we achieve
cross-conditioning of two pre-trained diffusion models. This unified approach,
named Uni-Renderer, allows the two processes to facilitate each other through a
cycle-consistent constrain, mitigating ambiguity by enforcing consistency
between intrinsic properties and rendered images. Combined with a meticulously
prepared dataset, our method effectively decomposition of intrinsic properties
and demonstrates a strong capability to recognize changes during rendering. We
will open-source our training and inference code to the public, fostering
further research and development in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Albert Ali Salah, Itir Onal Ertugrul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores image modeling from the frequency space and introduces
DCTdiff, an end-to-end diffusion generative paradigm that efficiently models
images in the discrete cosine transform (DCT) space. We investigate the design
space of DCTdiff and reveal the key design factors. Experiments on different
frameworks (UViT, DiT), generation tasks, and various diffusion samplers
demonstrate that DCTdiff outperforms pixel-based diffusion models regarding
generative quality and training efficiency. Remarkably, DCTdiff can seamlessly
scale up to high-resolution generation without using the latent diffusion
paradigm. Finally, we illustrate several intriguing properties of DCT image
modeling. For example, we provide a theoretical proof of why `image diffusion
can be seen as spectral autoregression', bridging the gap between diffusion and
autoregressive models. The effectiveness of DCTdiff and the introduced
properties suggest a promising direction for image modeling in the frequency
space. The code is at \url{https://github.com/forever208/DCTdiff}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and
  Semantic Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunità, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound designers and Foley artists usually sonorize a scene, such as from a
movie or video game, by manually annotating and sonorizing each action of
interest in the video. In our case, the intent is to leave full creative
control to sound designers with a tool that allows them to bypass the more
repetitive parts of their work, thus being able to focus on the creative
aspects of sound production. We achieve this presenting Stable-V2A, a two-stage
model consisting of: an RMS-Mapper that estimates an envelope representative of
the audio characteristics associated with the input video; and Stable-Foley, a
diffusion model based on Stable Audio Open that generates audio semantically
and temporally aligned with the target video. Temporal alignment is guaranteed
by the use of the envelope as a ControlNet input, while semantic alignment is
achieved through the use of sound representations chosen by the designer as
cross-attention conditioning of the diffusion process. We train and test our
model on Greatest Hits, a dataset commonly used to evaluate V2A models. In
addition, to test our model on a case study of interest, we introduce Walking
The Maps, a dataset of videos extracted from video games depicting animated
characters walking in different locations. Samples and code available on our
demo page at https://ispamm.github.io/Stable-V2A.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Federated Learning in the Face of Covariate Shift: A Magnitude
  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozgu Goksu, Nicolas Pugeault
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of highly sophisticated neural networks has allowed for fast
progress in every field of computer vision, however, applications where
annotated data is prohibited due to privacy or security concerns remain
challenging. Federated Learning (FL) offers a promising framework for
individuals aiming to collaboratively develop a shared model while preserving
data privacy. Nevertheless, our findings reveal that variations in data
distribution among clients can profoundly affect FL methodologies, primarily
due to instabilities in the aggregation process. We also propose a novel FL
framework to mitigate the adverse effects of covariate shifts among federated
clients by combining individual parameter pruning and regularization techniques
to improve the robustness of individual clients' models to aggregate. Each
client's model is optimized through magnitude-based pruning and the addition of
dropout and noise injection layers to build more resilient decision pathways in
the networks and improve the robustness of the model's parameter aggregation
step. The proposed framework is capable of extracting robust representations
even in the presence of very large covariate shifts among client data
distributions and in the federation of a small number of clients. Empirical
findings substantiate the effectiveness of our proposed methodology across
common benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.
Furthermore, we introduce the CelebA-Gender dataset, specifically designed to
evaluate performance on a more realistic domain. The proposed method is capable
of extracting robust representations even in the presence of both high and low
covariate shifts among client data distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stitch Contrast and Segment_Learning a Human Action Segmentation Model
  Using Trimmed Skeleton Videos <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Tian, Pierre Payeur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing skeleton-based human action classification models rely on
well-trimmed action-specific skeleton videos for both training and testing,
precluding their scalability to real-world applications where untrimmed videos
exhibiting concatenated actions are predominant. To overcome this limitation,
recently introduced skeleton action segmentation models involve un-trimmed
skeleton videos into end-to-end training. The model is optimized to provide
frame-wise predictions for any length of testing videos, simultaneously
realizing action localization and classification. Yet, achieving such an
improvement im-poses frame-wise annotated skeleton videos, which remains
time-consuming in practice. This paper features a novel framework for
skeleton-based action segmentation trained on short trimmed skeleton videos,
but that can run on longer un-trimmed videos. The approach is implemented in
three steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral
skeleton stitching scheme that treats trimmed skeleton videos as elementary
human motions that compose a semantic space and can be sampled to generate
multi-action stitched se-quences. Contrast learns contrastive representations
from stitched sequences with a novel discrimination pretext task that enables a
skeleton encoder to learn meaningful action-temporal contexts to improve action
segmentation. Finally, Segment relates the proposed method to action
segmentation by learning a segmentation layer while handling particular da-ta
availability. Experiments involve a trimmed source dataset and an untrimmed
target dataset in an adaptation formulation for real-world skeleton-based human
action segmentation to evaluate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse
  Articulated Objects with Rich Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhua Sun, Yuxuan Li, Jiude Wei, Longfei Xu, Nange Wang, Yining Zhang, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The acquisition of substantial volumes of 3D articulated object data is
expensive and time-consuming, and consequently the scarcity of 3D articulated
object data becomes an obstacle for deep learning methods to achieve remarkable
performance in various articulated object understanding tasks. Meanwhile,
pairing these object data with detailed annotations to enable training for
various tasks is also difficult and labor-intensive to achieve. In order to
expeditiously gather a significant number of 3D articulated objects with
comprehensive and detailed annotations for training, we propose Articulated
Object Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox
consists of i) descriptions of articulated objects by means of a generalized
structure program along with their analytic correspondence to the objects'
point cloud, ii) procedural rules about manipulations on the structure program
to synthesize large-scale and diverse new articulated objects, and iii)
mathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to
provide annotations to the synthesized object. Arti-PG has two appealing
properties for providing training data for articulated object understanding
tasks: i) objects are created with unlimited variations in shape through
program-oriented structure manipulation, ii) Arti-PG is widely applicable to
diverse tasks by easily providing comprehensive and detailed annotations.
Arti-PG now supports the procedural generation of 26 categories of articulate
objects and provides annotations across a wide range of both vision and
manipulation tasks, and we provide exhaustive experiments which fully
demonstrate its advantages. We will make Arti-PG toolbox publicly available for
the community to use.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PhotoHolmes: a Python library for forgery detection in digital images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julián O'Flaherty, Rodrigo Paganini, Juan Pablo Sotelo, Julieta Umpiérrez, Marina Gardella, Matías Tailanian, Pablo Musé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce PhotoHolmes, an open-source Python library
designed to easily run and benchmark forgery detection methods on digital
images. The library includes implementations of popular and state-of-the-art
methods, dataset integration tools, and evaluation metrics. Utilizing the
Benchmark tool in PhotoHolmes, users can effortlessly compare various methods.
This facilitates an accurate and reproducible comparison between their own
methods and those in the existing literature. Furthermore, PhotoHolmes includes
a command-line interface (CLI) to easily run the methods implemented in the
library on any suspicious image. As such, image forgery methods become more
accessible to the community. The library has been built with extensibility and
modularity in mind, which makes adding new methods, datasets and metrics to the
library a straightforward process. The source code is available at
https://github.com/photoholmes/photoholmes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movie2Story: A framework for understanding videos and telling stories in
  the form of novel text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Li, Zheyang Jia, Anyu Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal video-to-text models have made considerable progress, primarily in
generating brief descriptions of video content. However, there is still a
deficiency in generating rich long-form text descriptions that integrate both
video and audio. In this paper, we introduce a framework called M2S, designed
to generate novel-length text by combining audio, video, and character
recognition. M2S includes modules for video long-form text description and
comprehension, audio-based analysis of emotion, speech rate, and character
alignment, and visual-based character recognition alignment. By integrating
multimodal information using the large language model GPT4o, M2S stands out in
the field of multimodal text generation. We demonstrate the effectiveness and
accuracy of M2S through comparative experiments and human evaluation.
Additionally, the model framework has good scalability and significant
potential for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDOL: Instant Photorealistic 3D Human Creation from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating a high-fidelity, animatable 3D full-body avatar from a single image
is a challenging task due to the diverse appearance and poses of humans and the
limited availability of high-quality training data. To achieve fast and
high-quality human reconstruction, this work rethinks the task from the
perspectives of dataset, model, and representation. First, we introduce a
large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K
diverse, photorealistic sets of human images. Each set contains 24-view frames
in specific human poses, generated using a pose-controllable
image-to-multi-view model. Next, leveraging the diversity in views, poses, and
appearances within HuGe100K, we develop a scalable feed-forward transformer
model to predict a 3D human Gaussian representation in a uniform space from a
given human image. This model is trained to disentangle human pose, body shape,
clothing geometry, and texture. The estimated Gaussians can be animated without
post-processing. We conduct comprehensive experiments to validate the
effectiveness of the proposed dataset and method. Our model demonstrates the
ability to efficiently reconstruct photorealistic humans at 1K resolution from
a single input image using a single GPU instantly. Additionally, it seamlessly
supports various applications, as well as shape and texture editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures, includes main content, supplementary materials,
  and references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TDCNet: Transparent Objects Depth Completion with CNN-<span class="highlight-title">Transformer</span>
  Dual-Branch Parallel Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghui Fan, Chao Ye, Anping Deng, Xiaotian Wu, Mengyang Pan, Hang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sensing and manipulation of transparent objects present a critical
challenge in industrial and laboratory robotics. Conventional sensors face
challenges in obtaining the full depth of transparent objects due to the
refraction and reflection of light on their surfaces and their lack of visible
texture. Previous research has attempted to obtain complete depth maps of
transparent objects from RGB and damaged depth maps (collected by depth sensor)
using deep learning models. However, existing methods fail to fully utilize the
original depth map, resulting in limited accuracy for deep completion. To solve
this problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel
network for transparent object depth completion. The proposed framework
consists of two different branches: one extracts features from partial depth
maps, while the other processes RGB-D images. Experimental results demonstrate
that our model achieves state-of-the-art performance across multiple public
datasets. Our code and the pre-trained model are publicly available at
https://github.com/XianghuiFan/TDCNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dream to Manipulate: Compositional World Models Empowering Robot
  Imitation Learning with Imagination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Barcellona, Andrii Zadaianchuk, Davide Allegro, Samuele Papa, Stefano Ghidoni, Efstratios Gavves
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A world model provides an agent with a representation of its environment,
enabling it to predict the causal consequences of its actions. Current world
models typically cannot directly and explicitly imitate the actual environment
in front of a robot, often resulting in unrealistic behaviors and
hallucinations that make them unsuitable for real-world applications. In this
paper, we introduce a new paradigm for constructing world models that are
explicit representations of the real world and its dynamics. By integrating
cutting-edge advances in real-time photorealism with Gaussian Splatting and
physics simulators, we propose the first compositional manipulation world
model, which we call DreMa. DreMa replicates the observed world and its
dynamics, allowing it to imagine novel configurations of objects and predict
the future consequences of robot actions. We leverage this capability to
generate new data for imitation learning by applying equivariant
transformations to a small set of demonstrations. Our evaluations across
various settings demonstrate significant improvements in both accuracy and
robustness by incrementing actions and object distributions, reducing the data
needed to learn a policy and improving the generalization of the agents. As a
highlight, we show that a real Franka Emika Panda robot, powered by DreMa's
imagination, can successfully learn novel physical tasks from just a single
example per task variation (one-shot policy learning). Our project page and
source code can be found in https://leobarcellona.github.io/DreamToManipulate/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corn Ear Detection and Orientation Estimation Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Sprague, John Evans, Michael Mardikes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring growth behavior of maize plants such as the development of ears
can give key insights into the plant's health and development. Traditionally,
the measurement of the angle of ears is performed manually, which can be
time-consuming and prone to human error. To address these challenges, this
paper presents a computer vision-based system for detecting and tracking ears
of corn in an image sequence. The proposed system could accurately detect,
track, and predict the ear's orientation, which can be useful in monitoring
their growth behavior. This can significantly save time compared to manual
measurement and enables additional areas of ear orientation research and
potential increase in efficiencies for maize production. Using an object
detector with keypoint detection, the algorithm proposed could detect 90
percent of all ears. The cardinal estimation had a mean absolute error (MAE) of
18 degrees, compared to a mean 15 degree difference between two people
measuring by hand. These results demonstrate the feasibility of using computer
vision techniques for monitoring maize growth and can lead to further research
in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages;15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface
  Reconstruction <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zesong Yang, Ru Zhang, Jiale Shi, Zixiang Ai, Boming Zhao, Hujun Bao, Luwei Yang, Zhaopeng Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surface representation has demonstrated remarkable success in the
areas of novel view synthesis and 3D reconstruction. However, assessing the
geometric quality of 3D reconstructions in the absence of ground truth mesh
remains a significant challenge, due to its rendering-based optimization
process and entangled learning of appearance and geometry with photometric
losses. In this paper, we present a novel framework, i.e, GURecon, which
establishes a geometric uncertainty field for the neural surface based on
geometric consistency. Different from existing methods that rely on
rendering-based measurement, GURecon models a continuous 3D uncertainty field
for the reconstructed surface, and is learned by an online distillation
approach without introducing real geometric information for supervision.
Moreover, in order to mitigate the interference of illumination on geometric
consistency, a decoupled field is learned and exploited to finetune the
uncertainty field. Experiments on various datasets demonstrate the superiority
of GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play
extension to various neural surface representations and improvement on
downstream tasks such as incremental reconstruction. The code and supplementary
material are available on the project website:
https://zju3dv.github.io/GURecon/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025. Project page:
  https://zju3dv.github.io/gurecon/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Spectral Calibration of Hyperspectral Images:Method, <span class="highlight-title">Dataset</span>
  and Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Du, Shaodi You, Cheng Cheng, Shikui Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral image (HSI) densely samples the world in both the space and
frequency domain and therefore is more distinctive than RGB images. Usually,
HSI needs to be calibrated to minimize the impact of various illumination
conditions. The traditional way to calibrate HSI utilizes a physical reference,
which involves manual operations, occlusions, and/or limits camera mobility.
These limitations inspire this paper to automatically calibrate HSIs using a
learning-based method. Towards this goal, a large-scale HSI calibration dataset
is created, which has 765 high-quality HSI pairs covering diversified natural
scenes and illuminations. The dataset is further expanded to 7650 pairs by
combining with 10 different physically measured illuminations. A spectral
illumination transformer (SIT) together with an illumination attention module
is proposed. Extensive benchmarks demonstrate the SoTA performance of the
proposed SIT. The benchmarks also indicate that low-light conditions are more
challenging than normal conditions. The dataset and codes are available
online:https://github.com/duranze/Automatic-spectral-calibration-of-HSI
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MagicNaming: Consistent Identity Generation by Finding a "Name Space" in
  T2I Diffusion Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Hunag, Yuhua Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable
of generating famous persons by simply referring to their names. Is it possible
to make such models generate generic identities as simple as the famous ones,
e.g., just use a name? In this paper, we explore the existence of a "Name
Space", where any point in the space corresponds to a specific identity.
Fortunately, we find some clues in the feature space spanned by text embedding
of celebrities' names. Specifically, we first extract the embeddings of
celebrities' names in the Laion5B dataset with the text encoder of diffusion
models. Such embeddings are used as supervision to learn an encoder that can
predict the name (actually an embedding) of a given face image. We
experimentally find that such name embeddings work well in promising the
generated image with good identity consistency. Note that like the names of
celebrities, our predicted name embeddings are disentangled from the semantics
of text inputs, making the original generation capability of text-to-image
models well-preserved. Moreover, by simply plugging such name embeddings, all
variants (e.g., from Civitai) derived from the same base model (i.e., SDXL)
readily become identity-aware text-to-image models. Project homepage:
\url{https://magicfusion.github.io/MagicNaming/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Hypothetical Summary for Retrieval-based Multi-image Question
  Answering <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peize Li, Qingyi Si, Peng Fu, Zheng Lin, Yan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-based multi-image question answering (QA) task involves retrieving
multiple question-related images and synthesizing these images to generate an
answer. Conventional "retrieve-then-answer" pipelines often suffer from
cascading errors because the training objective of QA fails to optimize the
retrieval stage. To address this issue, we propose a novel method to
effectively introduce and reference retrieved information into the QA. Given
the image set to be retrieved, we employ a multimodal large language model
(visual perspective) and a large language model (textual perspective) to obtain
multimodal hypothetical summary in question-form and description-form. By
combining visual and textual perspectives, MHyS captures image content more
specifically and replaces real images in retrieval, which eliminates the
modality gap by transforming into text-to-text retrieval and helps improve
retrieval. To more advantageously introduce retrieval with QA, we employ
contrastive learning to align queries (questions) with MHyS. Moreover, we
propose a coarse-to-fine strategy for calculating both sentence-level and
word-level similarity scores, to further enhance retrieval and filter out
irrelevant details. Our approach achieves a 3.7% absolute improvement over
state-of-the-art methods on RETVQA and a 14.5% improvement over CLIP.
Comprehensive experiments and detailed ablation studies demonstrate the
superiority of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Artifact2Artifact: Self-incentive artifact removal for
  photoacoustic imaging without any data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Li, Qian Chen, Chulhong Kim, Seongwook Choi, Yibing Wang, Yu Zhang, Changhui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photoacoustic imaging (PAI) uniquely combines optical contrast with the
penetration depth of ultrasound, making it critical for clinical applications.
However, the quality of 3D PAI is often degraded due to reconstruction
artifacts caused by the sparse and angle-limited configuration of detector
arrays. Existing iterative or deep learning-based methods are either
time-consuming or require large training datasets, significantly limiting their
practical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a
zero-shot self-supervised artifact removal method based on a super-lightweight
network, which leverages the fact that reconstruction artifacts are sensitive
to irregularities caused by data loss. By introducing random perturbations to
the acquired PA data, it spontaneously generates subset data, which in turn
stimulates the network to learn the artifact patterns in the reconstruction
results, thus enabling zero-shot artifact removal. This approach requires
neither training data nor prior knowledge of the artifacts, and is capable of
artifact removal for 3D PAI. For maximum amplitude projection (MAP) images or
slice images in 3D PAI acquired with arbitrarily sparse or angle-limited
detector arrays, ZS-A2A employs a self-incentive strategy to complete artifact
removal and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in
both simulation study and $ in\ vivo $ animal experiments. Results demonstrate
that ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing
zero-shot methods, and for the $ in\ vivo $ rat liver, ZS-A2A improves CNR from
17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in
the following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-scale School Mapping using Weakly Supervised Deep Learning for
  Universal School Connectivity <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabelle Tingzon, Utku Can Ozturk, Ivan Dotu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving global school connectivity is critical for ensuring inclusive and
equitable quality education. To reliably estimate the cost of connecting
schools, governments and connectivity providers require complete and accurate
school location data - a resource that is often scarce in many low- and
middle-income countries. To address this challenge, we propose a
cost-effective, scalable approach to locating schools in high-resolution
satellite images using weakly supervised deep learning techniques. Our best
models, which combine vision transformers and convolutional neural networks,
achieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging
explainable AI techniques, our approach can approximate the precise
geographical coordinates of the school locations using only low-cost,
classification-level annotations. To demonstrate the scalability of our method,
we generate nationwide maps of school location predictions in African countries
and present a detailed analysis of our results, using Senegal as our case
study. Finally, we demonstrate the immediate usability of our work by
introducing an interactive web mapping tool to streamline human-in-the-loop
model validation efforts by government partners. This work successfully
showcases the real-world utility of deep learning and satellite images for
planning regional infrastructure and accelerating universal school
connectivity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI-25 Special Track on AI for Social Impact (AISI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional
  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature
  Screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood
within the skull, which occurs due to the rupture of blood vessels in or around
the brain. If this condition is not diagnosed in a timely manner and
appropriately treated, it can lead to serious complications such as decreased
consciousness, permanent neurological disabilities, or even death.The primary
aim of this study is to detect the occurrence or non-occurrence of ICH,
followed by determining the type of subdural hemorrhage (SDH). These tasks are
framed as two separate binary classification problems. By adding two layers to
the co-scale convolutional attention (CCA) classifier architecture, we
introduce a novel approach for ICH detection. In the first layer, after
extracting features from different slices of computed tomography (CT) scan
images, we combine these features and select the 50 components that capture the
highest variance in the data, considering them as informative features. We then
assess the discriminative power of these features using the bootstrap forest
algorithm, discarding those that lack sufficient discriminative ability between
different classes. This algorithm explicitly determines the contribution of
each feature to the final prediction, assisting us in developing an explainable
AI model. The features feed into a boosting neural network as a latent feature
space. In the second layer, we introduce a novel uncertainty-based fuzzy
integral operator to fuse information from different CT scan slices. This
operator, by accounting for the dependencies between consecutive slices,
significantly improves detection accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy
  with <span class="highlight-title">Pre-train</span>ing, Data Augmentation and Dual Flow UNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Litingyu Wang, Wenjun Liao, Shichuan Zhang, Guotai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Head and neck tumors and metastatic lymph nodes are crucial for treatment
planning and prognostic analysis. Accurate segmentation and quantitative
analysis of these structures require pixel-level annotation, making automated
segmentation techniques essential for the diagnosis and treatment of head and
neck cancer. In this study, we investigated the effects of multiple strategies
on the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)
images. For the segmentation of pre-RT images, we utilized: 1) a fully
supervised learning approach, and 2) the same approach enhanced with
pre-trained weights and the MixUp data augmentation technique. For mid-RT
images, we introduced a novel computational-friendly network architecture that
features separate encoders for mid-RT images and registered pre-RT images with
their labels. The mid-RT encoder branch integrates information from pre-RT
images and labels progressively during the forward propagation. We selected the
highest-performing model from each fold and used their predictions to create an
ensemble average for inference. In the final test, our models achieved a
segmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on
aggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at
https://github.com/WltyBY/HNTS-MRG2024_train_code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging
  Scenes with Subtly Distinguished Objects <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihang Cao, Huangxun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D scene understanding is an important task, and there has been a recent
surge of research interest in aligning 3D representations of point clouds with
text to empower embodied AI. However, due to the lack of comprehensive 3D
benchmarks, the capabilities of 3D models in real-world scenes, particularly
those that are challenging with subtly distinguished objects, remain
insufficiently investigated. To facilitate a more thorough evaluation of 3D
models' capabilities, we propose a scheme, ObjVariantEnsemble, to
systematically introduce more scenes with specified object classes, colors,
shapes, quantities, and spatial relationships to meet model evaluation needs.
More importantly, we intentionally construct scenes with similar objects to a
certain degree and design an LLM-VLM-cooperated annotator to capture key
distinctions as annotations. The resultant benchmark can better challenge 3D
models, reveal their shortcomings in understanding, and potentially aid in the
further development of 3D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Multimodal Reasoning via Active Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step multimodal reasoning tasks pose significant challenges for
multimodal large language models (MLLMs), and finding effective ways to enhance
their performance in such scenarios remains an unresolved issue. In this paper,
we propose AR-MCTS, a universal framework designed to progressively improve the
reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo
Tree Search (MCTS). Our approach begins with the development of a unified
retrieval module that retrieves key supporting insights for solving complex
reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in
automated multimodal reasoning verification, we employ the MCTS algorithm
combined with an active retrieval mechanism, which enables the automatic
generation of step-wise annotations. This strategy dynamically retrieves key
insights for each reasoning step, moving beyond traditional beam search
sampling to improve the diversity and reliability of the reasoning space.
Additionally, we introduce a process reward model that aligns progressively to
support the automatic verification of multimodal reasoning tasks. Experimental
results across three complex multimodal reasoning benchmarks confirm the
effectiveness of the AR-MCTS framework in enhancing the performance of various
multimodal models. Further analysis demonstrates that AR-MCTS can optimize
sampling diversity and accuracy, yielding reliable multimodal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Huang, Yujie Lin, Siyu Chen, Haiyang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action recognition using GCNs has achieved remarkable
performance, but recognizing ambiguous actions, such as "waving" and
"saluting", remains a significant challenge. Existing methods typically rely on
a serial combination of GCNs and TCNs, where spatial and temporal features are
extracted independently, leading to an unbalanced spatial-temporal information,
which hinders accurate action recognition. Moreover, existing methods for
ambiguous actions often overemphasize local details, resulting in the loss of
crucial global context, which further complicates the task of differentiating
ambiguous actions. To address these challenges, we propose a lightweight
plug-and-play module called Synchronized and Fine-grained Head (SF-Head),
inserted between GCN and TCN layers. SF-Head first conducts Synchronized
Spatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL),
ensuring a balanced interaction between the two types of features. It then
performs Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature
Consistency Loss (F-CL), which aligns the aggregated feature with their
original spatial-temporal feature. This aggregation step effectively combines
both global context and local details. Experimental results on NTU RGB+D 60,
NTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in
distinguishing ambiguous actions. Our code will be made available at
https://github.com/HaoHuang2003/SFHead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR
  Semantic Segmentation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoumeng Qiu, Xinrun Li, XiangYang Xue, Jian Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although multiview fusion has demonstrated potential in LiDAR segmentation,
its dependence on computationally intensive point-based interactions, arising
from the lack of fixed correspondences between views such as range view and
Bird's-Eye View (BEV), hinders its practical deployment. This paper challenges
the prevailing notion that multiview fusion is essential for achieving high
performance. We demonstrate that significant gains can be realized by directly
fusing Polar and Cartesian partitioning strategies within the BEV space. Our
proposed BEV-only segmentation model leverages the inherent fixed grid
correspondences between these partitioning schemes, enabling a fusion process
that is orders of magnitude faster (170$\times$ speedup) than conventional
point-based methods. Furthermore, our approach facilitates dense feature
fusion, preserving richer contextual information compared to sparse point-based
alternatives. To enhance scene understanding while maintaining inference
efficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive
evaluation on the SemanticKITTI and nuScenes datasets provides compelling
evidence that our method outperforms previous multiview fusion approaches in
terms of both performance and inference speed, highlighting the potential of
BEV-based fusion for LiDAR segmentation. Code is available at
\url{https://github.com/skyshoumeng/PC-BEV.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Level Embedding and Alignment Network with Consistency and
  Invariance Learning for Cross-View Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Chen, Zhao-Xu Yang, Hai-Jun Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-View Geo-Localization (CVGL) involves determining the localization of
drone images by retrieving the most similar GPS-tagged satellite images.
However, the imaging gaps between platforms are often significant and the
variations in viewpoints are substantial, which limits the ability of existing
methods to effectively associate cross-view features and extract consistent and
invariant characteristics. Moreover, existing methods often overlook the
problem of increased computational and storage requirements when improving
model performance. To handle these limitations, we propose a lightweight
enhanced alignment network, called the Multi-Level Embedding and Alignment
Network (MEAN). The MEAN network uses a progressive multi-level enhancement
strategy, global-to-local associations, and cross-domain alignment, enabling
feature communication across levels. This allows MEAN to effectively connect
features at different levels and learn robust cross-view consistent mappings
and modality-invariant features. Moreover, MEAN adopts a shallow backbone
network combined with a lightweight branch design, effectively reducing
parameter count and computational complexity. Experimental results on the
University-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter
count by 62.17% and computational complexity by 70.99% compared to
state-of-the-art models, while maintaining competitive or even superior
performance. The codes will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Tampered Text Detection via Multimodal Large Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenfan Qu, Jian Liu, Haoxing Chen, Baihan Yu, Jingjing Liu, Weiqiang Wang, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, tampered text detection has attracted increasing attention due to
its essential role in information security. Although existing methods can
detect the tampered text region, the interpretation of such detection remains
unclear, making the prediction unreliable. To address this black-box problem,
we propose to explain the basis of tampered text detection with natural
language via large multimodal models. To fill the data gap for this task, we
propose a large-scale, comprehensive dataset, ETTD, which contains both
pixel-level annotations indicating the tampered text region and natural
language annotations describing the anomaly of the tampered text. Multiple
methods are employed to improve the quality of the proposed data. For example,
a fused mask prompt is proposed to reduce confusion when querying GPT4o to
generate anomaly descriptions. By weighting the input image with the mask
annotation, the tampered region can be clearly indicated and the content in and
around the tampered region can also be preserved. We also propose prompting
GPT4o to recognize tampered texts and filtering out the responses with low OCR
accuracy, which can effectively improve annotation quality in an automatic
manner. To further improve explainable tampered text detection, we propose a
simple yet effective model called TTD, which benefits from improved
fine-grained perception by paying attention to the suspected region with
auxiliary reference grounding query. Extensive experiments on both the ETTD
dataset and the public dataset have verified the effectiveness of the proposed
methods. In-depth analysis is also provided to inspire further research. The
dataset and code will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first work for explainable tampered text detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video Prediction Policy: A Generalist Robot Policy with Predictive
  Visual Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yucheng Hu, Yanjiang Guo, Pengchao Wang, Xiaoyu Chen, Yen-Jen Wang, Jianke Zhang, Koushil Sreenath, Chaochao Lu, Jianyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in robotics have focused on developing generalist
policies capable of performing multiple tasks. Typically, these policies
utilize pre-trained vision encoders to capture crucial information from current
observations. However, previous vision encoders, which trained on two-image
contrastive learning or single-image reconstruction, can not perfectly capture
the sequential information essential for embodied tasks. Recently, video
diffusion models (VDMs) have demonstrated the capability to accurately predict
future image sequences, exhibiting a good understanding of physical dynamics.
Motivated by the strong visual prediction capabilities of VDMs, we hypothesize
that they inherently possess visual representations that reflect the evolution
of the physical world, which we term predictive visual representations.
Building on this hypothesis, we propose the Video Prediction Policy (VPP), a
generalist robotic policy conditioned on the predictive visual representations
from VDMs. To further enhance these representations, we incorporate diverse
human or robotic manipulation datasets, employing unified video-generation
training objectives. VPP consistently outperforms existing methods across two
simulated and two real-world benchmarks. Notably, it achieves a 28.1\% relative
improvement in the Calvin ABC-D benchmark compared to the previous
state-of-the-art and delivers a 28.8\% increase in success rates for complex
real-world dexterous manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally. Project Page at
  https://video-prediction-policy.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLOv11 Optimization for Efficient Resource Utilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Areeg Fagad Rasheed, M. Zarkoosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this research is to optimize the eleventh iteration of You
Only Look Once (YOLOv11) by developing size-specific modified versions of the
architecture. These modifications involve pruning unnecessary layers and
reconfiguring the main architecture of YOLOv11. Each proposed version is
tailored to detect objects of specific size ranges, from small to large. To
ensure proper model selection based on dataset characteristics, we introduced
an object classifier program. This program identifies the most suitable
modified version for a given dataset. The proposed models were evaluated on
various datasets and compared with the original YOLOv11 and YOLOv8 models. The
experimental results highlight significant improvements in computational
resource efficiency, with the proposed models maintaining the accuracy of the
original YOLOv11. In some cases, the modified versions outperformed the
original model regarding detection performance. Furthermore, the proposed
models demonstrated reduced model sizes and faster inference times. Models
weights and the object size classifier can be found in this repository
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 13 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal
  Keypoint <span class="highlight-title">Transformer</span>s for Pedestrian Fall Detection in Smart Cities <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14768v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14768v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Byeonghun Kim, Byeongjoon Noh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In smart cities, detecting pedestrian falls is a major challenge to ensure
the safety and quality of life of citizens. In this study, we propose a novel
fall detection system using FLAMe (Federated Learning with Attention
Mechanism), a federated learning (FL) based algorithm. FLAMe trains around
important keypoint information and only transmits the trained important weights
to the server, reducing communication costs and preserving data privacy.
Furthermore, the lightweight keypoint transformer model is integrated into the
FL framework to effectively learn spatio-temporal features. We validated the
experiment using 22,672 video samples from the "Fall Accident Risk Behavior
Video-Sensor Pair data" dataset from AI-Hub. As a result of the experiment, the
FLAMe-based system achieved an accuracy of 94.02% with about 190,000
transmission parameters, maintaining performance similar to that of existing
centralized learning while maximizing efficiency by reducing communication
costs by about 40% compared to the existing FL algorithm, FedAvg. Therefore,
the FLAMe algorithm has demonstrated that it provides robust performance in the
distributed environment of smart cities and is a practical and effective
solution for public safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, AAAI 2025 FLUID Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Li, Dan Guo, Guoliang Chen, Chunxiao Fan, Jingyuan Xu, Zhiliang Wu, Hehe Fan, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro-Action Recognition (MAR) has gained increasing attention due to its
crucial role as a form of non-verbal communication in social interactions, with
promising potential for applications in human communication and emotion
analysis. However, current approaches often overlook the inherent ambiguity in
micro-actions, which arises from the wide category range and subtle visual
differences between categories. This oversight hampers the accuracy of
micro-action recognition. In this paper, we propose a novel Prototypical
Calibrating Ambiguous Network (\textbf{PCAN}) to unleash and mitigate the
ambiguity of MAR. \textbf{Firstly}, we employ a hierarchical action-tree to
identify the ambiguous sample, categorizing them into distinct sets of
ambiguous samples of false negatives and false positives, considering both
body- and action-level categories. \textbf{Secondly}, we implement an ambiguous
contrastive refinement module to calibrate these ambiguous samples by
regulating the distance between ambiguous samples and their corresponding
prototypes. This calibration process aims to pull false negative
($\mathbb{FN}$) samples closer to their respective prototypes and push false
positive ($\mathbb{FP}$) samples apart from their affiliated prototypes. In
addition, we propose a new prototypical diversity amplification loss to
strengthen the model's capacity by amplifying the differences between different
prototypes. \textbf{Finally}, we propose a prototype-guided rectification to
rectify prediction by incorporating the representability of prototypes.
Extensive experiments conducted on the benchmark dataset demonstrate the
superior performance of our method compared to existing approaches. The code is
available at https://github.com/kunli-cs/PCAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EnergyMoGen: Compositional Human Motion Generation with Energy-Based
  Diffusion Model in Latent Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianrong Zhang, Hehe Fan, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models, particularly latent diffusion models, have demonstrated
remarkable success in text-driven human motion generation. However, it remains
challenging for latent diffusion models to effectively compose multiple
semantic concepts into a single, coherent motion sequence. To address this
issue, we propose EnergyMoGen, which includes two spectrums of Energy-Based
Models: (1) We interpret the diffusion model as a latent-aware energy-based
model that generates motions by composing a set of diffusion models in latent
space; (2) We introduce a semantic-aware energy model based on cross-attention,
which enables semantic composition and adaptive gradient descent for text
embeddings. To overcome the challenges of semantic inconsistency and motion
distortion across these two spectrums, we introduce Synergistic Energy Fusion.
This design allows the motion latent diffusion model to synthesize
high-quality, complex motions by combining multiple energy terms corresponding
to textual descriptions. Experiments show that our approach outperforms
existing state-of-the-art models on various motion generation tasks, including
text-to-motion generation, compositional motion generation, and multi-concept
motion generation. Additionally, we demonstrate that our method can be used to
extend motion datasets and improve the text-to-motion task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://jiro-zhang.github.io/EnergyMoGen/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event-assisted 12-stop HDR Imaging of Dynamic Scene 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Guo, Zixuan Chen, Ziran Zhang, Yutian Chen, Gangwei Xu, Tianfan Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High dynamic range (HDR) imaging is a crucial task in computational
photography, which captures details across diverse lighting conditions.
Traditional HDR fusion methods face limitations in dynamic scenes with extreme
exposure differences, as aligning low dynamic range (LDR) frames becomes
challenging due to motion and brightness variation. In this work, we propose a
novel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera
system with an event camera and an RGB camera. The event camera provides
temporally dense, high dynamic range signals that improve alignment between LDR
frames with large exposure differences, reducing ghosting artifacts caused by
motion. Also, a real-world finetuning strategy is proposed to increase the
generalization of alignment module on real-world events. Additionally, we
introduce a diffusion-based fusion module that incorporates image priors from
pre-trained diffusion models to address artifacts in high-contrast regions and
minimize errors from the alignment process. To support this work, we developed
the ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized
event signals, and validated our approach on both simulated and real-world
data. Extensive experiments demonstrate that our method achieves
state-of-the-art performance, successfully extending HDR imaging to 12 stops in
dynamic scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://openimaginglab.github.io/Event-Assisted-12stops-HDR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explicit Relational Reasoning Network for Scene Text Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Su, Zhineng Chen, Yongkun Du, Zhilong Ji, Kai Hu, Jinfeng Bai, Xieping Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected component (CC) is a proper text shape representation that aligns
with human reading intuition. However, CC-based text detection methods have
recently faced a developmental bottleneck that their time-consuming
post-processing is difficult to eliminate. To address this issue, we introduce
an explicit relational reasoning network (ERRNet) to elegantly model the
component relationships without post-processing. Concretely, we first represent
each text instance as multiple ordered text components, and then treat these
components as objects in sequential movement. In this way, scene text detection
can be innovatively viewed as a tracking problem. From this perspective, we
design an end-to-end tracking decoder to achieve a CC-based method dispensing
with post-processing entirely. Additionally, we observe that there is an
inconsistency between classification confidence and localization quality, so we
propose a Polygon Monte-Carlo method to quickly and accurately evaluate the
localization quality. Based on this, we introduce a position-supervised
classification loss to guide the task-aligned learning of ERRNet. Experiments
on challenging benchmarks demonstrate the effectiveness of our ERRNet. It
consistently achieves state-of-the-art accuracy while holding highly
competitive inference speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Light-Weight Framework for Open-Set Object Detection with Decoupled
  Feature Alignment in Joint Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghao He, Hu Su, Haiyong Yu, Cong Yang, Wei Sui, Cong Wang, Song Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-set object detection (OSOD) is highly desirable for robotic manipulation
in unstructured environments. However, existing OSOD methods often fail to meet
the requirements of robotic applications due to their high computational burden
and complex deployment. To address this issue, this paper proposes a
light-weight framework called Decoupled OSOD (DOSOD), which is a practical and
highly efficient solution to support real-time OSOD tasks in robotic systems.
Specifically, DOSOD builds upon the YOLO-World pipeline by integrating a
vision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)
adaptor is developed to transform text embeddings extracted by the VLM into a
joint space, within which the detector learns the region representations of
class-agnostic proposals. Cross-modality features are directly aligned in the
joint space, avoiding the complex feature interactions and thereby improving
computational efficiency. DOSOD operates like a traditional closed-set detector
during the testing phase, effectively bridging the gap between closed-set and
open-set detection. Compared to the baseline YOLO-World, the proposed DOSOD
significantly enhances real-time performance while maintaining comparable
accuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\%$, compared to
$26.2\%$ for YOLO-World-v1-S and $22.7\%$ for YOLO-World-v2-S, using similar
backbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is
$57.1\%$ higher than YOLO-World-v1-S and $29.6\%$ higher than YOLO-World-v2-S.
Meanwhile, we demonstrate that the DOSOD model facilitates the deployment of
edge devices. The codes and models are publicly available at
https://github.com/D-Robotics-AI-Lab/DOSOD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Few-Shot Neural Architecture Search by Counting the Number of
  Nonlinear Functions <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngmin Oh, Hyunju Lee, Bumsub Ham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural architecture search (NAS) enables finding the best-performing
architecture from a search space automatically. Most NAS methods exploit an
over-parameterized network (i.e., a supernet) containing all possible
architectures (i.e., subnets) in the search space. However, the subnets that
share the same set of parameters are likely to have different characteristics,
interfering with each other during training. To address this, few-shot NAS
methods have been proposed that divide the space into a few subspaces and
employ a separate supernet for each subspace to limit the extent of weight
sharing. They achieve state-of-the-art performance, but the computational cost
increases accordingly. We introduce in this paper a novel few-shot NAS method
that exploits the number of nonlinear functions to split the search space. To
be specific, our method divides the space such that each subspace consists of
subnets with the same number of nonlinear functions. Our splitting criterion is
efficient, since it does not require comparing gradients of a supernet to split
the space. In addition, we have found that dividing the space allows us to
reduce the channel dimensions required for each supernet, which enables
training multiple supernets in an efficient manner. We also introduce a
supernet-balanced sampling (SBS) technique, sampling several subnets at each
training step, to train different supernets evenly within a limited number of
training steps. Extensive experiments on standard NAS benchmarks demonstrate
the effectiveness of our approach. Our code is available at
https://cvlab.yonsei.ac.kr/projects/EFS-NAS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FiVL: A Framework for Improved Vision-Language Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Estelle Aflalo, Gabriela Ben Melech Stan, Tiep Le, Man Luo, Shachar Rosenman, Sayak Paul, Shao-Yen Tseng, Vasudev Lal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision Language Models (LVLMs) have achieved significant progress in
integrating visual and textual inputs for multimodal reasoning. However, a
recurring challenge is ensuring these models utilize visual information as
effectively as linguistic content when both modalities are necessary to
formulate an accurate answer. We hypothesize that hallucinations arise due to
the lack of effective visual grounding in current LVLMs. This issue extends to
vision-language benchmarks, where it is difficult to make the image
indispensable for accurate answer generation, particularly in vision
question-answering tasks. In this work, we introduce FiVL, a novel method for
constructing datasets designed to train LVLMs for enhanced visual grounding and
to evaluate their effectiveness in achieving it. These datasets can be utilized
for both training and assessing an LVLM's ability to use image content as
substantive evidence rather than relying solely on linguistic priors, providing
insights into the model's reliance on visual information. To demonstrate the
utility of our dataset, we introduce an innovative training task that
outperforms baselines alongside a validation method and application for
explainability. The code is available at https://github.com/IntelLabs/fivl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MUSTER: Longitudinal Deformable Registration by Composition of
  Consecutive Deformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edvard O. S. Grødem, Donatas Sederevičius, Esten H. Leonardsen, Bradley J. MacIntosh, Atle Bjørnerud, Till Schellhorn, Øystein Sørensen, Inge Amlien, Pablo F. Garrido, Anders M. Fjell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Longitudinal imaging allows for the study of structural changes over time.
One approach to detecting such changes is by non-linear image registration.
This study introduces Multi-Session Temporal Registration (MUSTER), a novel
method that facilitates longitudinal analysis of changes in extended series of
medical images. MUSTER improves upon conventional pairwise registration by
incorporating more than two imaging sessions to recover longitudinal
deformations. Longitudinal analysis at a voxel-level is challenging due to
effects of a changing image contrast as well as instrumental and environmental
sources of bias between sessions. We show that local normalized
cross-correlation as an image similarity metric leads to biased results and
propose a robust alternative. We test the performance of MUSTER on a synthetic
multi-site, multi-session neuroimaging dataset and show that, in various
scenarios, using MUSTER significantly enhances the estimated deformations
relative to pairwise registration. Additionally, we apply MUSTER on a sample of
older adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.
The results show that MUSTER can effectively identify patterns of
neuro-degeneration from T1-weighted images and that these changes correlate
with changes in cognition, matching the performance of state of the art
segmentation methods. By leveraging GPU acceleration, MUSTER efficiently
handles large datasets, making it feasible also in situations with limited
computational resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Uncertainty: A Deep Dive into Calibration and Performance of
  Multimodal Large Language Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) combine visual and textual data for
tasks such as image captioning and visual question answering. Proper
uncertainty calibration is crucial, yet challenging, for reliable use in areas
like healthcare and autonomous driving. This paper investigates representative
MLLMs, focusing on their calibration across various scenarios, including before
and after visual fine-tuning, as well as before and after multimodal training
of the base LLMs. We observed miscalibration in their performance, and at the
same time, no significant differences in calibration across these scenarios. We
also highlight how uncertainty differs between text and images and how their
integration affects overall uncertainty. To better understand MLLMs'
miscalibration and their ability to self-assess uncertainty, we construct the
IDK (I don't know) dataset, which is key to evaluating how they handle
unknowns. Our findings reveal that MLLMs tend to give answers rather than admit
uncertainty, but this self-assessment improves with proper prompt adjustments.
Finally, to calibrate MLLMs and enhance model reliability, we propose
techniques such as temperature scaling and iterative prompt optimization. Our
results provide insights into improving MLLMs for effective and responsible
deployment in multimodal applications. Code and IDK dataset:
\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RefHCM: A Unified Model for Referring Perceptions in Human-Centric
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Huang, Ruibing Hou, Jiahe Zhao, Hong Chang, Shiguang Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-centric perceptions play a crucial role in real-world applications.
While recent human-centric works have achieved impressive progress, these
efforts are often constrained to the visual domain and lack interaction with
human instructions, limiting their applicability in broader scenarios such as
chatbots and sports analysis. This paper introduces Referring Human
Perceptions, where a referring prompt specifies the person of interest in an
image. To tackle the new task, we propose RefHCM (Referring Human-Centric
Model), a unified framework to integrate a wide range of human-centric
referring tasks. Specifically, RefHCM employs sequence mergers to convert raw
multimodal data -- including images, text, coordinates, and parsing maps --
into semantic tokens. This standardized representation enables RefHCM to
reformulate diverse human-centric referring tasks into a sequence-to-sequence
paradigm, solved using a plain encoder-decoder transformer architecture.
Benefiting from a unified learning strategy, RefHCM effectively facilitates
knowledge transfer across tasks and exhibits unforeseen capabilities in
handling complex reasoning. This work represents the first attempt to address
referring human perceptions with a general-purpose framework, while
simultaneously establishing a corresponding benchmark that sets new standards
for the field. Extensive experiments showcase RefHCM's competitive and even
superior performance across multiple human-centric referring tasks. The code
and data are publicly at https://github.com/JJJYmmm/RefHCM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Prompt</span> Tuning: Vision Guided <span class="highlight-title">Prompt</span> Tuning with Cross-Attention
  for Fine-Grained Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot, fine-grained classification in computer vision poses significant
challenges due to the need to differentiate subtle class distinctions with
limited data. This paper presents a novel method that enhances the Contrastive
Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided
by real-time visual inputs. Unlike existing techniques such as Context
Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by
static prompts or visual token reliance, the proposed approach leverages a
cross-attention mechanism to dynamically refine text prompts for the image at
hand. This enables an image-specific alignment of textual features with image
patches extracted from the Vision Transformer, making the model more effective
for datasets with high intra-class variance and low inter-class differences.
The method is evaluated on several datasets, including CUBirds, Oxford Flowers,
and FGVC Aircraft, showing significant performance gains over static prompt
tuning approaches. To ensure these performance gains translate into trustworthy
predictions, we integrate Monte-Carlo Dropout in our approach to improve the
reliability of the model predictions and uncertainty estimates. This
integration provides valuable insights into the model's predictive confidence,
helping to identify when predictions can be trusted and when additional
verification is necessary. This dynamic approach offers a robust solution,
advancing the state-of-the-art for few-shot fine-grained classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit
  Post-Training Quantization in Vision <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Ding, Liang Yong, Sihuan Zhao, Jing Nie, Lihui Chen, Haijun Liu, Xichuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to its efficiency, Post-Training Quantization (PTQ) has been widely
adopted for compressing Vision Transformers (ViTs). However, when quantized
into low-bit representations, there is often a significant performance drop
compared to their full-precision counterparts. To address this issue,
reconstruction methods have been incorporated into the PTQ framework to improve
performance in low-bit quantization settings. Nevertheless, existing related
methods predefine the reconstruction granularity and seldom explore the
progressive relationships between different reconstruction granularities, which
leads to sub-optimal quantization results in ViTs. To this end, in this paper,
we propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for
accurate PTQ, which significantly improves the performance of low-bit quantized
vision transformers. Specifically, we define multi-head self-attention and
multi-layer perceptron modules along with their shortcuts as the finest
reconstruction units. After reconstructing these two fine-grained units, we
combine them to form coarser blocks and reconstruct them at a coarser
granularity level. We iteratively perform this combination and reconstruction
process, achieving progressive fine-to-coarse reconstruction. Additionally, we
introduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the
difficulty of training, thereby further enhancing model performance.
Experimental results on the ImageNet dataset demonstrate that our proposed
method achieves the best Top-1 accuracy among state-of-the-art methods,
particularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides,
quantization results on the COCO dataset reveal the effectiveness and
generalization of our proposed method on other computer vision tasks like
object detection and instance segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Review</span> of Fruit Tree Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Il-Seok Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fruit tree image segmentation is an essential problem in automating a variety
of agricultural tasks such as phenotyping, harvesting, spraying, and pruning.
Many research papers have proposed a diverse spectrum of solutions suitable to
specific tasks and environments. The review scope of this paper is confined to
the front views of fruit trees and based on 158 relevant papers collected using
a newly designed crawling review method. These papers are systematically
reviewed based on a taxonomy that sequentially considers the method, image,
task, and fruit. This taxonomy will assist readers to intuitively grasp the big
picture of these research activities. Our review reveals that the most
noticeable deficiency of the previous studies was the lack of a versatile
dataset and segmentation model that could be applied to a variety of tasks and
environments. Six important future research tasks are suggested, with the
expectation that these will pave the way to building a versatile tree
segmentation module.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Image Restoration and Enhancement: Degradation Calibrated Cycle
  Reconstruction Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minglong Xue, Jinhong He, Shivakumara Palaiahnakote, Mingliang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image restoration and enhancement are pivotal for numerous computer vision
applications, yet unifying these tasks efficiently remains a significant
challenge. Inspired by the iterative refinement capabilities of diffusion
models, we propose CycleRDM, a novel framework designed to unify restoration
and enhancement tasks while achieving high-quality mapping. Specifically,
CycleRDM first learns the mapping relationships among the degraded domain, the
rough normal domain, and the normal domain through a two-stage diffusion
inference process. Subsequently, we transfer the final calibration process to
the wavelet low-frequency domain using discrete wavelet transform, performing
fine-grained calibration from a frequency domain perspective by leveraging
task-specific frequency spaces. To improve restoration quality, we design a
feature gain module for the decomposed wavelet high-frequency domain to
eliminate redundant features. Additionally, we employ multimodal textual
prompts and Fourier transform to drive stable denoising and reduce randomness
during the inference process. After extensive validation, CycleRDM can be
effectively generalized to a wide range of image restoration and enhancement
tasks while requiring only a small number of training samples to be
significantly superior on various benchmarks of reconstruction quality and
perceptual quality. The source code will be available at
https://github.com/hejh8/CycleRDM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Li, You-wei Wen, Xu Xiao, Mingchao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Principal Component Analysis (RPCA) is a fundamental technique for
decomposing data into low-rank and sparse components, which plays a critical
role for applications such as image processing and anomaly detection.
Traditional RPCA methods commonly use $\ell_1$ norm regularization to enforce
sparsity, but this approach can introduce bias and result in suboptimal
estimates, particularly in the presence of significant noise or outliers.
Non-convex regularization methods have been proposed to mitigate these
challenges, but they tend to be complex to optimize and sensitive to initial
conditions, leading to potential instability in solutions. To overcome these
challenges, in this paper, we propose a novel RPCA model that integrates
adaptive weighted least squares (AWLS) and low-rank matrix factorization
(LRMF). The model employs a {self-attention-inspired} mechanism in its weight
update process, allowing the weight matrix to dynamically adjust and emphasize
significant components during each iteration. By employing a weighted F-norm
for the sparse component, our method effectively reduces bias while simplifying
the computational process compared to traditional $\ell_1$-norm-based methods.
We use an alternating minimization algorithm, where each subproblem has an
explicit solution, thereby improving computational efficiency. Despite its
simplicity, numerical experiments demonstrate that our method outperforms
existing non-convex regularization approaches, offering superior performance
and stability, as well as enhanced accuracy and robustness in practical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DM) have democratized AI image generation through an
iterative denoising process. Quantization is a major technique to alleviate the
inference cost and reduce the size of DM denoiser networks. However, as
denoisers evolve from variants of convolutional U-Nets toward newer Transformer
architectures, it is of growing importance to understand the quantization
sensitivity of different weight layers, operations and architecture types to
performance. In this work, we address this challenge with Qua$^2$SeDiMo, a
mixed-precision Post-Training Quantization framework that generates explainable
insights on the cost-effectiveness of various model weight quantization methods
for different denoiser operation types and block structures. We leverage these
insights to make high-quality mixed-precision quantization decisions for a
myriad of diffusion models ranging from foundational U-Nets to state-of-the-art
Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,
3.65-bit and 3.7-bit weight quantization on PixArt-${\alpha}$,
PixArt-${\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our
weight-quantization configurations with 6-bit activation quantization and
outperform existing approaches in terms of quantitative metrics and generative
image quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025; version includes supplementary material; 22 Pages, 18
  Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors
  Guided by Facial Recognizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younhun Kim, Myung-Joon Kwon, Wonjun Lee, Changick Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous Deepfake detection methods perform well within their training
domains, but their effectiveness diminishes significantly with new synthesis
techniques. Recent studies have revealed that detection models often create
decision boundaries based on facial identity rather than synthetic artifacts,
resulting in poor performance on cross-domain datasets. To address this
limitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a
novel training method that mitigates facial identity influence using a face
recognizer. Specifically, we first train a face recognizer using the same
backbone as the Deepfake detector. The recognizer is then frozen and employed
during the detector's training to reduce facial identity information. This is
achieved by feeding input images into both the recognizer and the detector, and
minimizing the similarity of their feature embeddings through our Facial
Identity Attenuating loss. This process encourages the detector to generate
embeddings distinct from the recognizer, effectively reducing the impact of
facial identity. Extensive experiments demonstrate that our approach
significantly enhances detection performance on both in-domain and cross-domain
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures. In 2024 IEEE International Conference on Visual
  Communications and Image Processing (VCIP) Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pitfalls of topology-aware image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Berger, Laurin Lux, Alexander Weers, Martin Menten, Daniel Rueckert, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological correctness, i.e., the preservation of structural integrity and
specific characteristics of shape, is a fundamental requirement for medical
imaging tasks, such as neuron or vessel segmentation. Despite the recent surge
in topology-aware methods addressing this challenge, their real-world
applicability is hindered by flawed benchmarking practices. In this paper, we
identify critical pitfalls in model evaluation that include inadequate
connectivity choices, overlooked topological artifacts in ground truth
annotations, and inappropriate use of evaluation metrics. Through detailed
empirical analysis, we uncover these issues' profound impact on the evaluation
and ranking of segmentation methods. Drawing from our findings, we propose a
set of actionable recommendations to establish fair and robust evaluation
standards for topology-aware medical image segmentation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/AlexanderHBerger/topo-pitfalls</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic
  Evaluation Using a Vision Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown impressive abilities in text and
image understanding. However, existing metrics for evaluating the text
generated by VLMs focus exclusively on overall quality, leading to two
limitations: 1) it is challenging to identify which aspects of the text need
improvement from the overall score; 2) metrics may overlook specific evaluation
criteria when predicting an overall score. To address these limitations, we
propose HarmonicEval, a reference-free evaluation metric that aggregates
criterion-wise scores to produce the overall score in a bottom-up manner.
Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)
dataset, which comprises 18,000 expert human judgments across four
vision-language tasks. Our experiments demonstrate that HarmonicEval achieves
higher correlations with human judgments than conventional metrics while
providing numerical scores for each criterion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Successive optimization of optics and post-processing with
  differentiable coherent PSF operator and field information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Ren, Jingwen Zhou, Wenguan Zhang, Jiapu Yan, Bingkun Chen, Huajun Feng, Shiqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the joint design of optical systems and downstream algorithms is
showing significant potential. However, existing rays-described methods are
limited to optimizing geometric degradation, making it difficult to fully
represent the optical characteristics of complex, miniaturized lenses
constrained by wavefront aberration or diffraction effects. In this work, we
introduce a precise optical simulation model, and every operation in pipeline
is differentiable. This model employs a novel initial value strategy to enhance
the reliability of intersection calculation on high aspherics. Moreover, it
utilizes a differential operator to reduce memory consumption during coherent
point spread function calculations. To efficiently address various degradation,
we design a joint optimization procedure that leverages field information.
Guided by a general restoration network, the proposed method not only enhances
the image quality, but also successively improves the optical performance
across multiple lenses that are already in professional level. This joint
optimization pipeline offers innovative insights into the practical design of
sophisticated optical systems and post-processing algorithms. The source code
will be made publicly available at
https://github.com/Zrr-ZJU/Successive-optimization
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Get Rid of Handcrafted Feature Extractors? SparseViT:
  Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization
  Through Spare-Coding <span class="highlight-title">Transformer</span> <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Su, Xiaochen Ma, Xuekang Zhu, Chaoqun Niu, Zeyu Lei, Ji-Zhe Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-semantic features or semantic-agnostic features, which are irrelevant to
image context but sensitive to image manipulations, are recognized as
evidential to Image Manipulation Localization (IML). Since manual labels are
impossible, existing works rely on handcrafted methods to extract non-semantic
features. Handcrafted non-semantic features jeopardize IML model's
generalization ability in unseen or complex scenarios. Therefore, for IML, the
elephant in the room is: How to adaptively extract non-semantic features?
Non-semantic features are context-irrelevant and manipulation-sensitive. That
is, within an image, they are consistent across patches unless manipulation
occurs. Then, spare and discrete interactions among image patches are
sufficient for extracting non-semantic features. However, image semantics vary
drastically on different patches, requiring dense and continuous interactions
among image patches for learning semantic representations. Hence, in this
paper, we propose a Sparse Vision Transformer (SparseViT), which reformulates
the dense, global self-attention in ViT into a sparse, discrete manner. Such
sparse self-attention breaks image semantics and forces SparseViT to adaptively
extract non-semantic features for images. Besides, compared with existing IML
models, the sparse self-attention mechanism largely reduced the model size (max
80% in FLOPs), achieving stunning parameter efficiency and computation
reduction. Extensive experiments demonstrate that, without any handcrafted
feature extractors, SparseViT is superior in both generalization and efficiency
across benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 page, 8 figures, published to AAAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDP: Generalizing to Multilingual Visual Information Extraction by
  Language Decoupled <span class="highlight-title">Pretrain</span>ing <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Information Extraction (VIE) plays a crucial role in the comprehension
of semi-structured documents, and several pre-trained models have been
developed to enhance performance. However, most of these works are monolingual
(usually English). Due to the extremely unbalanced quantity and quality of
pre-training corpora between English and other languages, few works can extend
to non-English scenarios. In this paper, we conduct systematic experiments to
show that vision and layout modality hold invariance among images with
different languages. If decoupling language bias from document images, a
vision-layout-based model can achieve impressive cross-lingual generalization.
Accordingly, we present a simple but effective multilingual training paradigm
LDP (Language Decoupled Pre-training) for better utilization of monolingual
pre-training data. Our proposed model LDM (Language Decoupled Model) is first
pre-trained on the language-independent data, where the language knowledge is
decoupled by a diffusion model, and then the LDM is fine-tuned on the
downstream languages. Extensive experiments show that the LDM outperformed all
SOTA multilingual pre-trained models, and also maintains competitiveness on
downstream monolingual/English benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,
  and Internal Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Li, Bozhong Zheng, Xiaohao Xu, Jinye Gan, Fading Lu, Xiang Li, Na Ni, Zheng Tian, Xiaonan Huang, Shenghua Gao, Yingna Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object anomaly detection is essential for industrial quality inspection, yet
traditional single-sensor methods face critical limitations. They fail to
capture the wide range of anomaly types, as single sensors are often
constrained to either external appearance, geometric structure, or internal
properties. To overcome these challenges, we introduce MulSen-AD, the first
high-resolution, multi-sensor anomaly detection dataset tailored for industrial
applications. MulSen-AD unifies data from RGB cameras, laser scanners, and
lock-in infrared thermography, effectively capturing external appearance,
geometric deformations, and internal defects. The dataset spans 15 industrial
products with diverse, real-world anomalies. We also present MulSen-AD Bench, a
benchmark designed to evaluate multi-sensor methods, and propose
MulSen-TripleAD, a decision-level fusion algorithm that integrates these three
modalities for robust, unsupervised object anomaly detection. Our experiments
demonstrate that multi-sensor fusion substantially outperforms single-sensor
approaches, achieving 96.1% AUROC in object-level detection accuracy. These
results highlight the importance of integrating multi-sensor data for
comprehensive industrial anomaly detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spike2Former: Efficient Spiking <span class="highlight-title">Transformer</span> for High-performance Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenxin Lei, Man Yao, Jiakui Hu, Xinhao Luo, Yanye Lu, Bo Xu, Guoqi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly
in image segmentation tasks. The reason is that directly converting neural
networks with complex architectural designs for segmentation tasks into spiking
versions leads to performance degradation and non-convergence. To address this
challenge, we first identify the modules in the architecture design that lead
to the severe reduction in spike firing, make targeted improvements, and
propose Spike2Former architecture. Second, we propose normalized integer
spiking neurons to solve the training stability problem of SNNs with complex
architectures. We set a new state-of-the-art for SNNs in various semantic
segmentation datasets, with a significant improvement of +12.7% mIoU and 5.0
efficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU
and 6.6 efficiency on CityScapes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted on Association for the Advancement of
  Artificial Intelligence 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video
  Captioning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minkuk Kim, Hyeon Bae Kim, Jinyoung Moon, Jinwoo Choi, Seong Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing demand for solutions to real-world video challenges,
interest in dense video captioning (DVC) has been on the rise. DVC involves the
automatic captioning and localization of untrimmed videos. Several studies
highlight the challenges of DVC and introduce improved methods utilizing prior
knowledge, such as pre-training and external memory. In this research, we
propose a model that leverages the prior knowledge of human-oriented
hierarchical compact memory inspired by human memory hierarchy and cognition.
To mimic human-like memory recall, we construct a hierarchical memory and a
hierarchical memory reading module. We build an efficient hierarchical compact
memory by employing clustering of memory events and summarization using large
language models. Comparative experiments demonstrate that this hierarchical
memory recall process improves the performance of DVC by achieving
state-of-the-art performance on YouCook2 and ViTT datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffSim: Taming Diffusion Models for Evaluating Visual Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiren Song, Xiaokang Liu, Mike Zheng Shou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have fundamentally transformed the field of generative
models, making the assessment of similarity between customized model outputs
and reference inputs critically important. However, traditional perceptual
similarity metrics operate primarily at the pixel and patch levels, comparing
low-level colors and textures but failing to capture mid-level similarities and
differences in image layout, object pose, and semantic content. Contrastive
learning-based CLIP and self-supervised learning-based DINO are often used to
measure semantic similarity, but they highly compress image features,
inadequately assessing appearance details. This paper is the first to discover
that pretrained diffusion models can be utilized for measuring visual
similarity and introduces the DiffSim method, addressing the limitations of
traditional metrics in capturing perceptual consistency in custom generation
tasks. By aligning features in the attention layers of the denoising U-Net,
DiffSim evaluates both appearance and style similarity, showing superior
alignment with human visual preferences. Additionally, we introduce the Sref
and IP benchmarks to evaluate visual similarity at the level of style and
instance, respectively. Comprehensive evaluations across multiple benchmarks
demonstrate that DiffSim achieves state-of-the-art performance, providing a
robust tool for measuring visual coherence in generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D
  Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianpu Sun, Changyong Shu, Sifan Zhou, Zichen Yu, Yan Chen, Dawei Yang, Yuan Chun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D occupancy perception is gaining increasing attention due to its capability
to offer detailed and precise environment representations. Previous
weakly-supervised NeRF methods balance efficiency and accuracy, with mIoU
varying by 5-10 points due to sampling count along camera rays. Recently,
real-time Gaussian splatting has gained widespread popularity in 3D
reconstruction, and the occupancy prediction task can also be viewed as a
reconstruction task. Consequently, we propose GSRender, which naturally employs
3D Gaussian Splatting for occupancy prediction, simplifying the sampling
process. In addition, the limitations of 2D supervision result in duplicate
predictions along the same camera ray. We implemented the Ray Compensation (RC)
module, which mitigates this issue by compensating for features from adjacent
frames. Finally, we redesigned the loss to eliminate the impact of dynamic
objects from adjacent frames. Extensive experiments demonstrate that our
approach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while
narrowing the gap with 3D supervision methods. Our code will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alignment-Free RGB-T Salient Object Detection: A Large-scale <span class="highlight-title">Dataset</span> and
  Progressive Correlation Network <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunpeng Wang, Keke Chen, Chenglong Li, Zhengzheng Tu, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment-free RGB-Thermal (RGB-T) salient object detection (SOD) aims to
achieve robust performance in complex scenes by directly leveraging the
complementary information from unaligned visible-thermal image pairs, without
requiring manual alignment. However, the labor-intensive process of collecting
and annotating image pairs limits the scale of existing benchmarks, hindering
the advancement of alignment-free RGB-T SOD. In this paper, we construct a
large-scale and high-diversity unaligned RGB-T SOD dataset named UVT20K,
comprising 20,000 image pairs, 407 scenes, and 1256 object categories. All
samples are collected from real-world scenarios with various challenges, such
as low illumination, image clutter, complex salient objects, and so on. To
support the exploration for further research, each sample in UVT20K is
annotated with a comprehensive set of ground truths, including saliency masks,
scribbles, boundaries, and challenge attributes. In addition, we propose a
Progressive Correlation Network (PCNet), which models inter- and intra-modal
correlations on the basis of explicit alignment to achieve accurate predictions
in unaligned image pairs. Extensive experiments conducted on unaligned and
aligned datasets demonstrate the effectiveness of our method.Code and dataset
are available at https://github.com/Angknpng/PCNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar
  Object Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Xu, Zhiyu Xiang, Chenwei Zhang, Hanzhi Zhong, Xijun Zhao, Ruina Dang, Peng Xu, Tianyu Pu, Eryun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection is one of the fundamental perception tasks for autonomous
vehicles. Fulfilling such a task with a 4D millimeter-wave radar is very
attractive since the sensor is able to acquire 3D point clouds similar to Lidar
while maintaining robust measurements under adverse weather. However, due to
the high sparsity and noise associated with the radar point clouds, the
performance of the existing methods is still much lower than expected. In this
paper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation
(SCKD) method for 4D radar-based 3D object detection. It characterizes the
capability of learning the feature from a Lidar-radar-fused teacher network
with semi-supervised distillation. We first propose an adaptive fusion module
in the teacher network to boost its performance. Then, two feature distillation
modules are designed to facilitate the cross-modality knowledge transfer.
Finally, a semi-supervised output distillation is proposed to increase the
effectiveness and flexibility of the distillation framework. With the same
network structure, our radar-only student trained by SCKD boosts the mAP by
10.38% over the baseline and outperforms the state-of-the-art works on the VoD
dataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the
moderate difficulty level over the baseline when extra unlabeled data are
available. Code is available at https://github.com/Ruoyu-Xu/SCKD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF
  Separation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14568v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14568v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongsung Kim, Minjun Park, Jooyoung Choi, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent learning-based Multi-View Stereo models have demonstrated
state-of-the-art performance in sparse-view 3D reconstruction. However,
directly applying 3D Gaussian Splatting (3DGS) as a refinement step following
these models presents challenges. We hypothesize that the excessive positional
degrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting
color patterns at the cost of structural fidelity. To address this, we propose
reprojection-based DoF separation, a method distinguishing positional DoFs in
terms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To
independently manage each DoF, we introduce a reprojection process along with
tailored constraints for each DoF. Through experiments across various datasets,
we confirm that separating the positional DoFs of Gaussians and applying
targeted constraints effectively suppresses geometric artifacts, producing
reconstruction results that are both visually and geometrically plausible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GBRIP: Granular Ball Representation for Imbalanced Partial Label
  Learning <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial label learning (PLL) is a complicated weakly supervised
multi-classification task compounded by class imbalance. Currently, existing
methods only rely on inter-class pseudo-labeling from inter-class features,
often overlooking the significant impact of the intra-class imbalanced features
combined with the inter-class. To address these limitations, we introduce
Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for
imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and
multi-center loss to construct a granular ball-based nfeature space through
unsupervised learning, effectively capturing the feature distribution within
each class. GBRIP mitigates the impact of confusing features by systematically
refining label disambiguation and estimating imbalance distributions. The novel
multi-center loss function enhances learning by emphasizing the relationships
between samples and their respective centers within the granular balls.
Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms
existing state-of-the-art methods, offering a robust solution to the challenges
of imbalanced PLL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14559v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14559v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, Ruimao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling law has been validated in various domains, such as natural
language processing (NLP) and massive computer vision tasks; however, its
application to motion generation remains largely unexplored. In this paper, we
introduce a scalable motion generation framework that includes the motion
tokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through
comprehensive experiments, we observe the scaling behavior of this system. For
the first time, we confirm the existence of scaling laws within the context of
motion generation. Specifically, our results demonstrate that the normalized
test loss of our prefix autoregressive models adheres to a logarithmic law in
relation to compute budgets. Furthermore, we also confirm the power law between
Non-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect
to compute budgets respectively. Leveraging the scaling law, we predict the
optimal transformer size, vocabulary size, and data requirements for a compute
budget of $1e18$. The test loss of the system, when trained with the optimal
model size, vocabulary size, and required data, aligns precisely with the
predicted test loss, thereby validating the scaling law.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bright-NeRF:Brightening Neural Radiance Field with Color Restoration
  from Low-light Raw Images <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Wang, Xin Huang, Guoqing Zhou, Qifeng Guo, Qing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRFs) have demonstrated prominent performance in
novel view synthesis. However, their input heavily relies on image acquisition
under normal light conditions, making it challenging to learn accurate scene
representation in low-light environments where images typically exhibit
significant noise and severe color distortion. To address these challenges, we
propose a novel approach, Bright-NeRF, which learns enhanced and high-quality
radiance fields from multi-view low-light raw images in an unsupervised manner.
Our method simultaneously achieves color restoration, denoising, and enhanced
novel view synthesis. Specifically, we leverage a physically-inspired model of
the sensor's response to illumination and introduce a chromatic adaptation loss
to constrain the learning of response, enabling consistent color perception of
objects regardless of lighting conditions. We further utilize the raw data's
properties to expose the scene's intensity automatically. Additionally, we have
collected a multi-view low-light raw image dataset to advance research in this
field. Experimental results demonstrate that our proposed method significantly
outperforms existing 2D and 3D approaches. Our code and dataset will be made
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ {S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gui Wang, Yuexiang Li, Wenting Chen, Meidan Ding, Wooi Ping Cheah, Rong Qu, Jianfeng Ren, Linlin Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small lesions play a critical role in early disease diagnosis and
intervention of severe infections. Popular models often face challenges in
segmenting small lesions, as it occupies only a minor portion of an image,
while down\_sampling operations may inevitably lose focus on local features of
small lesions. To tackle the challenges, we propose a {\bf S}mall-{\bf
S}ize-{\bf S}ensitive {\bf Mamba} ({\bf S$^3$-Mamba}), which promotes the
sensitivity to small lesions across three dimensions: channel, spatial, and
training strategy. Specifically, an Enhanced Visual State Space block is
designed to focus on small lesions through multiple residual connections to
preserve local features, and selectively amplify important details while
suppressing irrelevant ones through channel-wise attention. A Tensor-based
Cross-feature Multi-scale Attention is designed to integrate input image
features and intermediate-layer features with edge features and exploit the
attentive support of features across multiple scales, thereby retaining spatial
details of small lesions at various granularities. Finally, we introduce a
novel regularized curriculum learning to automatically assess lesion size and
sample difficulty, and gradually focus from easy samples to hard ones like
small lesions. Extensive experiments on three medical image segmentation
datasets show the superiority of our S$^3$-Mamba, especially in segmenting
small lesions. Our code is available at
https://github.com/ErinWang2023/S3-Mamba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Summary of Point <span class="highlight-title">Transformer</span> with Federated Learning for Predicting
  Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamorudeen A. Amuda, Almustapha A. Wakili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a federated learning-based approach to predict HER2
status from hematoxylin and eosin (HE)-stained whole slide images (WSIs),
reducing costs and speeding up treatment decisions. To address label imbalance
and feature representation challenges in multisite datasets, a point
transformer is proposed, incorporating dynamic label distribution, an auxiliary
classifier, and farthest cosine sampling. Extensive experiments demonstrate
state-of-the-art performance across four sites (2687 WSIs) and strong
generalization to two unseen sites (229 WSIs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracing the Roots: Leveraging Temporal Dynamics in Diffusion
  Trajectories for Origin Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Floros, Seyed-Mohsen Moosavi-Dezfooli, Pier Luigi Dragotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have revolutionized image synthesis, garnering significant
research interest in recent years. Diffusion is an iterative algorithm in which
samples are generated step-by-step, starting from pure noise. This process
introduces the notion of diffusion trajectories, i.e., paths from the standard
Gaussian distribution to the target image distribution. In this context, we
study discriminative algorithms operating on these trajectories. Specifically,
given a pre-trained diffusion model, we consider the problem of classifying
images as part of the training dataset, generated by the model or originating
from an external source. Our approach demonstrates the presence of patterns
across steps that can be leveraged for classification. We also conduct ablation
studies, which reveal that using higher-order gradient features to characterize
the trajectories leads to significant performance gains and more robust
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does VLM Classification Benefit from LLM Description Semantics? <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pingchuan Ma, Lennart Rietdorf, Dmytro Kotovenko, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately describing images with text is a foundation of explainable AI.
Vision-Language Models (VLMs) like CLIP have recently addressed this by
aligning images and texts in a shared embedding space, expressing semantic
similarities between vision and language embeddings. VLM classification can be
improved with descriptions generated by Large Language Models (LLMs). However,
it is difficult to determine the contribution of actual description semantics,
as the performance gain may also stem from a semantic-agnostic ensembling
effect, where multiple modified text prompts act as a noisy test-time
augmentation for the original one. We propose an alternative evaluation
scenario to decide if a performance boost of LLM-generated descriptions is
caused by such a noise augmentation effect or rather by genuine description
semantics. The proposed scenario avoids noisy test-time augmentation and
ensures that genuine, distinctive descriptions cause the performance boost.
Furthermore, we propose a training-free method for selecting discriminative
descriptions that work independently of classname-ensembling effects. Our
approach identifies descriptions that effectively differentiate classes within
a local CLIP label neighborhood, improving classification accuracy across seven
datasets. Additionally, we provide insights into the explainability of
description-based image classification with VLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI-25 (extended version), Code: https://github.com/CompVis/DisCLIP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DepthFM: Fast Monocular Depth Estimation with Flow Matching <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Gui, Johannes Schusterbauer, Ulrich Prestel, Pingchuan Ma, Dmytro Kotovenko, Olga Grebenkova, Stefan Andreas Baumann, Vincent Tao Hu, Björn Ommer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current discriminative depth estimation methods often produce blurry
artifacts, while generative approaches suffer from slow sampling due to
curvatures in the noise-to-depth transport. Our method addresses these
challenges by framing depth estimation as a direct transport between image and
depth distributions. We are the first to explore flow matching in this field,
and we demonstrate that its interpolation trajectories enhance both training
and sampling efficiency while preserving high performance. While generative
models typically require extensive training data, we mitigate this dependency
by integrating external knowledge from a pre-trained image diffusion model,
enabling effective transfer even across differing objectives. To further boost
our model performance, we employ synthetic data and utilize image-depth pairs
generated by a discriminative model on an in-the-wild image dataset. As a
generative model, our model can reliably estimate depth confidence, which
provides an additional advantage. Our approach achieves competitive zero-shot
performance on standard benchmarks of complex natural scenes while improving
sampling efficiency and only requiring minimal synthetic data for training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025, Project Page: https://github.com/CompVis/depth-fm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Compatible Training for Online Backfilling in Large-Scale
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backfilling is the process of re-extracting all gallery embeddings from
upgraded models in image retrieval systems. It inevitably requires a
prohibitively large amount of computational cost and even entails the downtime
of the service. Although backward-compatible learning sidesteps this challenge
by tackling query-side representations, this leads to suboptimal solutions in
principle because gallery embeddings cannot benefit from model upgrades. We
address this dilemma by introducing an online backfilling algorithm, which
enables us to achieve a progressive performance improvement during the
backfilling process while not sacrificing the final performance of new model
after the completion of backfilling. To this end, we first propose a simple
distance rank merge technique for online backfilling. Then, we incorporate a
reverse transformation module for more effective and efficient merging, which
is further enhanced by adopting a metric-compatible contrastive learning
approach. These two components help to make the distances of old and new models
compatible, resulting in desirable merge results during backfilling with no
extra computational overhead. Extensive experiments show the effectiveness of
our framework on four standard benchmarks in various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning-Based and Fully Automated Pipeline for Regurgitant
  Mitral Valve Anatomy Analysis from 3D Echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Munafò, Simone Saitta, Giacomo Ingallina, Paolo Denti, Francesco Maisano, Eustachio Agricola, Alberto Redaelli, Emiliano Votta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D transesophageal echocardiography (3DTEE), is the recommended method for
diagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of
the mitral valve (MV), allowing for precise segmentation and measurement of the
regurgitant valve anatomy. However, manual TEE segmentations are time-consuming
and prone to intra-operator variability, affecting the reliability of the
measurements. To address this, we developed a fully automated pipeline using a
3D convolutional neural network (CNN) to segment MV substructures (annulus,
anterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN,
based on a multi-decoder residual U-Net architecture, was trained and tested on
a dataset comprising 100 3DTEE images with corresponding segmentations. Within
the pipeline, a custom algorithm refines the CNN-based segmentations and
extracts MV models, from which anatomical landmarks and features are
quantified. The accuracy of the proposed method was assessed using Dice score
and mean surface distance (MSD) against ground truth segmentations, and the
extracted anatomical parameters were compared against a semiautomated
commercial software TomTec Image Arena. The trained 3D CNN achieved an average
Dice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the
annulus, anterior and posterior leaflet. The proposed CNN architecture
outperformed a baseline residual U-Net architecture in MV substructure
segmentation, and the refinement of the predicted annulus segmentation improved
MSD by 8.36%. The annular and leaflet linear measurements differed by less than
7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained
with TomTec Image Arena. The proposed pipeline was faster than the commercial
software, with a modeling time of 12.54 s and a quantification time of 54.42 s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Gradient Clipping for Noisy Label Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08941v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08941v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xichen Ye, Yifan Wu, Weizhong Zhang, Xiaoqiang Li, Yifan Chen, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has shown that constraining the gradient of loss function
with respect to model-predicted probabilities can enhance the model robustness
against noisy labels. These methods typically specify a fixed optimal threshold
for gradient clipping through validation data to obtain the desired robustness
against noise. However, this common practice overlooks the dynamic distribution
of gradients from both clean and noisy-labeled samples at different stages of
training, significantly limiting the model capability to adapt to the variable
nature of gradients throughout the training process. To address this issue, we
propose a simple yet effective approach called Optimized Gradient Clipping
(OGC), which dynamically adjusts the clipping threshold based on the ratio of
noise gradients to clean gradients after clipping, estimated by modeling the
distributions of clean and noisy samples. This approach allows us to modify the
clipping threshold at each training step, effectively controlling the influence
of noise gradients. Additionally, we provide statistical analysis to certify
the noise-tolerance ability of OGC. Our extensive experiments across various
types of label noise, including symmetric, asymmetric, instance-dependent, and
real-world noise, demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G-VEval: A Versatile Metric for Evaluating Image and Video Captions
  Using <span class="highlight-title">GPT</span>-4o 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation metric of visual captioning is important yet not thoroughly
explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss
semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are
limited in zero-shot scenarios. Advanced Language Model-based metrics also
struggle with aligning to nuanced human preferences. To address these issues,
we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new
GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and
supports three modes: reference-free, reference-only, and combined,
accommodating both video and image inputs. We also propose MSVD-Eval, a new
dataset for video captioning evaluation, to establish a more transparent and
consistent framework for both human experts and evaluation metrics. It is
designed to address the lack of clear criteria in existing datasets by
introducing distinct dimensions of Accuracy, Completeness, Conciseness, and
Relevance (ACCR). Extensive results show that G-VEval outperforms existing
methods in correlation with human annotations, as measured by Kendall tau-b and
Kendall tau-c. This provides a flexible solution for diverse captioning tasks
and suggests a straightforward yet effective approach for large language models
to understand video content, paving the way for advancements in automated
captioning. Codes are available at https://github.com/ztangaj/gveval
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait
  Image <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinkun Hao, Junshu Tang, Jiangning Zhang, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent works have achieved great success on image-to-3D object
generation, high quality and fidelity 3D head generation from a single image
remains a great challenge. Previous text-based methods for generating 3D heads
were limited by text descriptions and image-based methods struggled to produce
high-quality head geometry. To handle this challenging problem, we propose a
novel framework, ID-Sculpt, to generate high-quality 3D heads while preserving
their identities. Our work incorporates the identity information of the
portrait image into three parts: 1) geometry initialization, 2) geometry
sculpting, and 3) texture generation stages. Given a reference portrait image,
we first align the identity features with text features to realize ID-aware
guidance enhancement, which contains the control signals representing the face
information. We then use the canny map, ID features of the portrait image, and
a pre-trained text-to-normal/depth diffusion model to generate ID-aware
geometry supervision, and 3D-GAN inversion is employed to generate ID-aware
geometry initialization. Furthermore, with the ability to inject identity
information into 3D head generation, we use ID-aware guidance to calculate
ID-aware Score Distillation (ISD) for geometry sculpting. For texture
generation, we adopt the ID Consistent Texture Inpainting and Refinement which
progressively expands the view for texture inpainting to obtain an
initialization UV texture map. We then use the ID-aware guidance to provide
image-level supervision for noisy multi-view images to obtain a refined texture
map. Extensive experiments demonstrate that we can generate high-quality 3D
heads with accurate geometry and texture from a single in-the-wild portrait
image.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025; Project page:
  https://jinkun-hao.github.io/ID-Sculpt/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SageAttention2: Efficient Attention with Thorough Outlier Smoothing and
  Per-thread INT4 Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although quantization for linear layers has been widely used, its application
to accelerate the attention process remains limited. To further enhance the
efficiency of attention computation compared to SageAttention while maintaining
precision, we propose SageAttention2, which utilizes significantly faster 4-bit
matrix multiplication (Matmul) alongside additional precision-enhancing
techniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a
hardware-friendly thread-level granularity and quantize matrixes $(\widetilde
P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the
accuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$
to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS)
of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on
RTX4090, respectively. Comprehensive experiments confirm that our approach
incurs negligible end-to-end metrics loss across diverse models, including
those for large language processing, image generation, and video generation.
The codes are available at https://github.com/thu-ml/SageAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Jalonen, Mohammad Al-Sa'd, Roope Mellanen, Serkan Kiranyaz, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The health and safety hazards posed by worn crane lifting ropes mandate
periodic inspection for damage. This task is time-consuming, prone to human
error, halts operation, and may result in the premature disposal of ropes.
Therefore, we propose using efficient deep learning and computer vision methods
to automate the process of detecting damaged ropes. Specifically, we present a
vision-based system for detecting damage in synthetic fiber rope images using
lightweight convolutional neural networks. We develop a camera-based apparatus
to photograph the lifting rope's surface, while in operation, and capture the
progressive wear-and-tear as well as the more significant degradation in the
rope's health state. Experts from Konecranes annotate the collected images in
accordance with the rope's condition; normal or damaged. Then, we pre-process
the images, systematically design a deep learning model, evaluate its detection
and prediction performance, analyze its computational complexity, and compare
it with various other models. Experimental results show the proposed model
outperforms other similar techniques with 96.5% accuracy, 94.8% precision,
98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the
model's real-time operation, low memory footprint, robustness to various
environmental and operational conditions, and adequacy for deployment in
industrial applications such as lifting, mooring, towing, climbing, and
sailing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cycle Pixel Difference Network for Crisp Edge Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changsong Liu, Wei Zhang, Yanyan Liu, Mingyang Li, Wenlin Li, Yimeng Fan, Xiangnan Bai, Liang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge detection, as a fundamental task in computer vision, has garnered
increasing attention. The advent of deep learning has significantly advanced
this field. However, recent deep learning-based methods generally face two
significant issues: 1) reliance on large-scale pre-trained weights, and 2)
generation of thick edges. We construct a U-shape encoder-decoder model named
CPD-Net that successfully addresses these two issues simultaneously. In
response to issue 1), we propose a novel cycle pixel difference convolution
(CPDC), which effectively integrates edge prior knowledge with modern
convolution operations, consequently successfully eliminating the dependence on
large-scale pre-trained weights. As for issue 2), we construct a multi-scale
information enhancement module (MSEM) and a dual residual connection-based
(DRC) decoder to enhance the edge location ability of the model, thereby
generating crisp and clean contour maps. Comprehensive experiments conducted on
four standard benchmarks demonstrate that our method achieves competitive
performance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760
and AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our
approach provides a novel perspective for addressing these challenges in edge
detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth
  Estimation of Endoscopic Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16571v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16571v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Wang, Ying Zhou, Shiquan He, Ting Li, Fan Huang, Qiang Ding, Xinxia Feng, Mei Liu, Qiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Photometric constraint is indispensable for self-supervised monocular depth
estimation. It involves warping a source image onto a target view using
estimated depth&pose, and then minimizing the difference between the warped and
target images. However, the endoscopic built-in light causes significant
brightness fluctuations, and thus makes the photometric constraint unreliable.
Previous efforts only mitigate this relying on extra models to calibrate image
brightness. In this paper, we propose MonoPCC to address the brightness
inconsistency radically by reshaping the photometric constraint into a cycle
form. Instead of only warping the source image, MonoPCC constructs a closed
loop consisting of two opposite forward-backward warping paths: from target to
source and then back to target. Thus, the target image finally receives an
image cycle-warped from itself, which naturally makes the constraint invariant
to brightness changes. Moreover, MonoPCC transplants the source image's
phase-frequency into the intermediate warped image to avoid structure lost, and
also stabilizes the training via an exponential moving average (EMA) strategy
to avoid frequent changes in the forward warping. The comprehensive and
extensive experimental results on four endoscopic datasets demonstrate that our
proposed MonoPCC shows a great robustness to the brightness inconsistency, and
exceeds other state-of-the-arts by reducing the absolute relative error by at
least 7.27%, 9.38%, 9.90% and 3.17%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Union-over-Intersections: Object Detection beyond Winner-Takes-All 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Bhowmik, Pascal Mettes, Martin R. Oswald, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the problem of predicting box locations in object
detection architectures. Typically, each box proposal or box query aims to
directly maximize the intersection-over-union score with the ground truth,
followed by a winner-takes-all non-maximum suppression where only the highest
scoring box in each region is retained. We observe that both steps are
sub-optimal: the first involves regressing proposals to the entire ground
truth, which is a difficult task even with large receptive fields, and the
second neglects valuable information from boxes other than the top candidate.
Instead of regressing proposals to the whole ground truth, we propose a simpler
approach: regress only to the area of intersection between the proposal and the
ground truth. This avoids the need for proposals to extrapolate beyond their
visual scope, improving localization accuracy. Rather than adopting a
winner-takes-all strategy, we take the union over the regressed intersections
of all boxes in a region to generate the final box outputs. Our plug-and-play
method integrates seamlessly into proposal-based, grid-based, and query-based
detection architectures with minimal modifications, consistently improving
object localization and instance segmentation. We demonstrate its broad
applicability and versatility across various detection and segmentation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point Cloud Semantic Segmentation with Sparse and Inhomogeneous
  Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyi Pan, Nan Zhang, Wei Gao, Shan Liu, Ge Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing uniformly distributed sparse annotations, weakly supervised
learning alleviates the heavy reliance on fine-grained annotations in point
cloud semantic segmentation tasks. However, few works discuss the inhomogeneity
of sparse annotations, albeit it is common in real-world scenarios. Therefore,
this work introduces the probability density function into the gradient
sampling approximation method to qualitatively analyze the impact of annotation
sparsity and inhomogeneity under weakly supervised learning. Based on our
analysis, we propose an Adaptive Annotation Distribution Network (AADNet)
capable of robust learning on arbitrarily distributed sparse annotations.
Specifically, we propose a label-aware point cloud downsampling strategy to
increase the proportion of annotations involved in the training stage.
Furthermore, we design the multiplicative dynamic entropy as the gradient
calibration function to mitigate the gradient bias caused by non-uniformly
distributed sparse annotations and explicitly reduce the epistemic uncertainty.
Without any prior restrictions and additional information, our proposed method
achieves comprehensive performance improvements at multiple label rates and
different annotation distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accuracy Limits as a Barrier to Biometric System Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Durbet, Paul-Marie Grollemund, Pascal Lafourcade, Kevin Thiry-Atighehchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biometric systems are widely used for identity verification and
identification, including authentication (i.e., one-to-one matching to verify a
claimed identity) and identification (i.e., one-to-many matching to find a
subject in a database). The matching process relies on measuring similarities
or dissimilarities between a fresh biometric template and enrolled templates.
The False Match Rate FMR is a key metric for assessing the accuracy and
reliability of such systems. This paper analyzes biometric systems based on
their FMR, with two main contributions. First, we explore untargeted attacks,
where an adversary aims to impersonate any user within a database. We determine
the number of trials required for an attacker to successfully impersonate a
user and derive the critical population size (i.e., the maximum number of users
in the database) required to maintain a given level of security. Furthermore,
we compute the critical FMR value needed to ensure resistance against
untargeted attacks as the database size increases. Second, we revisit the
biometric birthday problem to evaluate the approximate and exact probabilities
that two users in a database collide (i.e., can impersonate each other). Based
on this analysis, we derive both the approximate critical population size and
the critical FMR value needed to bound the likelihood of such collisions
occurring with a given probability. These thresholds offer insights for
designing systems that mitigate the risk of impersonation and collisions,
particularly in large-scale biometric databases. Our findings indicate that
current biometric systems fail to deliver sufficient accuracy to achieve an
adequate security level against untargeted attacks, even in small-scale
databases. Moreover, state-of-the-art systems face significant challenges in
addressing the biometric birthday problem, especially as database sizes grow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for
  Efficient Multi-Frame Interpolation in DSA Images <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Xu, Huangxuan Zhao, Wenyu Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid and accurate direct multi-frame interpolation method for Digital
Subtraction Angiography (DSA) images is crucial for reducing radiation and
providing real-time assistance to physicians for precise diagnostics and
treatment. DSA images contain complex vascular structures and various motions.
Applying natural scene Video Frame Interpolation (VFI) methods results in
motion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA
has specifically addressed these issues for the first time and achieved SOTA
results. However, MoSt-DSA's focus on real-time performance leads to
insufficient suppression of high-frequency noise and incomplete filtering of
low-frequency noise in the generated images. To address these issues within the
same computational time scale, we propose GaraMoSt. Specifically, we optimize
the network pipeline with a parallel design and propose a module named MG-MSFE.
MG-MSFE extracts frame-relative motion and structural features at various
granularities in a fully convolutional parallel manner and supports
independent, flexible adjustment of context-aware granularity at different
scales, thus enhancing computational efficiency and accuracy. Extensive
experiments demonstrate that GaraMoSt achieves the SOTA performance in
accuracy, robustness, visual effects, and noise suppression, comprehensively
surpassing MoSt-DSA and other natural scene VFI methods. The code and models
are available at https://github.com/ZyoungXu/GaraMoSt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large video-language models (LVLMs) struggle to comprehend long
videos correctly due to limited context. To address this problem, fine-tuning
long-context LVLMs and employing GPT-based agents have emerged as promising
solutions. However, fine-tuning LVLMs would require extensive high-quality data
and substantial GPU resources, while GPT-based agents would rely on proprietary
models (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented
Generation (Video-RAG), a training-free and cost-effective pipeline that
employs visually-aligned auxiliary texts to help facilitate cross-modality
alignment while providing additional information beyond the visual content.
Specifically, we leverage open-source external tools to extract
visually-aligned information from pure video data (e.g., audio, optical
character, and object detection), and incorporate the extracted information
into an existing LVLM as auxiliary texts, alongside video frames and queries,
in a plug-and-play manner. Our Video-RAG offers several key advantages: (i)
lightweight with low computing overhead due to single-turn retrieval; (ii) easy
implementation and compatibility with any LVLM; and (iii) significant,
consistent performance gains across long video understanding benchmarks,
including Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates
superior performance over proprietary models like Gemini-1.5-Pro and GPT-4o
when utilized with a 72B model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Adversarial Networks for Image Super-Resolution: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13620v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13620v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunwei Tian, Xuanyu Zhang, Qi Zhu, Bob Zhang, Jerry Chun-Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single image super-resolution (SISR) has played an important role in the
field of image processing. Recent generative adversarial networks (GANs) can
achieve excellent results on low-resolution images with small samples. However,
there are little literatures summarizing different GANs in SISR. In this paper,
we conduct a comparative study of GANs from different perspectives. We first
take a look at developments of GANs. Second, we present popular architectures
for GANs in big and small samples for image applications. Then, we analyze
motivations, implementations and differences of GANs based optimization methods
and discriminative learning for image super-resolution in terms of supervised,
semi-supervised and unsupervised manners, where these GANs are analyzed via
integrating different network architectures, prior knowledge, loss functions
and multiple tasks. Next, we compare performance of these popular GANs on
public datasets via quantitative and qualitative analysis in SISR. Finally, we
highlight challenges of GANs and potential research points for SISR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Scalability of Self-Training for Open-Vocabulary Temporal
  Action Localization <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongseok Hyun, Su Ho Han, Hyolim Kang, Joon-Young Lee, Seon Joo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vocabulary size in temporal action localization (TAL) is limited by the
scarcity of large-scale annotated datasets. To overcome this, recent works
integrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL
(OV-TAL). However, despite the success of VLMs trained on extensive datasets,
existing OV-TAL methods still rely on human-labeled TAL datasets of limited
size to train action localizers, limiting their generalizability. In this
paper, we explore the scalability of self-training with unlabeled YouTube
videos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic
action localizer is trained on a human-labeled TAL dataset to generate
pseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled
dataset is then used to train the localizer. Extensive experiments demonstrate
that leveraging web-scale videos in self-training significantly enhances the
generalizability of an action localizer. Additionally, we identify limitations
in existing OV-TAL evaluation schemes and propose a new benchmark for thorough
assessment. Finally, we showcase the TAL performance of the large multimodal
model Gemini-1.5 on our new benchmark. Code is released at
https://github.com/HYUNJS/STOV-TAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VHM: Versatile and Honest Vision Language Model for Remote Sensing Image
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20213v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20213v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun, Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a Versatile and Honest vision language Model (VHM) for
remote sensing image analysis. VHM is built on a large-scale remote sensing
image-text dataset with rich-content captions (VersaD), and an honest
instruction dataset comprising both factual and deceptive questions (HnstD).
Unlike prevailing remote sensing image-text datasets, in which image captions
focus on a few prominent objects and their relationships, VersaD captions
provide detailed information about image properties, object attributes, and the
overall scene. This comprehensive captioning enables VHM to thoroughly
understand remote sensing images and perform diverse remote sensing tasks.
Moreover, different from existing remote sensing instruction datasets that only
include factual questions, HnstD contains additional deceptive questions
stemming from the non-existence of objects. This feature prevents VHM from
producing affirmative answers to nonsense queries, thereby ensuring its
honesty. In our experiments, VHM significantly outperforms various vision
language models on common tasks of scene classification, visual question
answering, and visual grounding. Additionally, VHM achieves competent
performance on several unexplored tasks, such as building vectorizing,
multi-label classification and honest question answering. We will release the
code, data and model weights at https://github.com/opendatalab/VHM .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding
  author: Gui-Song Xia, Conghui He</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZAHA: Introducing the Level of Facade Generalization and the Large-Scale
  Point Cloud Facade Semantic Segmentation Benchmark <span class="highlight-title">Dataset</span> <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04865v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04865v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olaf Wysocki, Yue Tan, Thomas Froech, Yan Xia, Magdalena Wysocki, Ludwig Hoegner, Daniel Cremers, Christoph Holst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Facade semantic segmentation is a long-standing challenge in photogrammetry
and computer vision. Although the last decades have witnessed the influx of
facade segmentation methods, there is a lack of comprehensive facade classes
and data covering the architectural variability. In ZAHA, we introduce Level of
Facade Generalization (LoFG), novel hierarchical facade classes designed based
on international urban modeling standards, ensuring compatibility with
real-world challenging classes and uniform methods' comparison. Realizing the
LoFG, we present to date the largest semantic 3D facade segmentation dataset,
providing 601 million annotated points at five and 15 classes of LoFG2 and
LoFG3, respectively. Moreover, we analyze the performance of baseline semantic
segmentation methods on our introduced LoFG classes and data, complementing it
with a discussion on the unresolved challenges for facade segmentation. We
firmly believe that ZAHA shall facilitate further development of 3D facade
semantic segmentation methods, enabling robust segmentation indispensable in
creating urban digital twins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV))</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye
  View Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13913v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13913v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fu Wang, Yanghao Zhang, Xiangyu Yin, Guangliang Cheng, Zeyu Fu, Xiaowei Huang, Wenjie Ruan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera-based Bird's Eye View (BEV) perception models receive increasing
attention for their crucial role in autonomous driving, a domain where concerns
about the robustness and reliability of deep learning have been raised. While
only a few works have investigated the effects of randomly generated semantic
perturbations, aka natural corruptions, on the multi-view BEV detection task,
we develop a black-box robustness evaluation framework that adversarially
optimises three common semantic perturbations: geometric transformation, colour
shifting, and motion blur, to deceive BEV models, serving as the first approach
in this emerging field. To address the challenge posed by optimising the
semantic perturbation, we design a smoothed, distance-based surrogate function
to replace the mAP metric and introduce SimpleDIRECT, a deterministic
optimisation algorithm that utilises observed slopes to guide the optimisation
process. By comparing with randomised perturbation and two optimisation
baselines, we demonstrate the effectiveness of the proposed framework.
Additionally, we provide a benchmark on the semantic robustness of ten recent
BEV models. The results reveal that PolarFormer, which emphasises geometric
information from multi-view images, exhibits the highest robustness, whereas
BEVDet is fully compromised, with its precision reduced to zero.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASTM :Autonomous Smart Traffic Management System Using Artificial
  Intelligence CNN and LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10929v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10929v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christofel Rio Goenawan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the modern world, the development of Artificial Intelligence (AI) has
contributed to improvements in various areas, including automation, computer
vision, fraud detection, and more. AI can be leveraged to enhance the
efficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce
traffic congestion rates. This paper presents an Autonomous Smart Traffic
Management (STM) system that uses AI to improve traffic flow rates. The system
employs the YOLO V5 Convolutional Neural Network to detect vehicles in traffic
management images. Additionally, it predicts the number of vehicles for the
next 12 hours using a Recurrent Neural Network with Long Short-Term Memory
(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the
traffic cycle length based on these vehicle predictions, aided by AI. From the
results of the RNN-LSTM model for predicting vehicle numbers over the next 12
hours, we observe that the model predicts traffic with a Mean Squared Error
(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.
After simulating the STM system in the CARLA simulation environment, we found
that the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per
minute) is 50\% higher than the rate without STM (around 15 vehicles per
minute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5
seconds per vehicle) is 70\% lower than without STM (around 12 seconds per
vehicle). These results demonstrate that the STM system using AI can increase
traffic flow by 50\% and reduce vehicle pass delays by 70\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In process to IEEE Intelligent Vehicle Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCB-<span class="highlight-title">dataset</span>: A <span class="highlight-title">Dataset</span> for Detecting Student Classroom Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02488v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02488v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of deep learning methods for automatic detection of students'
classroom behavior is a promising approach to analyze their class performance
and enhance teaching effectiveness. However, the lack of publicly available
datasets on student behavior poses a challenge for researchers in this field.
To address this issue, we propose a Student Classroom Behavior dataset
(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248
labels and 4,003 images, with a focus on hand-raising behavior. We evaluated
the dataset using the YOLOv7 algorithm, achieving a mean average precision
(map) of up to 85.3%. We believe that our dataset can serve as a robust
foundation for future research in the field of student behavior detection and
promote further advancements in this area.Our SCB-dataset can be downloaded
from: https://github.com/Whiffe/SCB-dataset
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D Registration in 30 Years: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Yang, Chu'ai Zhang, Zhengbao Wang, Xinyue Cao, Xuan Ouyang, Xiyu Zhang, Zhenxuan Zeng, Zhao Zeng, Borui Lu, Zhiyi Xia, Qian Zhang, Yulan Guo, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D point cloud registration is a fundamental problem in computer vision,
computer graphics, robotics, remote sensing, and etc. Over the last thirty
years, we have witnessed the amazing advancement in this area with numerous
kinds of solutions. Although a handful of relevant surveys have been conducted,
their coverage is still limited. In this work, we present a comprehensive
survey on 3D point cloud registration, covering a set of sub-areas such as
pairwise coarse registration, pairwise fine registration, multi-view
registration, cross-scale registration, and multi-instance registration. The
datasets, evaluation metrics, method taxonomy, discussions of the merits and
demerits, insightful thoughts of future directions are comprehensively
presented in this survey. The regularly updated project page of the survey is
available at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Diffusion <span class="highlight-title">Transformer</span>s with Token-wise Feature Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion transformers have shown significant effectiveness in both image and
video synthesis at the expense of huge computation costs. To address this
problem, feature caching methods have been introduced to accelerate diffusion
transformers by caching the features in previous timesteps and reusing them in
the following timesteps. However, previous caching methods ignore that
different tokens exhibit different sensitivities to feature caching, and
feature caching on some tokens may lead to 10$\times$ more destruction to the
overall generation quality compared with other tokens. In this paper, we
introduce token-wise feature caching, allowing us to adaptively select the most
suitable tokens for caching, and further enable us to apply different caching
ratios to neural layers in different types and depths. Extensive experiments on
PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image
and video generation with no requirements for training. For instance,
2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and
PixArt-$\alpha$ with almost no drop in generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In this version, we achieved a nearly lossless acceleration of 1.51
  times for ToCa on FLUX in the appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Jiaxin Li, Liming Tan, Yejie Guo, Junxuan Liang, Cewu Lu, Yong-Lu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent robots need to interact with diverse objects across various
environments. The appearance and state of objects frequently undergo complex
transformations depending on the object properties, e.g., phase transitions.
However, in the vision community, segmenting dynamic objects with phase
transitions is overlooked. In light of this, we introduce the concept of phase
in segmentation, which categorizes real-world objects based on their visual
characteristics and potential morphological and appearance changes. Then, we
present a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video
Object Segmentation (M$^3$-VOS), to verify the ability of models to understand
object phases, which consists of 479 high-resolution videos spanning over 10
distinct everyday scenarios. It provides dense instance mask annotations that
capture both object phases and their transitions. We evaluate state-of-the-art
methods on M$^3$-VOS, yielding several key insights. Notably, current
appearancebased approaches show significant room for improvement when handling
objects with phase transitions. The inherent changes in disorder suggest that
the predictive performance of the forward entropy-increasing process can be
improved through a reverse entropy-reducing process. These findings lead us to
propose ReVOS, a new plug-andplay model that improves its performance by
reversal refinement. Our data and code will be publicly available at
https://zixuan-chen.github.io/M-cubeVOS.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yanchao Yang, Qingnan Fan, Baoquan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM
system for real-time and high-quality dense 3D reconstruction. SLAM3R provides
an end-to-end solution by seamlessly integrating local 3D reconstruction and
global coordinate registration through feed-forward neural networks. Given an
input video, the system first converts it into overlapping clips using a
sliding window mechanism. Unlike traditional pose optimization-based methods,
SLAM3R directly regresses 3D pointmaps from RGB images in each window and
progressively aligns and deforms these local pointmaps to create a globally
consistent scene reconstruction - all without explicitly solving any camera
parameters. Experiments across datasets consistently show that SLAM3R achieves
state-of-the-art reconstruction accuracy and completeness while maintaining
real-time performance at 20+ FPS. Code and weights at:
https://github.com/PKU-VCL-3DV/SLAM3R.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepClean: Integrated Distortion Identification and Algorithm Selection
  for Rectifying Image Corruptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16302v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16302v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kapoor, Harshad Khadilkar, Jayvardhana Gubbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distortion identification and rectification in images and videos is vital for
achieving good performance in downstream vision applications. Instead of
relying on fixed trial-and-error based image processing pipelines, we propose a
two-level sequential planning approach for automated image distortion
classification and rectification. At the higher level it detects the class of
corruptions present in the input image, if any. The lower level selects a
specific algorithm to be applied, from a set of externally provided candidate
algorithms. The entire two-level setup runs in the form of a single forward
pass during inference and it is to be queried iteratively until the retrieval
of the original image. We demonstrate improvements compared to three baselines
on the object detection task on COCO image dataset with rich set of
distortions. The advantage of our approach is its dynamic reconfiguration,
conditioned on the input image and generalisability to unseen candidate
algorithms at inference time, since it relies only on the comparison of their
output of the image embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FashionComposer: Compositional Fashion Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihui Ji, Yiyang Wang, Xi Chen, Xiaogang Xu, Hao Luo, Hengshuang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FashionComposer for compositional fashion image generation. Unlike
previous methods, FashionComposer is highly flexible. It takes multi-modal
input (i.e., text prompt, parametric human model, garment image, and face
image) and supports personalizing the appearance, pose, and figure of the human
and assigning multiple garments in one pass. To achieve this, we first develop
a universal framework capable of handling diverse input modalities. We
construct scaled training data to enhance the model's robust compositional
capabilities. To accommodate multiple reference images (garments and faces)
seamlessly, we organize these references in a single image as an "asset
library" and employ a reference UNet to extract appearance features. To inject
the appearance features into the correct pixels in the generated result, we
propose subject-binding attention. It binds the appearance features from
different "assets" with the corresponding text features. In this way, the model
could understand each asset according to their semantics, supporting arbitrary
numbers and types of reference images. As a comprehensive solution,
FashionComposer also supports many other applications like human album
generation, diverse virtual try-on tasks, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://sihuiji.github.io/FashionComposer-Page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and
  BEV Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01812v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01812v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan Ye, Jun He, Weijia Li, Zhutao Lv, Yi Lin, Jinhua Yu, Haote Yang, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ground-to-aerial image synthesis focuses on generating realistic aerial
images from corresponding ground street view images while maintaining
consistent content layout, simulating a top-down view. The significant
viewpoint difference leads to domain gaps between views, and dense urban scenes
limit the visible range of street views, making this cross-view generation task
particularly challenging. In this paper, we introduce SkyDiffusion, a novel
cross-view generation method for synthesizing aerial images from street view
images, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The
Curved-BEV method in SkyDiffusion converts street-view images into a BEV
perspective, effectively bridging the domain gap, and employs a "multi-to-one"
mapping strategy to address occlusion issues in dense urban scenes. Next,
SkyDiffusion designed a BEV-guided diffusion model to generate
content-consistent and realistic aerial images. Additionally, we introduce a
novel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image
synthesis applications, including disaster scene aerial synthesis, historical
high-resolution satellite image synthesis, and low-altitude UAV image synthesis
tasks. Experimental results demonstrate that SkyDiffusion outperforms
state-of-the-art methods on cross-view datasets across natural (CVUSA),
suburban (CVACT), urban (VIGOR-Chicago), and various application scenarios
(G2A-3), achieving realistic and content-consistent aerial image generation.
More result and dataset information can be found at
https://opendatalab.github.io/skydiffusion/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Training-Free to Adaptive: Empirical Insights into MLLMs'
  Understanding of Detection Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17981v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17981v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive capabilities of Multimodal Large Language Models
(MLLMs) in integrating text and image modalities, challenges remain in
accurately interpreting detailed visual elements. Vision detection models excel
at recognizing fine-grained image details, prompting researchers to use them to
enhance MLLMs. One effective strategy is to infuse detection information in
text format, which has proven simple and effective. However, most studies
utilize this method without training, leaving the potential of adaptive
training largely unexplored. Adaptive training could significantly enhance
MLLMs' comprehension of unique inputs while filtering out irrelevant
information. This paper addresses the crucial question: How does training
impact MLLMs' understanding of infused textual detection information? We
systematically experiment with various representative models to evaluate the
effects of training-free, retraining, and fine-tuning strategies. We also
examine the influence of training on MLLMs' original abilities and the
interchangeability of detection models. Our findings indicate that fine-tuning
a pre-trained MLLM to incorporate textual detection information delivers
superior results compared to training-free and retraining methods, improving
performance by 6.71% across 10 widely recognized benchmarks. Furthermore,
fine-tuning enables MLLMs to retain performance enhancements even when
detection models are swapped, indicating improved understanding of formatted
textual data. We release our codes to support further exploration of fusion
strategies for vision detection models and the enhancement of MLLMs'
fine-grained multimodal capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 22 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for
  Adversarial Defense <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23091v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23091v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkun Zhang, Keping Bi, Wei Chen, Quanrun Chen, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite ongoing efforts to defend neural classifiers from adversarial
attacks, they remain vulnerable, especially to unseen attacks. In contrast,
humans are difficult to be cheated by subtle manipulations, since we make
judgments only based on essential factors. Inspired by this observation, we
attempt to model label generation with essential label-causative factors and
incorporate label-non-causative factors to assist data generation. For an
adversarial example, we aim to discriminate the perturbations as non-causative
factors and make predictions only based on the label-causative factors.
Concretely, we propose a casual diffusion model (CausalDiff) that adapts
diffusion models for conditional data generation and disentangles the two types
of casual factors by learning towards a novel casual information bottleneck
objective. Empirically, CausalDiff has significantly outperformed
state-of-the-art defense methods on various unseen attacks, achieving an
average robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on
CIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition
Benchmark). The code is available at
https://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training <span class="highlight-title">Dataset</span>s Generation for Machine Learning: Application to Vision
  Based Navigation <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Hans Krüger, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Based Navigation consists in utilizing cameras as precision sensors
for GNC after extracting information from images. To enable the adoption of
machine learning for space applications, one of obstacles is the demonstration
that available training datasets are adequate to validate the algorithms. The
objective of the study is to generate datasets of images and metadata suitable
for training machine learning algorithms. Two use cases were selected and a
robust methodology was developed to validate the datasets including the ground
truth. The first use case is in-orbit rendezvous with a man-made object: a
mockup of satellite ENVISAT. The second use case is a Lunar landing scenario.
Datasets were produced from archival datasets (Chang'e 3), from the laboratory
at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software
high fidelity image simulator using Model Capture and from Generative
Adversarial Networks. The use case definition included the selection of
algorithms as benchmark: an AI-based pose estimation algorithm and a dense
optical flow algorithm were selected. Eventually it is demonstrated that
datasets produced with SurRender and selected laboratory facilities are
adequate to train machine learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, preprint of the proceedings of ESA SPAICE
  conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Img-Diff: Contrastive Data Synthesis for Multimodal Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04594v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04594v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qirui Jiao, Daoyuan Chen, Yilun Huang, Bolin Ding, Yaliang Li, Ying Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-performance Multimodal Large Language Models (MLLMs) are heavily
dependent on data quality. To advance fine-grained image recognition within
MLLMs, we introduce a novel data synthesis method inspired by contrastive
learning and image difference captioning. Our key idea involves challenging the
model to discern both matching and distinct elements by scrutinizing object
differences in detailed regions across similar images. We begin by generating
pairs of similar images that emphasize object variations. Following this, we
employ a Difference Area Generator to pinpoint object differences, and
subsequently, a Difference Captions Generator to articulate these differences.
This process results in a high-quality dataset of "object replacement" samples,
termed Img-Diff, which can be scaled as needed due to its automated nature. We
leverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs,
such as InternVL2, achieving substantial improvements across various image
difference and Visual Question Answering tasks. Notably, the trained models
significantly outperform existing SOTA models like GPT-4V and Gemini on the
MMVP benchmark. Additionally, we conduct comprehensive evaluations to validate
the dataset's diversity, quality, and robustness, offering several insights
into the synthesis of such contrastive datasets. We release our codes and
dataset to encourage further research on multimodal data synthesis and MLLMs'
fundamental capabilities for image understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Pixel is All I Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deng Siqin, Zhou Xiaoyi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have achieved record-breaking performance in
various visual tasks. However, concerns about their robustness against backdoor
attacks have grown. Backdoor attacks involve associating a specific trigger
with a target label, causing the model to predict the attacker-specified label
when the trigger is present, while correctly identifying clean images.We found
that ViTs exhibit higher attack success rates for quasi-triggers(patterns
different from but similar to the original training triggers)compared to CNNs.
Moreover, some backdoor features in clean samples can suppress the original
trigger, making quasi-triggers more effective.To better understand and exploit
these vulnerabilities, we developed a tool called the Perturbation Sensitivity
Distribution Map (PSDM). PSDM computes and sums gradients over many inputs to
show how sensitive the model is to small changes in the input. In ViTs, PSDM
reveals a patch-like pattern where central pixels are more sensitive than
edges. We use PSDM to guide the creation of quasi-triggers.Based on these
findings, we designed "WorstVIT," a simple yet effective data poisoning
backdoor for ViT models. This attack requires an extremely low poisoning rate,
trains for just one epoch, and modifies a single pixel to successfully attack
all validation images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction-Feedback DETR for Temporal Action Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16729v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16729v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihwan Kim, Miso Lee, Cheol-Ho Cho, Jihyun Lee, Jae-Pil Heo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Action Detection (TAD) is fundamental yet challenging for real-world
video applications. Leveraging the unique benefits of transformers, various
DETR-based approaches have been adopted in TAD. However, it has recently been
identified that the attention collapse in self-attention causes the performance
degradation of DETR for TAD. Building upon previous research, this paper newly
addresses the attention collapse problem in cross-attention within DETR-based
TAD methods. Moreover, our findings reveal that cross-attention exhibits
patterns distinct from predictions, indicating a short-cut phenomenon. To
resolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),
which utilizes predictions to restore the collapse and align the cross- and
self-attention with predictions. Specifically, we devise novel
prediction-feedback objectives using guidance from the relations of the
predictions. As a result, Pred-DETR significantly alleviates the collapse and
achieves state-of-the-art performance among DETR-based methods on various
challenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and
FineAction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding a Diffusion Model with a Bad Version of Itself <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary axes of interest in image-generating diffusion models are image
quality, the amount of variation in the results, and how well the results align
with a given condition, e.g., a class label or a text prompt. The popular
classifier-free guidance approach uses an unconditional model to guide a
conditional model, leading to simultaneously better prompt alignment and
higher-quality images at the cost of reduced variation. These effects seem
inherently entangled, and thus hard to control. We make the surprising
observation that it is possible to obtain disentangled control over image
quality without compromising the amount of variation by guiding generation
using a smaller, less-trained version of the model itself rather than an
unconditional model. This leads to significant improvements in ImageNet
generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using
publicly available networks. Furthermore, the method is also applicable to
unconditional diffusion models, drastically improving their quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Activity Recognition on Avatar-Anonymized <span class="highlight-title">Dataset</span>s with Masked
  Differential Privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Schneider, Sina Sajadmanesh, Vikash Sehwag, Saquib Sarfraz, Rainer Stiefelhagen, Lingjuan Lyu, Vivek Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy-preserving computer vision is an important emerging problem in
machine learning and artificial intelligence. Prevalent methods tackling this
problem use differential privacy (DP) or obfuscation techniques to protect the
privacy of individuals. In both cases, the utility of the trained model is
sacrificed heavily in this process. In this work, we present an anonymization
pipeline that replaces sensitive human subjects in video datasets with
synthetic avatars within context, employing a combined rendering and stable
diffusion-based strategy. Additionally we propose masked differential privacy
({MaskDP}) to protect non-anonymized but privacy sensitive background
information. MaskDP allows for controlling sensitive regions where differential
privacy is applied, in contrast to applying DP on the entire input. This
combined methodology provides strong privacy protection while minimizing the
usual performance penalty of privacy preserving methods. Experiments on
multiple challenging action recognition datasets demonstrate that our proposed
techniques result in better utility-privacy trade-offs compared to standard
differentially private training in the especially demanding $\epsilon<1$
regime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Gao, Lingling Li, Licheng Jiao, Fang Liu, Xu Liu, Wenping Ma, Yuwei Guo, Shuyuan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding 3D scenes is a crucial challenge in computer vision research
with applications spanning multiple domains. Recent advancements in distilling
2D vision-language foundation models into neural fields, like NeRF and 3DGS,
enable open-vocabulary segmentation of 3D scenes from 2D multi-view images
without the need for precise 3D annotations. However, while effective, these
methods typically rely on the per-pixel distillation of high-dimensional CLIP
features, introducing ambiguity and necessitating complex regularization
strategies, which adds inefficiency during training. This paper presents
MaskField, which enables efficient 3D open-vocabulary segmentation with neural
fields from a novel perspective. Unlike previous methods, MaskField decomposes
the distillation of mask and semantic features from foundation models by
formulating a mask feature field and queries. MaskField overcomes ambiguous
object boundaries by naturally introducing SAM segmented object shapes without
extra regularization during training. By circumventing the direct handling of
dense high-dimensional CLIP features during training, MaskField is particularly
compatible with explicit scene representations like 3DGS. Our extensive
experiments show that MaskField not only surpasses prior state-of-the-art
methods but also achieves remarkably fast convergence. We hope that MaskField
will inspire further exploration into how neural fields can be trained to
comprehend 3D scenes from 2D models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, Code:https://github.com/keloee/MaskField</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Classification with Rotation-Invariant Variational Quantum
  Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul San Sebastian, Mikel Cañizo, Román Orús
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational quantum algorithms are gaining attention as an early application
of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of
variational methods lies in the phenomenon of Barren Plateaus, present in the
optimization of variational parameters. Adding geometric inductive bias to the
quantum models has been proposed as a potential solution to mitigate this
problem, leading to a new field called Geometric Quantum Machine Learning. In
this work, an equivariant architecture for variational quantum classifiers is
introduced to create a label-invariant model for image classification with
$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against
two different architectures, and it is experimentally observed that the
geometric approach boosts the model's performance. Finally, a classical
equivariant convolution operation is proposed to extend the quantum model for
the processing of larger images, employing the resources available in NISQ
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversifying Query: Region-Guided <span class="highlight-title">Transformer</span> for Temporal Sentence
  Grounding <span class="chip">AAAI-25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Sun, Liushuai Shi, Le Wang, Sanping Zhou, Kun Xia, Yabing Wang, Gang Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal sentence grounding is a challenging task that aims to localize the
moment spans relevant to a language description. Although recent DETR-based
models have achieved notable progress by leveraging multiple learnable moment
queries, they suffer from overlapped and redundant proposals, leading to
inaccurate predictions. We attribute this limitation to the lack of
task-related guidance for the learnable queries to serve a specific mode.
Furthermore, the complex solution space generated by variable and
open-vocabulary language descriptions complicates optimization, making it
harder for learnable queries to distinguish each other adaptively. To tackle
this limitation, we present a Region-Guided TRansformer (RGTR) for temporal
sentence grounding, which diversifies moment queries to eliminate overlapped
and redundant predictions. Instead of using learnable queries, RGTR adopts a
set of anchor pairs as moment queries to introduce explicit regional guidance.
Each anchor pair takes charge of moment prediction for a specific temporal
region, which reduces the optimization difficulty and ensures the diversity of
the final predictions. In addition, we design an IoU-aware scoring head to
improve proposal quality. Extensive experiments demonstrate the effectiveness
of RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA
and TACoS datasets. Codes are available at https://github.com/TensorsSun/RGTR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-25. Code is available at
  https://github.com/TensorsSun/RGTR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reliable Breast Cancer Molecular Subtype Prediction based on
  uncertainty-aware Bayesian Deep Learning by Mammography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohaddeseh Chegini, Ali Mahloojifar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is a heterogeneous disease with different molecular subtypes,
clinical behavior, treatment responses as well as survival outcomes. The
development of a reliable, accurate, available and inexpensive method to
predict the molecular subtypes using medical images plays an important role in
the diagnosis and prognosis of breast cancer. Recently, deep learning methods
have shown good performance in the breast cancer classification tasks using
various medical images. Despite all that success, classical deep learning
cannot deliver the predictive uncertainty. The uncertainty represents the
validity of the predictions. Therefore, the high predicted uncertainty might
cause a negative effect in the accurate diagnosis of breast cancer molecular
subtypes. To overcome this, uncertainty quantification methods are used to
determine the predictive uncertainty. Accordingly, in this study, we proposed
an uncertainty-aware Bayesian deep learning model using the full mammogram
images. In addition, to increase the performance of the multi-class molecular
subtype classification task, we proposed a novel hierarchical classification
strategy, named the two-stage classification strategy. The separate AUC of the
proposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,
luminal and triple-negative classes, respectively. The proposed model not only
has a comparable performance to other studies in the field of breast cancer
molecular subtypes prediction, even using full mammography images, but it is
also more reliable, due to quantify the predictive uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in
  the Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donggyun Kim, Seongwoong Cho, Semin Kim, Chong Luo, Seunghoon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have evolved data-efficient generalists, benefiting
from the universal language interface and large-scale pre-training. However,
constructing a data-efficient generalist for dense visual prediction presents a
distinct challenge due to the variation in label structures across different
tasks. Consequently, generalization to unseen dense prediction tasks in the
low-data regime is not straightforward and has received less attention from
previous vision generalists. In this study, we explore a universal model that
can flexibly adapt to unseen dense label structures with a few examples,
enabling it to serve as a data-efficient vision generalist in diverse
real-world scenarios. To this end, we base our method on a powerful
meta-learning framework and explore several axes to improve its performance and
versatility for real-world problems, such as flexible adaptation mechanisms and
scalability. We evaluate our model across a spectrum of unseen real-world
scenarios where low-shot learning is desirable, including video, 3D, medical,
biological, and user-interactive tasks. Equipped with a generic architecture
and an effective adaptation mechanism, our model flexibly adapts to all of
these tasks with at most 50 labeled images, showcasing a significant
advancement over existing data-efficient generalist approaches. Codes are
available at https://github.com/GitGyun/chameleon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential
  via Self-Attention Redirection Guidance <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Sun, Benlei Cui, Xue-Mei Dong, Jingqun Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion models have emerged as promising newcomers in the field
of generative models, shining brightly in image generation. However, when
employed for object removal tasks, they still encounter issues such as
generating random artifacts and the incapacity to repaint foreground object
areas with appropriate content after removal. To tackle these problems, we
propose Attentive Eraser, a tuning-free method to empower pre-trained diffusion
models for stable and effective object removal. Firstly, in light of the
observation that the self-attention maps influence the structure and shape
details of the generated images, we propose Attention Activation and
Suppression (ASS), which re-engineers the self-attention mechanism within the
pre-trained diffusion models based on the given mask, thereby prioritizing the
background over the foreground object during the reverse generation process.
Moreover, we introduce Self-Attention Redirection Guidance (SARG), which
utilizes the self-attention redirected by ASS to guide the generation process,
effectively removing foreground objects within the mask while simultaneously
generating content that is both plausible and coherent. Experiments demonstrate
the stability and effectiveness of Attentive Eraser in object removal across a
variety of pre-trained diffusion models, outperforming even training-based
methods. Furthermore, Attentive Eraser can be implemented in various diffusion
model architectures and checkpoints, enabling excellent scalability. Code is
available at https://github.com/Anonym0u3/AttentiveEraser.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution-Consistency-Guided Multi-modal Hashing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Yu Liu, Xian-Ling Mao, Tian-Yi Che, Rong-Cheng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal hashing methods have gained popularity due to their fast speed
and low storage requirements. Among them, the supervised methods demonstrate
better performance by utilizing labels as supervisory signals compared with
unsupervised methods. Currently, for almost all supervised multi-modal hashing
methods, there is a hidden assumption that training sets have no noisy labels.
However, labels are often annotated incorrectly due to manual labeling in
real-world scenarios, which will greatly harm the retrieval performance. To
address this issue, we first discover a significant distribution consistency
pattern through experiments, i.e., the 1-0 distribution of the presence or
absence of each category in the label is consistent with the high-low
distribution of similarity scores of the hash codes relative to category
centers. Then, inspired by this pattern, we propose a novel
Distribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to
filter and reconstruct noisy labels to enhance retrieval performance.
Specifically, the proposed method first randomly initializes several category
centers, which are used to compute the high-low distribution of similarity
scores; Noisy and clean labels are then separately filtered out via the
discovered distribution consistency pattern to mitigate the impact of noisy
labels; Subsequently, a correction strategy, which is indirectly designed via
the distribution consistency pattern, is applied to the filtered noisy labels,
correcting high-confidence ones while treating low-confidence ones as unlabeled
for unsupervised learning, thereby further enhancing the model's performance.
Extensive experiments on three widely used datasets demonstrate the superiority
of the proposed method compared to state-of-the-art baselines in multi-modal
retrieval tasks. The code is available at
https://github.com/LiuJinyu1229/DCGMH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Anthropometric Measurements to Improve Human Mesh Estimation
  and Ensure Consistent Body Shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17671v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17671v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katja Ludwig, Julian Lorenz, Daniel Kienzle, Tuan Bui, Rainer Lienhart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The basic body shape (i.e., the body shape in T-pose) of a person does not
change within a single video. However, most SOTA human mesh estimation (HME)
models output a slightly different, thus inconsistent basic body shape for each
video frame. Furthermore, we find that SOTA 3D human pose estimation (HPE)
models outperform HME models regarding the precision of the estimated 3D
keypoint positions. We solve the problem of inconsistent body shapes by
leveraging anthropometric measurements like taken by tailors from humans. We
create a model called A2B that converts given anthropometric measurements to
basic body shape parameters of human mesh models. We obtain superior and
consistent human meshes by combining the A2B model results with the keypoints
of 3D HPE models using inverse kinematics. We evaluate our approach on
challenging datasets like ASPset or fit3D, where we can lower the MPJPE by over
30 mm compared to SOTA HME models. Further, replacing estimates of the body
shape parameters from existing HME models with A2B results not only increases
the performance of these HME models, but also guarantees consistent body
shapes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DocKylin: A Large Multimodal Model for Visual Document Understanding
  with Efficient Visual Slimming <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.19101v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.19101v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Zhang, Wentao Yang, Songxuan Lai, Zecheng Xie, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal large language models (MLLMs) face significant challenges
in visual document understanding (VDU) tasks due to the high resolution, dense
text, and complex layouts typical of document images. These characteristics
demand a high level of detail perception ability from MLLMs. While increasing
input resolution improves detail perception capability, it also leads to longer
sequences of visual tokens, increasing computational costs and straining the
models' ability to handle long contexts. To address these challenges, we
introduce DocKylin, a document-centric MLLM that performs visual content
slimming at both the pixel and token levels, thereby reducing token sequence
length in VDU scenarios. We introduce an Adaptive Pixel Slimming (APS)
preprocessing module to perform pixel-level slimming, increasing the proportion
of informative pixels. Moreover, we propose a novel Dynamic Token Slimming
(DTS) module to conduct token-level slimming, filtering essential tokens and
removing others to adaptively create a more compact visual sequence.
Experiments demonstrate DocKylin's promising performance across various VDU
benchmarks and the effectiveness of each component.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POPoS: Improving Efficient and Robust Facial Landmark Detection with
  Parallel Optimal Position Search <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09583v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09583v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chong-Yang Xiang, Jun-Yan He, Zhi-Qi Cheng, Xiao Wu, Xian-Sheng Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving a balance between accuracy and efficiency is a critical challenge
in facial landmark detection (FLD). This paper introduces Parallel Optimal
Position Search (POPoS), a high-precision encoding-decoding framework designed
to address the limitations of traditional FLD methods. POPoS employs three key
contributions: (1) Pseudo-range multilateration is utilized to correct heatmap
errors, improving landmark localization accuracy. By integrating multiple
anchor points, it reduces the impact of individual heatmap inaccuracies,
leading to robust overall positioning. (2) To enhance the pseudo-range accuracy
of selected anchor points, a new loss function, named multilateration anchor
loss, is proposed. This loss function enhances the accuracy of the distance
map, mitigates the risk of local optima, and ensures optimal solutions. (3) A
single-step parallel computation algorithm is introduced, boosting
computational efficiency and reducing processing time. Extensive evaluations
across five benchmark datasets demonstrate that POPoS consistently outperforms
existing methods, particularly excelling in low-resolution heatmaps scenarios
with minimal computational overhead. These advantages make POPoS a highly
efficient and accurate tool for FLD, with broad applicability in real-world
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025, 9 pages, 6 figures. Code:
  https://github.com/teslatasy/POPoS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian
  Splatting <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20815v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20815v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Xu, Zexin Fan, Jian Yang, Jin Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Gaussian splatting has received more and more attention in the
field of static scene rendering. Due to the low computational overhead and
inherent flexibility of explicit representations, plane-based explicit methods
are popular ways to predict deformations for Gaussian-based dynamic scene
rendering models. However, plane-based methods rely on the inappropriate
low-rank assumption and excessively decompose the space-time 4D encoding,
resulting in overmuch feature overlap and unsatisfactory rendering quality. To
tackle these problems, we propose Grid4D, a dynamic scene rendering model based
on Gaussian splatting and employing a novel explicit encoding method for the 4D
input through the hash encoding. Different from plane-based explicit
representations, we decompose the 4D encoding into one spatial and three
temporal 3D hash encodings without the low-rank assumption. Additionally, we
design a novel attention module that generates the attention scores in a
directional range to aggregate the spatial and temporal features. The
directional attention enables Grid4D to more accurately fit the diverse
deformations across distinct scene components based on the spatial encoded
features. Moreover, to mitigate the inherent lack of smoothness in explicit
representation methods, we introduce a smooth regularization term that keeps
our model from the chaos of deformation prediction. Our experiments demonstrate
that Grid4D significantly outperforms the state-of-the-art models in visual
quality and rendering speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA Needs More Knowledge: Retrieval Augmented Natural Language
  Generation with Knowledge Graph for Explaining Thoracic Pathologies <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ameer Hamza,  Abdullah, Yong Hyun Ahn, Sungyoung Lee, Seong Tae Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating Natural Language Explanations (NLEs) for model predictions on
medical images, particularly those depicting thoracic pathologies, remains a
critical and challenging task. Existing methodologies often struggle due to
general models' insufficient domain-specific medical knowledge and privacy
concerns associated with retrieval-based augmentation techniques. To address
these issues, we propose a novel Vision-Language framework augmented with a
Knowledge Graph (KG)-based datastore, which enhances the model's understanding
by incorporating additional domain-specific medical knowledge essential for
generating accurate and informative NLEs. Our framework employs a KG-based
retrieval mechanism that not only improves the precision of the generated
explanations but also preserves data privacy by avoiding direct data retrieval.
The KG datastore is designed as a plug-and-play module, allowing for seamless
integration with various model architectures. We introduce and evaluate three
distinct frameworks within this paradigm: KG-LLaVA, which integrates the
pre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining
MedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts
LLaVA by incorporating the Bio-ViT-L vision model. These frameworks are
validated on the MIMIC-NLE dataset, where they achieve state-of-the-art
results, underscoring the effectiveness of KG augmentation in generating
high-quality NLEs for thoracic pathologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoMeO: Robust Metric Visual Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Cheng, Zhipeng Cai, Zhaoxing Zhang, Wei Yin, Matthias Muller, Michael Paulitsch, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a
fundamental building block for many applications such as VR/AR and robotics.
This work focuses on monocular RGB VO where the input is a monocular RGB video
without IMU or 3D sensors. Existing approaches lack robustness under this
challenging scenario and fail to generalize to unseen data (especially
outdoors); they also cannot recover metric-scale poses. We propose Robust
Metric Visual Odometry (RoMeO), a novel method that resolves these issues
leveraging priors from pre-trained depth models. RoMeO incorporates both
monocular metric depth and multi-view stereo (MVS) models to recover
metric-scale, simplify correspondence search, provide better initialization and
regularize optimization. Effective strategies are proposed to inject noise
during training and adaptively filter noisy depth priors, which ensure the
robustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the
state-of-the-art (SOTA) by a large margin across 6 diverse datasets covering
both indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO
reduces the relative (align the trajectory scale with GT) and absolute
trajectory errors both by >50%. The performance gain also transfers to the full
SLAM pipeline (with global BA & loop closure). Code will be released upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Forget-free Winning Subnetworks for Video
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11973v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11973v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Jaehong Yoon, Sung Ju Hwang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) and Task-agnostic
Incremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning
(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is
designed to prevent overfitting when the data samples are scarce. Furthermore,
the sparse reuse of WSN weights is considered for Video Incremental Learning
(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It
enables compact encoding of videos and identifies reusable subnetworks across
varying bandwidths. We have integrated FSO into different architectural
frameworks for continual learning, including VIL, TIL, and FSCIL. Our
comprehensive experiments demonstrate FSO's effectiveness, significantly
improving task performance at various convolutional representational levels.
Specifically, FSO enhances higher-layer performance in TIL and FSCIL and
lower-layer performance in VIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Pattern Analysis and Machine Intelligence
  (T-PAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recoverable Compression: A Multimodal Vision Token Recovery Mechanism
  Guided by Text Information <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01179v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01179v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Chen, Jian Xu, Xu-Yao Zhang, Wen-Zhuo Liu, Yang-Yang Liu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of large-scale language modeling techniques, large
multimodal models combining visual encoders with large language models have
demonstrated exceptional performance in various visual tasks. Most of the
current large-scale multimodal models achieve this by mapping visual features
obtained from the visual encoder into a large language model and using them as
inputs alongside text for downstream tasks. Therefore, the number of visual
tokens directly affects the training and inference speed of the model. There
has been significant work on token pruning for visual transformers, but for
large multimodal models, only relying on visual information for token pruning
or compression may lead to significant loss of important information. On the
other hand, the textual input in the form of a question may contain valuable
information that can aid in answering the question, providing additional
knowledge to the model. To address the potential oversimplification and
excessive pruning that can occur with most purely visual token pruning methods,
we propose a text information-guided dynamic visual token recovery mechanism
that does not require training. This mechanism leverages the similarity between
the question text and visual tokens to recover visually meaningful tokens with
important text information while merging other less important tokens.
Experimental results demonstrate that our proposed method achieves comparable
performance to the original approach while compressing the visual tokens to an
average of 10% of the original quantity. Our source code will be made publicly
available following acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2025 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time
  Adaptation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world vision models in dynamic environments face rapid shifts in domain
distributions, leading to decreased recognition performance. Using unlabeled
test data, continuous test-time adaptation (CTTA) directly adjusts a
pre-trained source discriminative model to these changing domains. A highly
effective CTTA method involves applying layer-wise adaptive learning rates for
selectively adapting pre-trained layers. However, it suffers from the poor
estimation of domain shift and the inaccuracies arising from the pseudo-labels.
This work aims to overcome these limitations by identifying layers for
adaptation via quantifying model prediction uncertainty without relying on
pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by
backpropagating the KL divergence between the softmax output and a uniform
distribution, to select layers for further adaptation. Subsequently, for the
parameters exclusively belonging to these selected layers, with the remaining
ones frozen, we evaluate their sensitivity to approximate the domain shift and
adjust their learning rates accordingly. We conduct extensive image
classification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,
demonstrating the superior efficacy of our method compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust
  Out-of-Distribution Human Action Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20633v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20633v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu, Anqi Zhu, Jingyu Lin, Qiuhong Ke, Cunjian Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human action recognition is crucial in computer vision systems. However, in
real-world scenarios, human actions often fall outside the distribution of
training data, requiring a model to both recognize in-distribution (ID) actions
and reject out-of-distribution (OOD) ones. Despite its importance, there has
been limited research on OOD detection in human actions. Existing works on OOD
detection mainly focus on image data with RGB structure, and many methods are
post-hoc in nature. While these methods are convenient and computationally
efficient, they often lack sufficient accuracy, fail to consider the exposure
of OOD samples, and ignore the application in skeleton structure data. To
address these challenges, we propose a novel end-to-end skeleton-based model
called Skeleton-OOD, which is committed to improving the effectiveness of OOD
tasks while ensuring the accuracy of ID recognition. Through extensive
experiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400
datasets, Skeleton-OOD demonstrates the superior performance of our proposed
approach compared to state-of-the-art methods. Our findings underscore the
effectiveness of classic OOD detection techniques in the context of
skeleton-based action recognition tasks, offering promising avenues for future
research in this field. Code is available at
https://github.com/YilliaJing/Skeleton-OOD.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Neurocomputing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-Shadow: Global-guided Diffusion Model for Shadow Removal <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16214v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16214v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinting Luo, Ru Li, Chengzhi Jiang, Xiaoming Zhang, Mingyan Han, Ting Jiang, Haoqiang Fan, Shuaicheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Diff-Shadow, a global-guided diffusion model for shadow removal.
Previous transformer-based approaches can utilize global information to relate
shadow and non-shadow regions but are limited in their synthesis ability and
recover images with obvious boundaries. In contrast, diffusion-based methods
can generate better content but they are not exempt from issues related to
inconsistent illumination. In this work, we combine the advantages of diffusion
models and global guidance to achieve shadow-free restoration. Specifically, we
propose a parallel UNets architecture: 1) the local branch performs the
patch-based noise estimation in the diffusion process, and 2) the global branch
recovers the low-resolution shadow-free images. A Reweight Cross Attention
(RCA) module is designed to integrate global contextual information of
non-shadow regions into the local branch. We further design a Global-guided
Sampling Strategy (GSS) that mitigates patch boundary issues and ensures
consistent illumination across shaded and unshaded regions in the recovered
image. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have
demonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art
methods, our method achieves a significant improvement in terms of PSNR,
increasing from 32.33dB to 33.69dB on the ISTD dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 39th Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Multi-granular Alignments for Grounded Reasoning in Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang-Hung Le, Long Hoang Dang, Ngan Le, Truyen Tran, Thao Minh Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Large Vision-Language Models (LVLMs) excel at matching concepts
across multi-modal inputs but struggle with compositional concepts and
high-level relationships between entities. This paper introduces Progressive
multi-granular Vision-Language alignments (PromViL), a novel framework to
enhance LVLMs' ability in performing grounded compositional visual reasoning
tasks. Our approach constructs a hierarchical structure of multi-modal
alignments, ranging from simple to complex concepts. By progressively aligning
textual descriptions with corresponding visual regions, our model learns to
leverage contextual information from lower levels to inform higher-level
reasoning. To facilitate this learning process, we introduce a data generation
process that creates a novel dataset derived from Visual Genome, providing a
wide range of nested compositional vision-language pairs. Experimental results
demonstrate that our PromViL framework significantly outperforms baselines on
various visual grounding and compositional question answering tasks. The code
is available at: https://github.com/lqh52/PromViL.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-12-21T05:26:11.017749571Z">
            2024-12-21 05:26:11 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
