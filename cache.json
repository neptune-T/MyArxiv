{"2024-12-19T00:00:00Z":{"Machine Learning":[{"id":"http://arxiv.org/abs/2412.15212v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Scaling 4D Representations","summary":"  Scaling has not yet been convincingly demonstrated for pure self-supervised\nlearning from video. However, prior work has focused evaluations on\nsemantic-related tasks $\\unicode{x2013}$ action classification, ImageNet\nclassification, etc. In this paper we focus on evaluating self-supervised\nlearning on non-semantic vision tasks that are more spatial (3D) and temporal\n(+1D = 4D), such as camera pose estimation, point and object tracking, and\ndepth estimation. We show that by learning from very large video datasets,\nmasked auto-encoding (MAE) with transformer video models actually scales,\nconsistently improving performance on these 4D tasks, as model size increases\nfrom 20M all the way to the largest by far reported self-supervised video model\n$\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with\nmany recent image and video models demonstrates the benefits of scaling 4D\nrepresentations.\n","authors":["João Carreira","Dilara Gokay","Michael King","Chuhan Zhang","Ignacio Rocco","Aravindh Mahendran","Thomas Albert Keck","Joseph Heyward","Skanda Koppula","Etienne Pot","Goker Erdogan","Yana Hasson","Yi Yang","Klaus Greff","Guillaume Le Moing","Sjoerd van Steenkiste","Daniel Zoran","Drew A. Hudson","Pedro Vélez","Luisa Polanía","Luke Friedman","Chris Duvarney","Ross Goroshin","Kelsey Allen","Jacob Walker","Rishabh Kabra","Eric Aboussouan","Jennifer Sun","Thomas Kipf","Carl Doersch","Viorica Pătrăucean","Dima Damen","Pauline Luc","Mehdi S. M. Sajjadi","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2412.15212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15209v1","updated":"2024-12-19T18:59:44Z","published":"2024-12-19T18:59:44Z","title":"PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation","summary":"  Despite significant advancements in Large Vision-Language Models (LVLMs),\nexisting pixel-grounding models operate on single-image settings, limiting\ntheir ability to perform detailed, fine-grained comparisons across multiple\nimages. Conversely, current multi-image understanding models lack pixel-level\ngrounding. Our work addresses this gap by introducing the task of multi-image\npixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates\npixel-level grounding with robust multi-image reasoning capabilities to produce\ncontextually rich, pixel-grounded explanations. Central to PRIMA is an\nefficient vision module that queries fine-grained visual representations across\nmultiple images, reducing TFLOPs by $25.3\\%$. To support training and\nevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark\nconsisting of $\\sim$224K question-answer pairs that require fine-grained visual\nunderstanding across multiple images. Experimental results demonstrate PRIMA\noutperforms state-of-the-art baselines.\n","authors":["Muntasir Wahed","Kiet A. Nguyen","Adheesh Sunil Juvekar","Xinzhuo Li","Xiaona Zhou","Vedant Shah","Tianjiao Yu","Pinar Yanardag","Ismini Lourentzou"],"pdf_url":"https://arxiv.org/pdf/2412.15209v1.pdf","comment":"Project page: https://plan-lab.github.io/prima"},{"id":"http://arxiv.org/abs/2412.15208v1","updated":"2024-12-19T18:59:40Z","published":"2024-12-19T18:59:40Z","title":"OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving","summary":"  Since the advent of Multimodal Large Language Models (MLLMs), they have made\na significant impact across a wide range of real-world applications,\nparticularly in Autonomous Driving (AD). Their ability to process complex\nvisual data and reason about intricate driving scenarios has paved the way for\na new paradigm in end-to-end AD systems. However, the progress of developing\nend-to-end models for AD has been slow, as existing fine-tuning methods demand\nsubstantial resources, including extensive computational power, large-scale\ndatasets, and significant funding. Drawing inspiration from recent advancements\nin inference computing, we propose OpenEMMA, an open-source end-to-end\nframework based on MLLMs. By incorporating the Chain-of-Thought reasoning\nprocess, OpenEMMA achieves significant improvements compared to the baseline\nwhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates\neffectiveness, generalizability, and robustness across a variety of challenging\ndriving scenarios, offering a more efficient and effective approach to\nautonomous driving. We release all the codes in\nhttps://github.com/taco-group/OpenEMMA.\n","authors":["Shuo Xing","Chengyuan Qian","Yuping Wang","Hongyuan Hua","Kexin Tian","Yang Zhou","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15206v1","updated":"2024-12-19T18:59:33Z","published":"2024-12-19T18:59:33Z","title":"AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models\n  for Autonomous Driving","summary":"  Recent advancements in large vision language models (VLMs) tailored for\nautonomous driving (AD) have shown strong scene understanding and reasoning\ncapabilities, making them undeniable candidates for end-to-end driving systems.\nHowever, limited work exists on studying the trustworthiness of DriveVLMs -- a\ncritical factor that directly impacts public transportation safety. In this\npaper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for\nlarge vision-language models in autonomous driving (DriveVLMs), considering\ndiverse perspectives -- including trustfulness, safety, robustness, privacy,\nand fairness. We constructed the largest visual question-answering dataset for\ninvestigating trustworthiness issues in driving scenarios, comprising over 10k\nunique scenes and 18k queries. We evaluated six publicly available VLMs,\nspanning from generalist to specialist, from open-source to commercial models.\nOur exhaustive evaluations have unveiled previously undiscovered\nvulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found\nthat the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform\nspecialized models fine-tuned for driving in terms of overall trustworthiness.\nDriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing\nsensitive information. Additionally, both generalist and specialist VLMs remain\nsusceptible to adversarial attacks and struggle to ensure unbiased\ndecision-making across diverse environments and populations. Our findings call\nfor immediate and decisive action to address the trustworthiness of DriveVLMs\n-- an issue of critical importance to public safety and the welfare of all\ncitizens relying on autonomous transportation systems. Our benchmark is\npublicly available at \\url{https://github.com/taco-group/AutoTrust}, and the\nleaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.\n","authors":["Shuo Xing","Hongyuan Hua","Xiangbo Gao","Shenzhe Zhu","Renjie Li","Kexin Tian","Xiaopeng Li","Heng Huang","Tianbao Yang","Zhangyang Wang","Yang Zhou","Huaxiu Yao","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15206v1.pdf","comment":"55 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.15199v1","updated":"2024-12-19T18:58:36Z","published":"2024-12-19T18:58:36Z","title":"LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation","summary":"  This paper targets the challenge of real-time LiDAR re-simulation in dynamic\ndriving scenarios. Recent approaches utilize neural radiance fields combined\nwith the physical modeling of LiDAR sensors to achieve high-fidelity\nre-simulation results. Unfortunately, these methods face limitations due to\nhigh computational demands in large-scale scenes and cannot perform real-time\nLiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel\nframework that supports real-time, physically accurate LiDAR re-simulation for\ndriving scenes. Our primary contribution is the development of an efficient and\neffective rendering pipeline, which integrates Gaussian primitives and\nhardware-accelerated ray tracing technology. Specifically, we model the\nphysical properties of LiDAR sensors using Gaussian primitives with learnable\nparameters and incorporate scene graphs to handle scene dynamics. Building upon\nthis scene representation, our framework first constructs a bounding volume\nhierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views\nthrough a differentiable rendering algorithm. Importantly, our framework\nsupports realistic rendering with flexible scene editing operations and various\nsensor configurations. Extensive experiments across multiple public benchmarks\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nrendering quality and efficiency. Our project page is at\nhttps://zju3dv.github.io/lidar-rt.\n","authors":["Chenxu Zhou","Lvchang Fu","Sida Peng","Yunzhi Yan","Zhanhua Zhang","Yong Chen","Jiazhi Xia","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15199v1.pdf","comment":"Project page: https://zju3dv.github.io/lidar-rt"},{"id":"http://arxiv.org/abs/2412.15195v1","updated":"2024-12-19T18:58:14Z","published":"2024-12-19T18:58:14Z","title":"Preventing Local Pitfalls in Vector Quantization via Optimal Transport","summary":"  Vector-quantized networks (VQNs) have exhibited remarkable performance across\nvarious tasks, yet they are prone to training instability, which complicates\nthe training process due to the necessity for techniques such as subtle\ninitialization and model distillation. In this study, we identify the local\nminima issue as the primary cause of this instability. To address this, we\nintegrate an optimal transport method in place of the nearest neighbor search\nto achieve a more globally informed assignment. We introduce OptVQ, a novel\nvector quantization method that employs the Sinkhorn algorithm to optimize the\noptimal transport problem, thereby enhancing the stability and efficiency of\nthe training process. To mitigate the influence of diverse data distributions\non the Sinkhorn algorithm, we implement a straightforward yet effective\nnormalization strategy. Our comprehensive experiments on image reconstruction\ntasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses\ncurrent state-of-the-art VQNs in reconstruction quality.\n","authors":["Borui Zhang","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.15195v1.pdf","comment":"Code is available at https://github.com/zbr17/OptVQ"},{"id":"http://arxiv.org/abs/2412.15191v1","updated":"2024-12-19T18:57:21Z","published":"2024-12-19T18:57:21Z","title":"AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation","summary":"  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video\ngeneration that leverages the activations of frozen video and audio diffusion\nmodels for temporally-aligned cross-modal conditioning. The key to our\nframework is a Fusion Block that enables bidirectional information exchange\nbetween our backbone video and audio diffusion models through a\ntemporally-aligned self attention operation. Unlike prior work that uses\nfeature extractors pretrained for other tasks for the conditioning signal,\nAV-Link can directly leverage features obtained by the complementary modality\nin a single framework i.e. video features to generate audio, or audio features\nto generate video. We extensively evaluate our design choices and demonstrate\nthe ability of our method to achieve synchronized and high-quality audiovisual\ncontent, showcasing its potential for applications in immersive media\ngeneration. Project Page: snap-research.github.io/AVLink/\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Ivan Skorokhodov","Alper Canberk","Kwot Sin Lee","Vicente Ordonez","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2412.15191v1.pdf","comment":"Project Page: snap-research.github.io/AVLink/"},{"id":"http://arxiv.org/abs/2412.15188v1","updated":"2024-12-19T18:56:24Z","published":"2024-12-19T18:56:24Z","title":"LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation","summary":"  We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.\n","authors":["Weijia Shi","Xiaochuang Han","Chunting Zhou","Weixin Liang","Xi Victoria Lin","Luke Zettlemoyer","Lili Yu"],"pdf_url":"https://arxiv.org/pdf/2412.15188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15184v1","updated":"2024-12-19T18:55:17Z","published":"2024-12-19T18:55:17Z","title":"Data for Mathematical Copilots: Better Ways of Presenting Proofs for\n  Machine Learning","summary":"  The suite of datasets commonly used to train and evaluate the mathematical\ncapabilities of AI-based mathematical copilots (primarily large language\nmodels) exhibit several shortcomings. These limitations include a restricted\nscope of mathematical complexity, typically not exceeding lower\nundergraduate-level mathematics, binary rating protocols and other issues,\nwhich makes comprehensive proof-based evaluation suites difficult. We\nsystematically explore these limitations and contend that enhancing the\ncapabilities of large language models, or any forthcoming advancements in\nAI-based mathematical assistants (copilots or \"thought partners\"), necessitates\na paradigm shift in the design of mathematical datasets and the evaluation\ncriteria of mathematical ability: It is necessary to move away from\nresult-based datasets (theorem statement to theorem proof) and convert the rich\nfacets of mathematical research practice to data LLMs can train on. Examples of\nthese are mathematical workflows (sequences of atomic, potentially\nsubfield-dependent tasks that are often performed when creating new\nmathematics), which are an important part of the proof-discovery process.\nAdditionally, we advocate for mathematical dataset developers to consider the\nconcept of \"motivated proof\", introduced by G. P\\'olya in 1949, which can serve\nas a blueprint for datasets that offer a better proof learning signal,\nalleviating some of the mentioned limitations. Lastly, we introduce math\ndatasheets for datasets, extending the general, dataset-agnostic variants of\ndatasheets: We provide a questionnaire designed specifically for math datasets\nthat we urge dataset creators to include with their datasets. This will make\ncreators aware of potential limitations of their datasets while at the same\ntime making it easy for readers to assess it from the point of view of training\nand evaluating mathematical copilots.\n","authors":["Simon Frieder","Jonas Bayer","Katherine M. Collins","Julius Berner","Jacob Loader","András Juhász","Fabian Ruehle","Sean Welleck","Gabriel Poesia","Ryan-Rhys Griffiths","Adrian Weller","Anirudh Goyal","Thomas Lukasiewicz","Timothy Gowers"],"pdf_url":"https://arxiv.org/pdf/2412.15184v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2412.15182v1","updated":"2024-12-19T18:54:06Z","published":"2024-12-19T18:54:06Z","title":"STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning","summary":"  Robot learning is witnessing a significant increase in the size, diversity,\nand complexity of pre-collected datasets, mirroring trends in domains such as\nnatural language processing and computer vision. Many robot learning methods\ntreat such datasets as multi-task expert data and learn a multi-task,\ngeneralist policy by training broadly across them. Notably, while these\ngeneralist policies can improve the average performance across many tasks, the\nperformance of generalist policies on any one task is often suboptimal due to\nnegative transfer between partitions of the data, compared to task-specific\nspecialist policies. In this work, we argue for the paradigm of training\npolicies during deployment given the scenarios they encounter: rather than\ndeploying pre-trained policies to unseen problems in a zero-shot manner, we\nnon-parametrically retrieve and train models directly on relevant data at test\ntime. Furthermore, we show that many robotics tasks share considerable amounts\nof low-level behaviors and that retrieval at the \"sub\"-trajectory granularity\nenables significantly improved data utilization, generalization, and robustness\nin adapting policies to novel problems. In contrast, existing full-trajectory\nretrieval methods tend to underutilize the data and miss out on shared\ncross-task content. This work proposes STRAP, a technique for leveraging\npre-trained vision foundation models and dynamic time warping to retrieve\nsub-sequences of trajectories from large training corpora in a robust fashion.\nSTRAP outperforms both prior retrieval algorithms and multi-task learning\nmethods in simulated and real experiments, showing the ability to scale to much\nlarger offline datasets in the real world as well as the ability to learn\nrobust control policies with just a handful of real-world demonstrations.\n","authors":["Marius Memmel","Jacob Berg","Bingqing Chen","Abhishek Gupta","Jonathan Francis"],"pdf_url":"https://arxiv.org/pdf/2412.15182v1.pdf","comment":"Project website at https://weirdlabuw.github.io/strap/"},{"id":"http://arxiv.org/abs/2412.15178v1","updated":"2024-12-19T18:52:05Z","published":"2024-12-19T18:52:05Z","title":"HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages","summary":"  Large Language Model (LLM) based coding tools have been tremendously\nsuccessful as software development assistants, yet they are often designed for\ngeneral purpose programming tasks and perform poorly for more specialized\ndomains such as high performance computing. Creating specialized models and\ntools for these domains is crucial towards gaining the benefits of LLMs in\nareas such as HPC. While previous work has explored HPC-specific models, LLMs\nstill struggle to generate parallel code and it is not at all clear what\nhurdles are still holding back these LLMs and what must be done to overcome\nthem. In this work, we conduct an in-depth study along the many axes of\nfine-tuning a specialized HPC LLM in order to better understand the challenges.\nBased on our findings we fine-tune and evaluate a specialized HPC LLM that is\nshown to be the best performing open-source code LLM for parallel code\ngeneration to date.\n","authors":["Aman Chaturvedi","Daniel Nichols","Siddharth Singh","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2412.15178v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15176v1","updated":"2024-12-19T18:51:06Z","published":"2024-12-19T18:51:06Z","title":"Rethinking Uncertainty Estimation in Natural Language Generation","summary":"  Large Language Models (LLMs) are increasingly employed in real-world\napplications, driving the need to evaluate the trustworthiness of their\ngenerated text. To this end, reliable uncertainty estimation is essential.\nSince current LLMs generate text autoregressively through a stochastic process,\nthe same prompt can lead to varying outputs. Consequently, leading uncertainty\nestimation methods generate and analyze multiple output sequences to determine\nthe LLM's uncertainty. However, generating output sequences is computationally\nexpensive, making these methods impractical at scale. In this work, we inspect\nthe theoretical foundations of the leading methods and explore new directions\nto enhance their computational efficiency. Building on the framework of proper\nscoring rules, we find that the negative log-likelihood of the most likely\noutput sequence constitutes a theoretically grounded uncertainty measure. To\napproximate this alternative measure, we propose G-NLL, which has the advantage\nof being obtained using only a single output sequence generated by greedy\ndecoding. This makes uncertainty estimation more efficient and straightforward,\nwhile preserving theoretical rigor. Empirical results demonstrate that G-NLL\nachieves state-of-the-art performance across various LLMs and tasks. Our work\nlays the foundation for efficient and reliable uncertainty estimation in\nnatural language generation, challenging the necessity of more computationally\ninvolved methods currently leading the field.\n","authors":["Lukas Aichberger","Kajetan Schweighofer","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2412.15176v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18479v2","updated":"2024-12-19T18:49:00Z","published":"2024-11-27T16:22:33Z","title":"SoK: Watermarking for AI-Generated Content","summary":"  As the outputs of generative AI (GenAI) techniques improve in quality, it\nbecomes increasingly challenging to distinguish them from human-created\ncontent. Watermarking schemes are a promising approach to address the problem\nof distinguishing between AI and human-generated content. These schemes embed\nhidden signals within AI-generated content to enable reliable detection. While\nwatermarking is not a silver bullet for addressing all risks associated with\nGenAI, it can play a crucial role in enhancing AI safety and trustworthiness by\ncombating misinformation and deception. This paper presents a comprehensive\noverview of watermarking techniques for GenAI, beginning with the need for\nwatermarking from historical and regulatory perspectives. We formalize the\ndefinitions and desired properties of watermarking schemes and examine the key\nobjectives and threat models for existing approaches. Practical evaluation\nstrategies are also explored, providing insights into the development of robust\nwatermarking techniques capable of resisting various attacks. Additionally, we\nreview recent representative works, highlight open challenges, and discuss\npotential directions for this emerging field. By offering a thorough\nunderstanding of watermarking in GenAI, this work aims to guide researchers in\nadvancing watermarking methods and applications, and support policymakers in\naddressing the broader implications of GenAI.\n","authors":["Xuandong Zhao","Sam Gunn","Miranda Christ","Jaiden Fairoze","Andres Fabrega","Nicholas Carlini","Sanjam Garg","Sanghyun Hong","Milad Nasr","Florian Tramer","Somesh Jha","Lei Li","Yu-Xiang Wang","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2411.18479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06289v3","updated":"2024-12-19T18:47:54Z","published":"2024-12-09T08:24:11Z","title":"S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by\n  Structured Sparsity","summary":"  Current PEFT methods for LLMs can achieve either high quality, efficient\ntraining, or scalable serving, but not all three simultaneously. To address\nthis limitation, we investigate sparse fine-tuning and observe a remarkable\nimprovement in generalization ability. Utilizing this key insight, we propose a\nfamily of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which\nconcurrently achieve state-of-the-art fine-tuning performance, training\nefficiency, and inference scalability. S$^{2}$FT accomplishes this by\n\"selecting sparsely and computing densely\". It selects a few heads and channels\nin the MHA and FFN modules for each Transformer block, respectively. Next, it\nco-permutes weight matrices on both sides of the coupled structures in LLMs to\nconnect the selected components in each layer into a dense submatrix. Finally,\nS$^{2}$FT performs in-place gradient updates on all submatrices. Through\ntheoretical analysis and empirical results, our method prevents forgetting\nwhile simplifying optimization, delivers SOTA performance on both commonsense\nand arithmetic reasoning with 4.6% and 1.3% average improvements compared to\nLoRA, and surpasses full FT by 11.5% when generalizing to various domains after\ninstruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT\nsaves training memory up to 3$\\times$ and improves latency by 1.5-2.7$\\times$\ncompared to full FT, while delivering an average 10% improvement over LoRA on\nboth metrics. We further demonstrate that the weight updates in S$^{2}$FT can\nbe decoupled into adapters, enabling effective fusion, fast switch, and\nefficient parallelism for serving multiple fine-tuned models.\n","authors":["Xinyu Yang","Jixuan Leng","Geyang Guo","Jiawei Zhao","Ryumei Nakada","Linjun Zhang","Huaxiu Yao","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2412.06289v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15163v1","updated":"2024-12-19T18:38:13Z","published":"2024-12-19T18:38:13Z","title":"Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents","summary":"  Social norms are standards of behaviour common in a society. However, when\nagents make decisions without considering how others are impacted, norms can\nemerge that lead to the subjugation of certain agents. We present RAWL-E, a\nmethod to create ethical norm-learning agents. RAWL-E agents operationalise\nmaximin, a fairness principle from Rawlsian ethics, in their decision-making\nprocesses to promote ethical norms by balancing societal well-being with\nindividual goals. We evaluate RAWL-E agents in simulated harvesting scenarios.\nWe find that norms emerging in RAWL-E agent societies enhance social welfare,\nfairness, and robustness, and yield higher minimum experience compared to those\nthat emerge in agent societies that do not implement Rawlsian ethics.\n","authors":["Jessica Woodgate","Paul Marshall","Nirav Ajmeri"],"pdf_url":"https://arxiv.org/pdf/2412.15163v1.pdf","comment":"14 pages, 7 figures, 8 tables (and supplementary material with\n  reproducibility and additional results), accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15150v1","updated":"2024-12-19T18:28:37Z","published":"2024-12-19T18:28:37Z","title":"Leveraging Color Channel Independence for Improved Unsupervised Object\n  Detection","summary":"  Object-centric architectures can learn to extract distinct object\nrepresentations from visual scenes, enabling downstream applications on the\nobject level. Similarly to autoencoder-based image models, object-centric\napproaches have been trained on the unsupervised reconstruction loss of images\nencoded by RGB color spaces. In our work, we challenge the common assumption\nthat RGB images are the optimal color space for unsupervised learning in\ncomputer vision. We discuss conceptually and empirically that other color\nspaces, such as HSV, bear essential characteristics for object-centric\nrepresentation learning, like robustness to lighting conditions. We further\nshow that models improve when requiring them to predict additional color\nchannels. Specifically, we propose to transform the predicted targets to the\nRGB-S space, which extends RGB with HSV's saturation component and leads to\nmarkedly better reconstruction and disentanglement for five common evaluation\ndatasets. The use of composite color spaces can be implemented with basically\nno computational overhead, is agnostic of the models' architecture, and is\nuniversally applicable across a wide range of visual computing tasks and\ntraining types. The findings of our approach encourage additional\ninvestigations in computer vision tasks beyond object-centric learning.\n","authors":["Bastian Jäckl","Yannick Metz","Udo Schlegel","Daniel A. Keim","Maximilian T. Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.15150v1.pdf","comment":"38 pages incl. references, 16 figures"},{"id":"http://arxiv.org/abs/2412.15129v1","updated":"2024-12-19T18:09:42Z","published":"2024-12-19T18:09:42Z","title":"Jet: A Modern Transformer-Based Normalizing Flow","summary":"  In the past, normalizing generative flows have emerged as a promising class\nof generative models for natural images. This type of model has many modeling\nadvantages: the ability to efficiently compute log-likelihood of the input\ndata, fast generation and simple overall structure. Normalizing flows remained\na topic of active research but later fell out of favor, as visual quality of\nthe samples was not competitive with other model classes, such as GANs,\nVQ-VAE-based approaches or diffusion models. In this paper we revisit the\ndesign of the coupling-based normalizing flow models by carefully ablating\nprior design choices and using computational blocks based on the Vision\nTransformer architecture, not convolutional neural networks. As a result, we\nachieve state-of-the-art quantitative and qualitative performance with a much\nsimpler architecture. While the overall visual quality is still behind the\ncurrent state-of-the-art models, we argue that strong normalizing flow models\ncan help advancing research frontier by serving as building components of more\npowerful generative models.\n","authors":["Alexander Kolesnikov","André Susano Pinto","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2412.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15127v1","updated":"2024-12-19T18:08:04Z","published":"2024-12-19T18:08:04Z","title":"Adaptive Pruning for Large Language Models with Structural Importance\n  Awareness","summary":"  The recent advancements in large language models (LLMs) have significantly\nimproved language understanding and generation capabilities. However, it is\ndifficult to deploy LLMs on resource-constrained edge devices due to their high\ncomputational and storage resource demands. To address this issue, we propose a\nnovel LLM model pruning method, namely structurally-aware adaptive pruning\n(SAAP), to significantly reduce the computational and memory costs while\nmaintaining model performance. We first define an adaptive importance fusion\nmetric to evaluate the importance of all coupled structures in LLMs by\nconsidering their homoscedastic uncertainty. Then, we rank the importance of\nall modules to determine the specific layers that should be pruned to meet\nparticular performance requirements. Furthermore, we develop a new group\nfine-tuning strategy to improve the inference efficiency of LLMs. Finally, we\nevaluate the proposed SAAP method on multiple LLMs across two common tasks,\ni.e., zero-shot classification and text generation. Experimental results show\nthat our SAAP method outperforms several state-of-the-art baseline methods,\nachieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and\nLLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,\nshowcasing its practical advantages in resource-constrained scenarios.\n","authors":["Haotian Zheng","Jinke Ren","Yushan Sun","Ruichen Zhang","Wenbo Zhang","Zhen Li","Dusit Niyato","Shuguang Cui","Yatong Han"],"pdf_url":"https://arxiv.org/pdf/2412.15127v1.pdf","comment":"12 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2412.15118v1","updated":"2024-12-19T17:59:42Z","published":"2024-12-19T17:59:42Z","title":"Outcome-Refining Process Supervision for Code Generation","summary":"  Large Language Models have demonstrated remarkable capabilities in code\ngeneration, yet they often struggle with complex programming tasks that require\ndeep algorithmic reasoning. While process supervision through learned reward\nmodels shows promise in guiding reasoning steps, it requires expensive training\ndata and suffers from unreliable evaluation. We propose Outcome-Refining\nProcess Supervision, a novel paradigm that treats outcome refinement itself as\nthe process to be supervised. Our framework leverages concrete execution\nsignals to ground the supervision of reasoning steps, while using\ntree-structured exploration to maintain multiple solution trajectories\nsimultaneously. Experiments demonstrate that our approach enables even smaller\nmodels to achieve high success accuracy and performance metrics on competitive\nprogramming tasks, creates more reliable verification than traditional reward\nmodels without requiring training PRMs. Our approach achieves significant\nimprovements across 5 models and 3 datasets: an average of 26.9% increase in\ncorrectness and 42.2% in efficiency. The results suggest that providing\nstructured reasoning space with concrete verification signals is crucial for\nsolving complex programming tasks. We open-source all our code and data at:\nhttps://github.com/zhuohaoyu/ORPS\n","authors":["Zhuohao Yu","Weizheng Gu","Yidong Wang","Zhengran Zeng","Jindong Wang","Wei Ye","Shikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15118v1.pdf","comment":"18 pages, 5 figures, Code: https://github.com/zhuohaoyu/ORPS"},{"id":"http://arxiv.org/abs/2409.18472v2","updated":"2024-12-19T17:57:43Z","published":"2024-09-27T06:18:55Z","title":"URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological\n  and Multilingual Knowledge Base","summary":"  URIEL is a knowledge base offering geographical, phylogenetic, and\ntypological vector representations for 7970 languages. It includes distance\nmeasures between these vectors for 4005 languages, which are accessible via the\nlang2vec tool. Despite being frequently cited, URIEL is limited in terms of\nlinguistic inclusion and overall usability. To tackle these challenges, we\nintroduce URIEL+, an enhanced version of URIEL and lang2vec that addresses\nthese limitations. In addition to expanding typological feature coverage for\n2898 languages, URIEL+ improves the user experience with robust, customizable\ndistance calculations to better suit the needs of users. These upgrades also\noffer competitive performance on downstream tasks and provide distances that\nbetter align with linguistic distance studies.\n","authors":["Aditya Khan","Mason Shipton","David Anugraha","Kaiyao Duan","Phuong H. Hoang","Eric Khiu","A. Seza Doğruöz","En-Shiun Annie Lee"],"pdf_url":"https://arxiv.org/pdf/2409.18472v2.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.04619v3","updated":"2024-12-19T17:51:34Z","published":"2024-12-05T21:12:37Z","title":"Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization","summary":"  Language models (LMs), like other neural networks, often favor shortcut\nheuristics based on surface-level patterns. Although LMs behave like n-gram\nmodels early in training, they must eventually learn hierarchical syntactic\nrepresentations to correctly apply grammatical rules out-of-distribution (OOD).\nIn this work, we use case studies of English grammar to explore how complex,\ndiverse training data drives models to generalize OOD. We construct a framework\nthat unifies our understanding of random variation with training dynamics, rule\nselection with memorization, and data diversity with complexity. We show that\nthese factors are nuanced, and that intermediate levels of diversity and\ncomplexity lead to inconsistent behavior across random seeds and to unstable\ntraining dynamics. Our findings emphasize the critical role of training data in\nshaping generalization patterns and illuminate how competing model strategies\nlead to inconsistent generalization outcomes across random seeds. Code is\navailable at https://github.com/sunnytqin/concept_comp.git.\n","authors":["Tian Qin","Naomi Saphra","David Alvarez-Melis"],"pdf_url":"https://arxiv.org/pdf/2412.04619v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15100v1","updated":"2024-12-19T17:48:03Z","published":"2024-12-19T17:48:03Z","title":"Tests for model misspecification in simulation-based inference: from\n  local distortions to global model checks","summary":"  Model misspecification analysis strategies, such as anomaly detection, model\nvalidation, and model comparison are a key component of scientific model\ndevelopment. Over the last few years, there has been a rapid rise in the use of\nsimulation-based inference (SBI) techniques for Bayesian parameter estimation,\napplied to increasingly complex forward models. To move towards fully\nsimulation-based analysis pipelines, however, there is an urgent need for a\ncomprehensive simulation-based framework for model misspecification analysis.\nIn this work, we provide a solid and flexible foundation for a wide range of\nmodel discrepancy analysis tasks, using distortion-driven model\nmisspecification tests. From a theoretical perspective, we introduce the\nstatistical framework built around performing many hypothesis tests for\ndistortions of the simulation model. We also make explicit analytic connections\nto classical techniques: anomaly detection, model validation, and\ngoodness-of-fit residual analysis. Furthermore, we introduce an efficient\nself-calibrating training algorithm that is useful for practitioners. We\ndemonstrate the performance of the framework in multiple scenarios, making the\nconnection to classical results where they are valid. Finally, we show how to\nconduct such a distortion-driven model misspecification test for real\ngravitational wave data, specifically on the event GW150914.\n","authors":["Noemi Anau Montel","James Alvey","Christoph Weniger"],"pdf_url":"https://arxiv.org/pdf/2412.15100v1.pdf","comment":"11 pages, 5 figures. Code available on github (NoemiAM/mist) at\n  https://github.com/NoemiAM/mist"},{"id":"http://arxiv.org/abs/2412.15095v1","updated":"2024-12-19T17:45:08Z","published":"2024-12-19T17:45:08Z","title":"A Full Transformer-based Framework for Automatic Pain Estimation using\n  Videos","summary":"  The automatic estimation of pain is essential in designing an optimal pain\nmanagement system offering reliable assessment and reducing the suffering of\npatients. In this study, we present a novel full transformer-based framework\nconsisting of a Transformer in Transformer (TNT) model and a Transformer\nleveraging cross-attention and self-attention blocks. Elaborating on videos\nfrom the BioVid database, we demonstrate state-of-the-art performances, showing\nthe efficacy, efficiency, and generalization capability across all the primary\npain estimation tasks.\n","authors":["Stefanos Gkikas","Manolis Tsiknakis"],"pdf_url":"https://arxiv.org/pdf/2412.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15086v1","updated":"2024-12-19T17:33:56Z","published":"2024-12-19T17:33:56Z","title":"Learning Disentangled Equivariant Representation for Explicitly\n  Controllable 3D Molecule Generation","summary":"  We consider the conditional generation of 3D drug-like molecules with\n\\textit{explicit control} over molecular properties such as drug-like\nproperties (e.g., Quantitative Estimate of Druglikeness or Synthetic\nAccessibility score) and effectively binding to specific protein sites. To\ntackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and\nfactorize the latent space of our generative model into two disentangled\naspects: molecular properties and the remaining structural context of 3D\nmolecules. Our model ensures explicit control over these molecular attributes\nwhile maintaining equivariance of coordinate representation and invariance of\ndata likelihood. Furthermore, we introduce a novel alignment-based coordinate\nloss to adapt equivariant networks for auto-regressive de-novo 3D molecule\ngeneration from scratch. Extensive experiments validate our model's\neffectiveness on property-guided and context-guided molecule generation, both\nfor de-novo 3D molecule design and structure-based drug discovery against\nprotein targets.\n","authors":["Haoran Liu","Youzhi Luo","Tianxiao Li","James Caverlee","Martin Renqiang Min"],"pdf_url":"https://arxiv.org/pdf/2412.15086v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15084v1","updated":"2024-12-19T17:29:44Z","published":"2024-12-19T17:29:44Z","title":"AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward\n  Modeling","summary":"  In this paper, we introduce AceMath, a suite of frontier math models that\nexcel in solving complex math problems, along with highly effective reward\nmodels capable of evaluating generated solutions and reliably identifying the\ncorrect ones. To develop the instruction-tuned math models, we propose a\nsupervised fine-tuning (SFT) process that first achieves competitive\nperformance across general domains, followed by targeted fine-tuning for the\nmath domain using a carefully curated set of prompts and synthetically\ngenerated responses. The resulting model, AceMath-72B-Instruct greatly\noutperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop\nmath-specialized reward model, we first construct AceMath-RewardBench, a\ncomprehensive and robust benchmark for evaluating math reward models across\ndiverse problems and difficulty levels. After that, we present a systematic\napproach to build our math reward models. The resulting model, AceMath-72B-RM,\nconsistently outperforms state-of-the-art reward models. Furthermore, when\ncombining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest\naverage rm@8 score across the math reasoning benchmarks. We will release model\nweights, training data, and evaluation benchmarks at:\nhttps://research.nvidia.com/labs/adlr/acemath\n","authors":["Zihan Liu","Yang Chen","Mohammad Shoeybi","Bryan Catanzaro","Wei Ping"],"pdf_url":"https://arxiv.org/pdf/2412.15084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15077v1","updated":"2024-12-19T17:26:07Z","published":"2024-12-19T17:26:07Z","title":"Till the Layers Collapse: Compressing a Deep Neural Network through the\n  Lenses of Batch Normalization Layers","summary":"  Today, deep neural networks are widely used since they can handle a variety\nof complex tasks. Their generality makes them very powerful tools in modern\ntechnology. However, deep neural networks are often overparameterized. The\nusage of these large models consumes a lot of computation resources. In this\npaper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers\n\\textbf{C}ollapse (TLC), which compresses deep neural networks through the\nlenses of batch normalization layers. By reducing the depth of these networks,\nour method decreases deep neural networks' computational requirements and\noverall latency. We validate our method on popular models such as Swin-T,\nMobileNet-V2, and RoBERTa, across both image classification and natural\nlanguage processing (NLP) tasks.\n","authors":["Zhu Liao","Nour Hezbri","Victor Quétu","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2412.15077v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15075v1","updated":"2024-12-19T17:24:15Z","published":"2024-12-19T17:24:15Z","title":"DroughtSet: Understanding Drought Through Spatial-Temporal Learning","summary":"  Drought is one of the most destructive and expensive natural disasters,\nseverely impacting natural resources and risks by depleting water resources and\ndiminishing agricultural yields. Under climate change, accurately predicting\ndrought is critical for mitigating drought-induced risks. However, the\nintricate interplay among the physical and biological drivers that regulate\ndroughts limits the predictability and understanding of drought, particularly\nat a subseasonal to seasonal (S2S) time scale. While deep learning has been\ndemonstrated with potential in addressing climate forecasting challenges, its\napplication to drought prediction has received relatively less attention. In\nthis work, we propose a new dataset, DroughtSet, which integrates relevant\npredictive features and three drought indices from multiple remote sensing and\nreanalysis datasets across the contiguous United States (CONUS). DroughtSet\nspecifically provides the machine learning community with a new real-world\ndataset to benchmark drought prediction models and more generally, time-series\nforecasting methods. Furthermore, we propose a spatial-temporal model SPDrought\nto predict and interpret S2S droughts. Our model learns from the spatial and\ntemporal information of physical and biological features to predict three types\nof droughts simultaneously. Multiple strategies are employed to quantify the\nimportance of physical and biological features for drought prediction. Our\nresults provide insights for researchers to better understand the\npredictability and sensitivity of drought to biological and physical\nconditions. We aim to contribute to the climate field by proposing a new tool\nto predict and understand the occurrence of droughts and provide the AI\ncommunity with a new benchmark to study deep learning applications in climate\nscience.\n","authors":["Xuwei Tan","Qian Zhao","Yanlan Liu","Xueru Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.15075v1.pdf","comment":"Accepted by AAAI25"},{"id":"http://arxiv.org/abs/2408.15165v2","updated":"2024-12-19T17:11:11Z","published":"2024-08-27T16:03:18Z","title":"Latent Ewald summation for machine learning of long-range interactions","summary":"  Machine learning interatomic potentials (MLIPs) often neglect long-range\ninteractions, such as electrostatic and dispersion forces. In this work, we\nintroduce a straightforward and efficient method to account for long-range\ninteractions by learning a latent variable from local atomic descriptors and\napplying an Ewald summation to this variable. We demonstrate that in systems\nincluding charged and polar molecular dimers, bulk water, and water-vapor\ninterface, standard short-ranged MLIPs can lead to unphysical predictions even\nwhen employing message passing. The long-range models effectively eliminate\nthese artifacts, with only about twice the computational cost of short-range\nMLIPs.\n","authors":["Bingqing Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.15165v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15058v1","updated":"2024-12-19T17:06:53Z","published":"2024-12-19T17:06:53Z","title":"MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging\n  Datasets with In-Context Guidance","summary":"  Medical researchers and clinicians often need to perform novel segmentation\ntasks on a set of related images. Existing methods for segmenting a new dataset\nare either interactive, requiring substantial human effort for each image, or\nrequire an existing set of manually labeled images. We introduce a system,\nMultiverSeg, that enables practitioners to rapidly segment an entire new\ndataset without requiring access to any existing labeled data from that task or\ndomain. Along with the image to segment, the model takes user interactions such\nas clicks, bounding boxes or scribbles as input, and predicts a segmentation.\nAs the user segments more images, those images and segmentations become\nadditional inputs to the model, providing context. As the context set of\nlabeled images grows, the number of interactions required to segment each new\nimage decreases. We demonstrate that MultiverSeg enables users to interactively\nsegment new datasets efficiently, by amortizing the number of interactions per\nimage to achieve an accurate segmentation. Compared to using a state-of-the-art\ninteractive segmentation method, using MultiverSeg reduced the total number of\nscribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images\nfrom unseen tasks. We release code and model weights at\nhttps://multiverseg.csail.mit.edu\n","authors":["Hallee E. Wong","Jose Javier Gonzalez Ortiz","John Guttag","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2412.15058v1.pdf","comment":"Project Website: https://multiverseg.csail.mit.edu Keywords:\n  interactive segmentation, in-context learning, medical image analysis,\n  biomedical imaging, image annotation, visual prompting"},{"id":"http://arxiv.org/abs/2407.17710v2","updated":"2024-12-19T16:48:59Z","published":"2024-07-25T02:05:15Z","title":"Revisiting Machine Unlearning with Dimensional Alignment","summary":"  Machine unlearning, an emerging research topic focusing on compliance with\ndata privacy regulations, enables trained models to remove the information\nlearned from specific data. While many existing methods indirectly address this\nissue by intentionally injecting incorrect supervisions, they can drastically\nand unpredictably alter the decision boundaries and feature spaces, leading to\ntraining instability and undesired side effects. To fundamentally approach this\ntask, we first analyze the changes in latent feature spaces between original\nand retrained models, and observe that the feature representations of samples\nnot involved in training are closely aligned with the feature manifolds of\npreviously seen samples in training. Based on these findings, we introduce a\nnovel evaluation metric for machine unlearning, coined dimensional alignment,\nwhich measures the alignment between the eigenspaces of the forget and retain\nset samples. We employ this metric as a regularizer loss to build a robust and\nstable unlearning framework, which is further enhanced by integrating a\nself-distillation loss and an alternating training scheme. Our framework\neffectively eliminates information from the forget set and preserves knowledge\nfrom the retain set. Lastly, we identify critical flaws in established\nevaluation metrics for machine unlearning, and introduce new evaluation tools\nthat more accurately reflect the fundamental goals of machine unlearning.\n","authors":["Seonguk Seo","Dongwan Kim","Bohyung Han"],"pdf_url":"https://arxiv.org/pdf/2407.17710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03767v2","updated":"2024-12-19T16:45:52Z","published":"2023-01-10T03:10:32Z","title":"Metric Compatible Training for Online Backfilling in Large-Scale\n  Retrieval","summary":"  Backfilling is the process of re-extracting all gallery embeddings from\nupgraded models in image retrieval systems. It inevitably requires a\nprohibitively large amount of computational cost and even entails the downtime\nof the service. Although backward-compatible learning sidesteps this challenge\nby tackling query-side representations, this leads to suboptimal solutions in\nprinciple because gallery embeddings cannot benefit from model upgrades. We\naddress this dilemma by introducing an online backfilling algorithm, which\nenables us to achieve a progressive performance improvement during the\nbackfilling process while not sacrificing the final performance of new model\nafter the completion of backfilling. To this end, we first propose a simple\ndistance rank merge technique for online backfilling. Then, we incorporate a\nreverse transformation module for more effective and efficient merging, which\nis further enhanced by adopting a metric-compatible contrastive learning\napproach. These two components help to make the distances of old and new models\ncompatible, resulting in desirable merge results during backfilling with no\nextra computational overhead. Extensive experiments show the effectiveness of\nour framework on four standard benchmarks in various settings.\n","authors":["Seonguk Seo","Mustafa Gokhan Uzunbas","Bohyung Han","Sara Cao","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2301.03767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15032v1","updated":"2024-12-19T16:44:01Z","published":"2024-12-19T16:44:01Z","title":"DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT\n  Space","summary":"  This paper explores image modeling from the frequency space and introduces\nDCTdiff, an end-to-end diffusion generative paradigm that efficiently models\nimages in the discrete cosine transform (DCT) space. We investigate the design\nspace of DCTdiff and reveal the key design factors. Experiments on different\nframeworks (UViT, DiT), generation tasks, and various diffusion samplers\ndemonstrate that DCTdiff outperforms pixel-based diffusion models regarding\ngenerative quality and training efficiency. Remarkably, DCTdiff can seamlessly\nscale up to high-resolution generation without using the latent diffusion\nparadigm. Finally, we illustrate several intriguing properties of DCT image\nmodeling. For example, we provide a theoretical proof of why `image diffusion\ncan be seen as spectral autoregression', bridging the gap between diffusion and\nautoregressive models. The effectiveness of DCTdiff and the introduced\nproperties suggest a promising direction for image modeling in the frequency\nspace. The code is at \\url{https://github.com/forever208/DCTdiff}.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Haozhe Jia","Lanmiao Liu","Martin Beneš","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2412.15032v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2406.14742v2","updated":"2024-12-19T16:37:38Z","published":"2024-06-20T21:13:39Z","title":"Latent Variable Sequence Identification for Cognitive Models with Neural\n  Network Estimators","summary":"  Extracting time-varying latent variables from computational cognitive models\nis a key step in model-based neural analysis, which aims to understand the\nneural correlates of cognitive processes. However, existing methods only allow\nresearchers to infer latent variables that explain subjects' behavior in a\nrelatively small class of cognitive models. For example, a broad class of\nrelevant cognitive models with analytically intractable likelihood is currently\nout of reach from standard techniques, based on Maximum a Posteriori parameter\nestimation. Here, we present an approach that extends neural Bayes estimation\nto learn a direct mapping between experimental data and the targeted latent\nvariable space using recurrent neural networks and simulated datasets. We show\nthat our approach achieves competitive performance in inferring latent variable\nsequences in both tractable and intractable models. Furthermore, the approach\nis generalizable across different computational models and is adaptable for\nboth continuous and discrete latent spaces. We then demonstrate its\napplicability in real world datasets. Our work underscores that combining\nrecurrent neural networks and simulation-based inference to identify latent\nvariable sequences can enable researchers to access a wider class of cognitive\nmodels for model-based neural analyses, and thus test a broader set of\ntheories.\n","authors":["Ti-Fen Pan","Jing-Jing Li","Bill Thompson","Anne Collins"],"pdf_url":"https://arxiv.org/pdf/2406.14742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15023v1","updated":"2024-12-19T16:37:19Z","published":"2024-12-19T16:37:19Z","title":"Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and\n  Semantic Controls","summary":"  Sound designers and Foley artists usually sonorize a scene, such as from a\nmovie or video game, by manually annotating and sonorizing each action of\ninterest in the video. In our case, the intent is to leave full creative\ncontrol to sound designers with a tool that allows them to bypass the more\nrepetitive parts of their work, thus being able to focus on the creative\naspects of sound production. We achieve this presenting Stable-V2A, a two-stage\nmodel consisting of: an RMS-Mapper that estimates an envelope representative of\nthe audio characteristics associated with the input video; and Stable-Foley, a\ndiffusion model based on Stable Audio Open that generates audio semantically\nand temporally aligned with the target video. Temporal alignment is guaranteed\nby the use of the envelope as a ControlNet input, while semantic alignment is\nachieved through the use of sound representations chosen by the designer as\ncross-attention conditioning of the diffusion process. We train and test our\nmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. In\naddition, to test our model on a case study of interest, we introduce Walking\nThe Maps, a dataset of videos extracted from video games depicting animated\ncharacters walking in different locations. Samples and code available on our\ndemo page at https://ispamm.github.io/Stable-V2A.\n","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunità","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.15023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14512v3","updated":"2024-12-19T16:37:00Z","published":"2024-08-25T04:32:45Z","title":"LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with\n  LLM Token Embeddings","summary":"  Zero-shot graph machine learning, especially with graph neural networks\n(GNNs), has garnered significant interest due to the challenge of scarce\nlabeled data. While methods like self-supervised learning and graph prompt\nlearning have been extensively explored, they often rely on fine-tuning with\ntask-specific labels, limiting their effectiveness in zero-shot scenarios.\nInspired by the zero-shot capabilities of instruction-fine-tuned large language\nmodels (LLMs), we introduce a novel framework named Token Embedding-Aligned\nGraph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and\ncross-task zero-shot learners for graph machine learning. Concretely, we\npretrain a GNN, aligning its representations with token embeddings of an LLM.\nWe then train a linear projector that transforms the GNN's representations into\na fixed number of graph token embeddings without tuning the LLM. A unified\ninstruction is designed for various graph tasks at different levels, such as\nnode classification (node-level) and link prediction (edge-level). These design\nchoices collectively enhance our method's effectiveness in zero-shot learning,\nsetting it apart from existing methods. Experiments show that our graph token\nembeddings help the LLM predictor achieve state-of-the-art performance on\nunseen datasets and tasks compared to other methods using LLMs as predictors.\n","authors":["Duo Wang","Yuan Zuo","Fengzhi Li","Junjie Wu"],"pdf_url":"https://arxiv.org/pdf/2408.14512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15557v2","updated":"2024-12-19T16:32:43Z","published":"2024-05-24T13:44:30Z","title":"Learning from Linear Algebra: A Graph Neural Network Approach to\n  Preconditioner Design for Conjugate Gradient Solvers","summary":"  Large linear systems are ubiquitous in modern computational science and\nengineering. The main recipe for solving them is the use of Krylov subspace\niterative methods with well-designed preconditioners. Deep learning models can\nbe used as nonlinear preconditioners during the iteration of linear solvers\nsuch as the conjugate gradient (CG) method. Neural network models require an\nenormous number of parameters to approximate well in this setup. Another\napproach is to take advantage of small graph neural networks (GNNs) to\nconstruct preconditioners with predefined sparsity patterns. Recently, GNNs\nhave been shown to be a promising tool for designing preconditioners to reduce\nthe overall computational cost of iterative methods by constructing them more\nefficiently than with classical linear algebra techniques. However,\npreconditioners designed with these approaches cannot outperform those designed\nwith classical methods in terms of the number of iterations in CG. In our work,\nwe recall well-established preconditioners from linear algebra and use them as\na starting point for training the GNN to obtain preconditioners that reduce the\ncondition number of the system more significantly. Numerical experiments show\nthat our approach outperforms both classical and neural network-based methods\nfor an important class of parametric partial differential equations. We also\nprovide a heuristic justification for the loss function used and show that\npreconditioners obtained by learning with this loss function reduce the\ncondition number in a more desirable way for CG.\n","authors":["Vladislav Trifonov","Alexander Rudikov","Oleg Iliev","Yuri M. Laevsky","Ivan Oseledets","Ekaterina Muravleva"],"pdf_url":"https://arxiv.org/pdf/2405.15557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15010v1","updated":"2024-12-19T16:22:37Z","published":"2024-12-19T16:22:37Z","title":"Robust Federated Learning in the Face of Covariate Shift: A Magnitude\n  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation","summary":"  The development of highly sophisticated neural networks has allowed for fast\nprogress in every field of computer vision, however, applications where\nannotated data is prohibited due to privacy or security concerns remain\nchallenging. Federated Learning (FL) offers a promising framework for\nindividuals aiming to collaboratively develop a shared model while preserving\ndata privacy. Nevertheless, our findings reveal that variations in data\ndistribution among clients can profoundly affect FL methodologies, primarily\ndue to instabilities in the aggregation process. We also propose a novel FL\nframework to mitigate the adverse effects of covariate shifts among federated\nclients by combining individual parameter pruning and regularization techniques\nto improve the robustness of individual clients' models to aggregate. Each\nclient's model is optimized through magnitude-based pruning and the addition of\ndropout and noise injection layers to build more resilient decision pathways in\nthe networks and improve the robustness of the model's parameter aggregation\nstep. The proposed framework is capable of extracting robust representations\neven in the presence of very large covariate shifts among client data\ndistributions and in the federation of a small number of clients. Empirical\nfindings substantiate the effectiveness of our proposed methodology across\ncommon benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.\nFurthermore, we introduce the CelebA-Gender dataset, specifically designed to\nevaluate performance on a more realistic domain. The proposed method is capable\nof extracting robust representations even in the presence of both high and low\ncovariate shifts among client data distributions.\n","authors":["Ozgu Goksu","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2412.15010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15005v1","updated":"2024-12-19T16:20:42Z","published":"2024-12-19T16:20:42Z","title":"DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start\n  Cross-Domain Recommendation","summary":"  Recommender systems are widely used in various real-world applications, but\nthey often encounter the persistent challenge of the user cold-start problem.\nCross-domain recommendation (CDR), which leverages user interactions from one\ndomain to improve prediction performance in another, has emerged as a promising\nsolution. However, users with similar preferences in the source domain may\nexhibit different interests in the target domain. Therefore, directly\ntransferring embeddings may introduce irrelevant source-domain collaborative\ninformation. In this paper, we propose a novel graph-based disentangled\ncontrastive learning framework to capture fine-grained user intent and filter\nout irrelevant collaborative information, thereby avoiding negative transfer.\nSpecifically, for each domain, we use a multi-channel graph encoder to capture\ndiverse user intents. We then construct the affinity graph in the embedding\nspace and perform multi-step random walks to capture high-order user similarity\nrelationships. Treating one domain as the target, we propose a disentangled\nintent-wise contrastive learning approach, guided by user similarity, to refine\nthe bridging of user intents across domains. Extensive experiments on four\nbenchmark CDR datasets demonstrate that DisCo consistently outperforms existing\nstate-of-the-art baselines, thereby validating the effectiveness of both DisCo\nand its components.\n","authors":["Hourun Li","Yifan Wang","Zhiping Xiao","Jia Yang","Changling Zhou","Ming Zhang","Wei Ju"],"pdf_url":"https://arxiv.org/pdf/2412.15005v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12012v5","updated":"2024-12-19T16:15:27Z","published":"2024-01-22T14:59:11Z","title":"TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for\n  Lazy Clients","summary":"  Federated learning is a distributed collaborative machine learning paradigm\nthat has gained strong momentum in recent years. In federated learning, a\ncentral server periodically coordinates models with clients and aggregates the\nmodels trained locally by clients without necessitating access to local data.\nDespite its potential, the implementation of federated learning continues to\nencounter several challenges, predominantly the slow convergence that is\nlargely due to data heterogeneity. The slow convergence becomes particularly\nproblematic in cross-device federated learning scenarios where clients may be\nstrongly limited by computing power and storage space, and hence counteracting\nmethods that induce additional computation or memory cost on the client side\nsuch as auxiliary objective terms and larger training iterations can be\nimpractical. In this paper, we propose a novel federated aggregation strategy,\nTurboSVM-FL, that poses no additional computation burden on the client side and\ncan significantly accelerate convergence for federated classification task,\nespecially when clients are \"lazy\" and train their models solely for few epochs\nfor next global aggregation. TurboSVM-FL extensively utilizes support vector\nmachine to conduct selective aggregation and max-margin spread-out\nregularization on class embeddings. We evaluate TurboSVM-FL on multiple\ndatasets including FEMNIST, CelebA, and Shakespeare using user-independent\nvalidation with non-iid data distribution. Our results show that TurboSVM-FL\ncan significantly outperform existing popular algorithms on convergence rate\nand reduce communication rounds while delivering better test metrics including\naccuracy, F1 score, and MCC.\n","authors":["Mengdi Wang","Anna Bodonhelyi","Efe Bozkir","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2401.12012v5.pdf","comment":"Proceedings of the AAAI Conference on Artificial Intelligence 2024\n  (AAAI'24)"},{"id":"http://arxiv.org/abs/2405.08044v2","updated":"2024-12-19T16:08:31Z","published":"2024-05-13T13:55:34Z","title":"Mitigating federated learning contribution allocation instability\n  through randomized aggregation","summary":"  Federated learning (FL) is a collaborative and privacy-preserving Machine\nLearning paradigm, allowing the development of robust models without the need\nto centralise sensitive data. A critical challenge in FL lies in fairly and\naccurately allocating contributions from diverse participants. Inaccurate\nallocation can undermine trust, lead to unfair compensation, and thus\nparticipants may lack the incentive to join or actively contribute to the\nfederation.\n  Various remuneration strategies have been proposed to date, including\nauction-based approaches and Shapley-value based methods, the latter offering a\nmeans to quantify the contribution of each participant. However, little to no\nwork has studied the stability of these contribution evaluation methods.\n  In this paper, we focus on calculating contributions using gradient-based\nmodel reconstruction techniques with Shapley values. We first show that\nbaseline Shapley values do not accurately reflect clients' contributions,\nleading to unstable reward allocations amongst participants in a cross-silo\nfederation. We then introduce \\textsc{FedRandom}, a new method that mitigates\nthese shortcomings with additional data samplings, and show its efficacy at\nincreasing the stability of contribution evaluation in federated learning.\n","authors":["Arno Geimer","Beltran Fiz","Radu State"],"pdf_url":"https://arxiv.org/pdf/2405.08044v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08606v2","updated":"2024-12-19T16:05:31Z","published":"2024-02-13T17:12:01Z","title":"Arbitrary Polynomial Separations in Trainable Quantum Machine Learning","summary":"  Recent theoretical results in quantum machine learning have demonstrated a\ngeneral trade-off between the expressive power of quantum neural networks\n(QNNs) and their trainability; as a corollary of these results, practical\nexponential separations in expressive power over classical machine learning\nmodels are believed to be infeasible as such QNNs take a time to train that is\nexponential in the model size. We here circumvent these negative results by\nconstructing a hierarchy of efficiently trainable QNNs that exhibit\nunconditionally provable, polynomial memory separations of arbitrary constant\ndegree over classical neural networks -- including state-of-the-art models,\nsuch as Transformers -- in performing a classical sequence modeling task. This\nconstruction is also computationally efficient, as each unit cell of the\nintroduced class of QNNs only has constant gate complexity. We show that\ncontextuality -- informally, a quantitative notion of semantic ambiguity -- is\nthe source of the expressivity separation, suggesting that other learning tasks\nwith this property may be a natural setting for the use of quantum learning\nalgorithms.\n","authors":["Eric R. Anschuetz","Xun Gao"],"pdf_url":"https://arxiv.org/pdf/2402.08606v2.pdf","comment":"24 pages, 3 figures, strengthened and simplified results and\n  presentation"},{"id":"http://arxiv.org/abs/2412.14988v1","updated":"2024-12-19T16:00:10Z","published":"2024-12-19T16:00:10Z","title":"Stitch Contrast and Segment_Learning a Human Action Segmentation Model\n  Using Trimmed Skeleton Videos","summary":"  Existing skeleton-based human action classification models rely on\nwell-trimmed action-specific skeleton videos for both training and testing,\nprecluding their scalability to real-world applications where untrimmed videos\nexhibiting concatenated actions are predominant. To overcome this limitation,\nrecently introduced skeleton action segmentation models involve un-trimmed\nskeleton videos into end-to-end training. The model is optimized to provide\nframe-wise predictions for any length of testing videos, simultaneously\nrealizing action localization and classification. Yet, achieving such an\nimprovement im-poses frame-wise annotated skeleton videos, which remains\ntime-consuming in practice. This paper features a novel framework for\nskeleton-based action segmentation trained on short trimmed skeleton videos,\nbut that can run on longer un-trimmed videos. The approach is implemented in\nthree steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral\nskeleton stitching scheme that treats trimmed skeleton videos as elementary\nhuman motions that compose a semantic space and can be sampled to generate\nmulti-action stitched se-quences. Contrast learns contrastive representations\nfrom stitched sequences with a novel discrimination pretext task that enables a\nskeleton encoder to learn meaningful action-temporal contexts to improve action\nsegmentation. Finally, Segment relates the proposed method to action\nsegmentation by learning a segmentation layer while handling particular da-ta\navailability. Experiments involve a trimmed source dataset and an untrimmed\ntarget dataset in an adaptation formulation for real-world skeleton-based human\naction segmentation to evaluate the effectiveness of the proposed method.\n","authors":["Haitao Tian","Pierre Payeur"],"pdf_url":"https://arxiv.org/pdf/2412.14988v1.pdf","comment":"Accepted as AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08941v3","updated":"2024-12-19T15:59:19Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2404.15379v3","updated":"2024-12-19T15:54:17Z","published":"2024-04-23T07:16:13Z","title":"Clustering of timed sequences -- Application to the analysis of care\n  pathways","summary":"  Improving the future of healthcare starts by better understanding the current\nactual practices in hospital settings. This motivates the objective of\ndiscovering typical care pathways from patient data. Revealing typical care\npathways can be achieved through clustering. The difficulty in clustering care\npathways, represented by sequences of timestamped events, lies in defining a\nsemantically appropriate metric and clustering algorithms. In this article, we\nadapt two methods developed for time series to the clustering of timed\nsequences: the drop-DTW metric and the DBA approach for the construction of\naveraged time sequences. These methods are then applied in clustering\nalgorithms to propose original and sound clustering algorithms for timed\nsequences. This approach is experimented with and evaluated on synthetic and\nreal-world data.\n","authors":["Thomas Guyet","Pierre Pinson","Enoal Gesny"],"pdf_url":"https://arxiv.org/pdf/2404.15379v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01420v2","updated":"2024-12-19T15:51:33Z","published":"2024-12-02T12:00:27Z","title":"Task Adaptation of Reinforcement Learning-based NAS Agents through\n  Transfer Learning","summary":"  Recently, a novel paradigm has been proposed for reinforcement learning-based\nNAS agents, that revolves around the incremental improvement of a given\narchitecture. We assess the abilities of such reinforcement learning agents to\ntransfer between different tasks. We perform our evaluation using the\nTrans-NASBench-101 benchmark, and consider the efficacy of the transferred\nagents, as well as how quickly they can be trained. We find that pretraining an\nagent on one task benefits the performance of the agent in another task in all\nbut 1 task when considering final performance. We also show that the training\nprocedure for an agent can be shortened significantly by pretraining it on\nanother task. Our results indicate that these effects occur regardless of the\nsource or target task, although they are more pronounced for some tasks than\nfor others. Our results show that transfer learning can be an effective tool in\nmitigating the computational cost of the initial training procedure for\nreinforcement learning-based NAS agents.\n","authors":["Amber Cassimon","Siegfried Mercelis","Kevin Mets"],"pdf_url":"https://arxiv.org/pdf/2412.01420v2.pdf","comment":"15 Pages, 13 Figures, Corrected data in Figure 5"},{"id":"http://arxiv.org/abs/2412.14964v1","updated":"2024-12-19T15:44:01Z","published":"2024-12-19T15:44:01Z","title":"Knowledge Injection via Prompt Distillation","summary":"  In many practical applications, large language models (LLMs) need to\nincorporate new knowledge not present in their pre-training data. The primary\nmethods for this are fine-tuning and retrieval-augmented generation (RAG).\nAlthough RAG has emerged as the industry standard for knowledge injection,\nfine-tuning has not yet achieved comparable success. In this paper, we propose\na new fine-tuning technique for learning new knowledge and show that it can\nreach the performance of RAG. The proposed method is based on the\nself-distillation approach, which we call prompt distillation. First, we\ngenerate question-answer pairs about the new knowledge. Then, we fine-tune a\nstudent model on the question-answer pairs to imitate the output distributions\nof a teacher model, which additionally receives the new knowledge in its\nprompt. The student model is identical to the teacher, except it is equipped\nwith a LoRA adapter. This training procedure facilitates distilling the new\nknowledge from the teacher's prompt into the student's weights.\n","authors":["Kalle Kujanpää","Harri Valpola","Alexander Ilin"],"pdf_url":"https://arxiv.org/pdf/2412.14964v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.14963v1","updated":"2024-12-19T15:43:05Z","published":"2024-12-19T15:43:05Z","title":"IDOL: Instant Photorealistic 3D Human Creation from a Single Image","summary":"  Creating a high-fidelity, animatable 3D full-body avatar from a single image\nis a challenging task due to the diverse appearance and poses of humans and the\nlimited availability of high-quality training data. To achieve fast and\nhigh-quality human reconstruction, this work rethinks the task from the\nperspectives of dataset, model, and representation. First, we introduce a\nlarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K\ndiverse, photorealistic sets of human images. Each set contains 24-view frames\nin specific human poses, generated using a pose-controllable\nimage-to-multi-view model. Next, leveraging the diversity in views, poses, and\nappearances within HuGe100K, we develop a scalable feed-forward transformer\nmodel to predict a 3D human Gaussian representation in a uniform space from a\ngiven human image. This model is trained to disentangle human pose, body shape,\nclothing geometry, and texture. The estimated Gaussians can be animated without\npost-processing. We conduct comprehensive experiments to validate the\neffectiveness of the proposed dataset and method. Our model demonstrates the\nability to efficiently reconstruct photorealistic humans at 1K resolution from\na single input image using a single GPU instantly. Additionally, it seamlessly\nsupports various applications, as well as shape and texture editing tasks.\n","authors":["Yiyu Zhuang","Jiaxi Lv","Hao Wen","Qing Shuai","Ailing Zeng","Hao Zhu","Shifeng Chen","Yujiu Yang","Xun Cao","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14963v1.pdf","comment":"21 pages, 15 figures, includes main content, supplementary materials,\n  and references"},{"id":"http://arxiv.org/abs/2412.03795v2","updated":"2024-12-19T15:43:00Z","published":"2024-12-05T01:25:34Z","title":"Samudra: An AI Global Ocean Emulator for Climate","summary":"  AI emulators for forecasting have emerged as powerful tools that can\noutperform conventional numerical predictions. The next frontier is to build\nemulators for long climate simulations with skill across a range of\nspatiotemporal scales, a particularly important goal for the ocean. Our work\nbuilds a skillful global emulator of the ocean component of a state-of-the-art\nclimate model. We emulate key ocean variables, sea surface height, horizontal\nvelocities, temperature, and salinity, across their full depth. We use a\nmodified ConvNeXt UNet architecture trained on multidepth levels of ocean data.\nWe show that the ocean emulator - Samudra - which exhibits no drift relative to\nthe truth, can reproduce the depth structure of ocean variables and their\ninterannual variability. Samudra is stable for centuries and 150 times faster\nthan the original ocean model. Samudra struggles to capture the correct\nmagnitude of the forcing trends and simultaneously remains stable, requiring\nfurther work.\n","authors":["Surya Dheeshjith","Adam Subel","Alistair Adcroft","Julius Busecke","Carlos Fernandez-Granda","Shubham Gupta","Laure Zanna"],"pdf_url":"https://arxiv.org/pdf/2412.03795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00641v2","updated":"2024-12-19T15:42:00Z","published":"2024-08-01T15:30:43Z","title":"Enhancing Ethereum Fraud Detection via Generative and Contrastive\n  Self-supervision","summary":"  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be available in\nhttps://github.com/GISec-Team/Meta-IFD.\n","authors":["Chenxiang Jin","Jiajun Zhou","Chenxuan Xie","Shanqing Yu","Qi Xuan","Xiaoniu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00641v2.pdf","comment":"Accepted by IEEE Transactions on Information Forensics & Security"},{"id":"http://arxiv.org/abs/2412.14954v1","updated":"2024-12-19T15:36:30Z","published":"2024-12-19T15:36:30Z","title":"Corn Ear Detection and Orientation Estimation Using Deep Learning","summary":"  Monitoring growth behavior of maize plants such as the development of ears\ncan give key insights into the plant's health and development. Traditionally,\nthe measurement of the angle of ears is performed manually, which can be\ntime-consuming and prone to human error. To address these challenges, this\npaper presents a computer vision-based system for detecting and tracking ears\nof corn in an image sequence. The proposed system could accurately detect,\ntrack, and predict the ear's orientation, which can be useful in monitoring\ntheir growth behavior. This can significantly save time compared to manual\nmeasurement and enables additional areas of ear orientation research and\npotential increase in efficiencies for maize production. Using an object\ndetector with keypoint detection, the algorithm proposed could detect 90\npercent of all ears. The cardinal estimation had a mean absolute error (MAE) of\n18 degrees, compared to a mean 15 degree difference between two people\nmeasuring by hand. These results demonstrate the feasibility of using computer\nvision techniques for monitoring maize growth and can lead to further research\nin this area.\n","authors":["Nathan Sprague","John Evans","Michael Mardikes"],"pdf_url":"https://arxiv.org/pdf/2412.14954v1.pdf","comment":"22 pages;15 figures"},{"id":"http://arxiv.org/abs/2411.10958v2","updated":"2024-12-19T15:26:20Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrixes $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$\nto enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS)\nof SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on\nRTX4090, respectively. Comprehensive experiments confirm that our approach\nincurs negligible end-to-end metrics loss across diverse models, including\nthose for large language processing, image generation, and video generation.\nThe codes are available at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.10839v2","updated":"2024-12-19T15:25:41Z","published":"2024-08-20T13:34:17Z","title":"Benchmarking Large Language Models for Math Reasoning Tasks","summary":"  The use of Large Language Models (LLMs) in mathematical reasoning has become\na cornerstone of related research, demonstrating the intelligence of these\nmodels and enabling potential practical applications through their advanced\nperformance, such as in educational settings. Despite the variety of datasets\nand in-context learning algorithms designed to improve the ability of LLMs to\nautomate mathematical problem solving, the lack of comprehensive benchmarking\nacross different datasets makes it complicated to select an appropriate model\nfor specific tasks. In this project, we present a benchmark that fairly\ncompares seven state-of-the-art in-context learning algorithms for mathematical\nproblem solving across five widely used mathematical datasets on four powerful\nfoundation models. Furthermore, we explore the trade-off between efficiency and\nperformance, highlighting the practical applications of LLMs for mathematical\nreasoning. Our results indicate that larger foundation models like GPT-4o and\nLLaMA 3-70B can solve mathematical reasoning independently from the concrete\nprompting strategy, while for smaller models the in-context learning approach\nsignificantly influences the performance. Moreover, the optimal prompt depends\non the chosen foundation model. We open-source our benchmark code to support\nthe integration of additional models in future research.\n","authors":["Kathrin Seßler","Yao Rong","Emek Gözlüklü","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2408.10839v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2302.11947v2","updated":"2024-12-19T15:13:46Z","published":"2023-02-23T11:44:43Z","title":"Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight\n  Convolutional Neural Networks","summary":"  The health and safety hazards posed by worn crane lifting ropes mandate\nperiodic inspection for damage. This task is time-consuming, prone to human\nerror, halts operation, and may result in the premature disposal of ropes.\nTherefore, we propose using efficient deep learning and computer vision methods\nto automate the process of detecting damaged ropes. Specifically, we present a\nvision-based system for detecting damage in synthetic fiber rope images using\nlightweight convolutional neural networks. We develop a camera-based apparatus\nto photograph the lifting rope's surface, while in operation, and capture the\nprogressive wear-and-tear as well as the more significant degradation in the\nrope's health state. Experts from Konecranes annotate the collected images in\naccordance with the rope's condition; normal or damaged. Then, we pre-process\nthe images, systematically design a deep learning model, evaluate its detection\nand prediction performance, analyze its computational complexity, and compare\nit with various other models. Experimental results show the proposed model\noutperforms other similar techniques with 96.5% accuracy, 94.8% precision,\n98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the\nmodel's real-time operation, low memory footprint, robustness to various\nenvironmental and operational conditions, and adequacy for deployment in\nindustrial applications such as lifting, mooring, towing, climbing, and\nsailing.\n","authors":["Tuomas Jalonen","Mohammad Al-Sa'd","Roope Mellanen","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2302.11947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09423v3","updated":"2024-12-19T15:10:18Z","published":"2023-07-18T16:43:03Z","title":"Scaling Laws for Imitation Learning in Single-Agent Games","summary":"  Imitation Learning (IL) is one of the most widely used methods in machine\nlearning. Yet, many works find it is often unable to fully recover the\nunderlying expert behavior, even in constrained environments like single-agent\ngames. However, none of these works deeply investigate the role of scaling up\nthe model and data size. Inspired by recent work in Natural Language Processing\n(NLP) where \"scaling up\" has resulted in increasingly more capable LLMs, we\ninvestigate whether carefully scaling up model and data size can bring similar\nimprovements in the imitation learning setting for single-agent games. We first\ndemonstrate our findings on a variety of Atari games, and thereafter focus on\nthe extremely challenging game of NetHack. In all games, we find that IL loss\nand mean return scale smoothly with the compute budget (FLOPs) and are strongly\ncorrelated, resulting in power laws for training compute-optimal IL agents.\nFinally, we forecast and train several NetHack agents with IL and find they\noutperform prior state-of-the-art by 1.5x in all settings. Our work both\ndemonstrates the scaling behavior of imitation learning in a variety of\nsingle-agent games, as well as the viability of scaling up current approaches\nfor increasingly capable agents in NetHack, a game that remains elusively hard\nfor current AI systems.\n","authors":["Jens Tuyls","Dhruv Madeka","Kari Torkkola","Dean Foster","Karthik Narasimhan","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2307.09423v3.pdf","comment":"Accepted at TMLR 2024"},{"id":"http://arxiv.org/abs/2412.14916v1","updated":"2024-12-19T14:50:10Z","published":"2024-12-19T14:50:10Z","title":"From Point to probabilistic gradient boosting for claim frequency and\n  severity prediction","summary":"  Gradient boosting for decision tree algorithms are increasingly used in\nactuarial applications as they show superior predictive performance over\ntraditional generalized linear models. Many improvements and sophistications to\nthe first gradient boosting machine algorithm exist. We present in a unified\nnotation, and contrast, all the existing point and probabilistic gradient\nboosting for decision tree algorithms: GBM, XGBoost, DART, LightGBM, CatBoost,\nEGBM, PGBM, XGBoostLSS, cyclic GBM, and NGBoost. In this comprehensive\nnumerical study, we compare their performance on five publicly available\ndatasets for claim frequency and severity, of various size and comprising\ndifferent number of (high cardinality) categorical variables. We explain how\nvarying exposure-to-risk can be handled with boosting in frequency models. We\ncompare the algorithms on the basis of computational efficiency, predictive\nperformance, and model adequacy. LightGBM and XGBoostLSS win in terms of\ncomputational efficiency. The fully interpretable EGBM achieves competitive\npredictive performance compared to the black box algorithms considered. We find\nthat there is no trade-off between model adequacy and predictive accuracy: both\nare achievable simultaneously.\n","authors":["Dominik Chevalier","Marie-Pier Côté"],"pdf_url":"https://arxiv.org/pdf/2412.14916v1.pdf","comment":"26 pages, 4 figures, 26 tables, 7 algorithms"},{"id":"http://arxiv.org/abs/2311.18512v2","updated":"2024-12-19T14:46:05Z","published":"2023-11-30T12:40:23Z","title":"Union-over-Intersections: Object Detection beyond Winner-Takes-All","summary":"  This paper revisits the problem of predicting box locations in object\ndetection architectures. Typically, each box proposal or box query aims to\ndirectly maximize the intersection-over-union score with the ground truth,\nfollowed by a winner-takes-all non-maximum suppression where only the highest\nscoring box in each region is retained. We observe that both steps are\nsub-optimal: the first involves regressing proposals to the entire ground\ntruth, which is a difficult task even with large receptive fields, and the\nsecond neglects valuable information from boxes other than the top candidate.\nInstead of regressing proposals to the whole ground truth, we propose a simpler\napproach: regress only to the area of intersection between the proposal and the\nground truth. This avoids the need for proposals to extrapolate beyond their\nvisual scope, improving localization accuracy. Rather than adopting a\nwinner-takes-all strategy, we take the union over the regressed intersections\nof all boxes in a region to generate the final box outputs. Our plug-and-play\nmethod integrates seamlessly into proposal-based, grid-based, and query-based\ndetection architectures with minimal modifications, consistently improving\nobject localization and instance segmentation. We demonstrate its broad\napplicability and versatility across various detection and segmentation tasks.\n","authors":["Aritra Bhowmik","Pascal Mettes","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2311.18512v2.pdf","comment":"17 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2412.14075v2","updated":"2024-12-19T14:41:03Z","published":"2024-12-18T17:19:55Z","title":"Online MDP with Transition Prototypes: A Robust Adaptive Approach","summary":"  In this work, we consider an online robust Markov Decision Process (MDP)\nwhere we have the information of finitely many prototypes of the underlying\ntransition kernel. We consider an adaptively updated ambiguity set of the\nprototypes and propose an algorithm that efficiently identifies the true\nunderlying transition kernel while guaranteeing the performance of the\ncorresponding robust policy. To be more specific, we provide a sublinear regret\nof the subsequent optimal robust policy. We also provide an early stopping\nmechanism and a worst-case performance bound of the value function. In\nnumerical experiments, we demonstrate that our method outperforms existing\napproaches, particularly in the early stage with limited data. This work\ncontributes to robust MDPs by considering possible prior information about the\nunderlying transition probability and online learning, offering both\ntheoretical insights and practical algorithms for improved decision-making\nunder uncertainty.\n","authors":["Shuo Sun","Meng Qi","Zuo-Jun Max Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11544v4","updated":"2024-12-19T14:33:00Z","published":"2024-06-17T13:42:28Z","title":"Do Parameters Reveal More than Loss for Membership Inference?","summary":"  Membership inference attacks are used as a key tool for disclosure auditing.\nThey aim to infer whether an individual record was used to train a model. While\nsuch evaluations are useful to demonstrate risk, they are computationally\nexpensive and often make strong assumptions about potential adversaries' access\nto models and training environments, and thus do not provide tight bounds on\nleakage from potential attacks. We show how prior claims around black-box\naccess being sufficient for optimal membership inference do not hold for\nstochastic gradient descent, and that optimal membership inference indeed\nrequires white-box access. Our theoretical results lead to a new white-box\ninference attack, IHA (Inverse Hessian Attack), that explicitly uses model\nparameters by taking advantage of computing inverse-Hessian vector products.\nOur results show that both auditors and adversaries may be able to benefit from\naccess to model parameters, and we advocate for further research into white-box\nmethods for membership inference.\n","authors":["Anshuman Suri","Xiao Zhang","David Evans"],"pdf_url":"https://arxiv.org/pdf/2406.11544v4.pdf","comment":"Accepted to Transactions on Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2412.14897v1","updated":"2024-12-19T14:28:00Z","published":"2024-12-19T14:28:00Z","title":"Diffusion priors for Bayesian 3D reconstruction from incomplete\n  measurements","summary":"  Many inverse problems are ill-posed and need to be complemented by prior\ninformation that restricts the class of admissible models. Bayesian approaches\nencode this information as prior distributions that impose generic properties\non the model such as sparsity, non-negativity or smoothness. However, in case\nof complex structured models such as images, graphs or three-dimensional (3D)\nobjects,generic prior distributions tend to favor models that differ largely\nfrom those observed in the real world. Here we explore the use of diffusion\nmodels as priors that are combined with experimental data within a Bayesian\nframework. We use 3D point clouds to represent 3D objects such as household\nitems or biomolecular complexes formed from proteins and nucleic acids. We\ntrain diffusion models that generate coarse-grained 3D structures at a medium\nresolution and integrate these with incomplete and noisy experimental data. To\ndemonstrate the power of our approach, we focus on the reconstruction of\nbiomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which\nis an important inverse problem in structural biology. We find that posterior\nsampling with diffusion model priors allows for 3D reconstruction from very\nsparse, low-resolution and partial observations.\n","authors":["Julian L. Möbius","Michael Habeck"],"pdf_url":"https://arxiv.org/pdf/2412.14897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.01519v3","updated":"2024-12-19T14:26:22Z","published":"2024-09-03T01:26:21Z","title":"Hybridization of Persistent Homology with Neural Networks for\n  Time-Series Prediction: A Case Study in Wave Height","summary":"  Time-series prediction is an active area of research across various fields,\noften challenged by the fluctuating influence of short-term and long-term\nfactors. In this study, we introduce a feature engineering method that enhances\nthe predictive performance of neural network models. Specifically, we leverage\ncomputational topology techniques to derive valuable topological features from\ninput data, boosting the predictive accuracy of our models. Our focus is on\npredicting wave heights, utilizing models based on topological features within\nfeedforward neural networks (FNNs), recurrent neural networks (RNNs), long\nshort-term memory networks (LSTM), and RNNs with gated recurrent units (GRU).\nFor time-ahead predictions, the enhancements in $R^2$ score were significant\nfor FNNs, RNNs, LSTM, and GRU models. Additionally, these models also showed\nsignificant reductions in maximum errors and mean squared errors.\n","authors":["Zixin Lin","Nur Fariha Syaqina Zulkepli","Mohd Shareduwan Mohd Kasihmuddin","R. U. Gobithaasan"],"pdf_url":"https://arxiv.org/pdf/2409.01519v3.pdf","comment":"the paper contain errors"},{"id":"http://arxiv.org/abs/2405.14573v4","updated":"2024-12-19T14:19:02Z","published":"2024-05-23T13:48:54Z","title":"AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents","summary":"  Autonomous agents that execute human tasks by controlling computers can\nenhance human productivity and application accessibility. However, progress in\nthis field will be driven by realistic and reproducible benchmarks. We present\nAndroidWorld, a fully functional Android environment that provides reward\nsignals for 116 programmatic tasks across 20 real-world Android apps. Unlike\nexisting interactive environments, which provide a static test set,\nAndroidWorld dynamically constructs tasks that are parameterized and expressed\nin natural language in unlimited ways, thus enabling testing on a much larger\nand more realistic suite of tasks. To ensure reproducibility, each task\nincludes dedicated initialization, success-checking, and tear-down logic, which\nmodifies and inspects the device's system state. We experiment with baseline\nagents to test AndroidWorld and provide initial results on the benchmark. Our\nbest agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for\nfuture work. Furthermore, we adapt a popular desktop web agent to work on\nAndroid, which we find to be less effective on mobile, suggesting future\nresearch is needed to achieve universal, cross-platform agents. Finally, we\nalso conduct a robustness analysis, showing that task variations can\nsignificantly affect agent performance, demonstrating that without such\ntesting, agent performance metrics may not fully reflect practical challenges.\nAndroidWorld and the experiments in this paper are available at\ngithub.com/google-research/android_world.\n","authors":["Christopher Rawles","Sarah Clinckemaillie","Yifan Chang","Jonathan Waltz","Gabrielle Lau","Marybeth Fair","Alice Li","William Bishop","Wei Li","Folawiyo Campbell-Ajala","Daniel Toyama","Robert Berry","Divya Tyamagundlu","Timothy Lillicrap","Oriana Riva"],"pdf_url":"https://arxiv.org/pdf/2405.14573v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16684v2","updated":"2024-12-19T14:18:15Z","published":"2024-09-25T07:20:59Z","title":"Erase then Rectify: A Training-Free Parameter Editing Approach for\n  Cost-Effective Graph Unlearning","summary":"  Graph unlearning, which aims to eliminate the influence of specific nodes,\nedges, or attributes from a trained Graph Neural Network (GNN), is essential in\napplications where privacy, bias, or data obsolescence is a concern. However,\nexisting graph unlearning techniques often necessitate additional training on\nthe remaining data, leading to significant computational costs, particularly\nwith large-scale graphs. To address these challenges, we propose a two-stage\ntraining-free approach, Erase then Rectify (ETR), designed for efficient and\nscalable graph unlearning while preserving the model utility. Specifically, we\nfirst build a theoretical foundation showing that masking parameters critical\nfor unlearned samples enables effective unlearning. Building on this insight,\nthe Erase stage strategically edits model parameters to eliminate the impact of\nunlearned samples and their propagated influence on intercorrelated nodes. To\nfurther ensure the GNN's utility, the Rectify stage devises a gradient\napproximation method to estimate the model's gradient on the remaining dataset,\nwhich is then used to enhance model performance. Overall, ETR achieves graph\nunlearning without additional training or full training data access,\nsignificantly reducing computational overhead and preserving data privacy.\nExtensive experiments on seven public datasets demonstrate the consistent\nsuperiority of ETR in model utility, unlearning efficiency, and unlearning\neffectiveness, establishing it as a promising solution for real-world graph\nunlearning challenges.\n","authors":["Zhe-Rui Yang","Jindong Han","Chang-Dong Wang","Hao Liu"],"pdf_url":"https://arxiv.org/pdf/2409.16684v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14869v1","updated":"2024-12-19T14:06:44Z","published":"2024-12-19T14:06:44Z","title":"AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional\n  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature\n  Screening","summary":"  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood\nwithin the skull, which occurs due to the rupture of blood vessels in or around\nthe brain. If this condition is not diagnosed in a timely manner and\nappropriately treated, it can lead to serious complications such as decreased\nconsciousness, permanent neurological disabilities, or even death.The primary\naim of this study is to detect the occurrence or non-occurrence of ICH,\nfollowed by determining the type of subdural hemorrhage (SDH). These tasks are\nframed as two separate binary classification problems. By adding two layers to\nthe co-scale convolutional attention (CCA) classifier architecture, we\nintroduce a novel approach for ICH detection. In the first layer, after\nextracting features from different slices of computed tomography (CT) scan\nimages, we combine these features and select the 50 components that capture the\nhighest variance in the data, considering them as informative features. We then\nassess the discriminative power of these features using the bootstrap forest\nalgorithm, discarding those that lack sufficient discriminative ability between\ndifferent classes. This algorithm explicitly determines the contribution of\neach feature to the final prediction, assisting us in developing an explainable\nAI model. The features feed into a boosting neural network as a latent feature\nspace. In the second layer, we introduce a novel uncertainty-based fuzzy\nintegral operator to fuse information from different CT scan slices. This\noperator, by accounting for the dependencies between consecutive slices,\nsignificantly improves detection accuracy.\n","authors":["Mehdi Hosseini Chagahi","Md. Jalil Piran","Niloufar Delfan","Behzad Moshiri","Jaber Hatam Parikhan"],"pdf_url":"https://arxiv.org/pdf/2412.14869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14865v1","updated":"2024-12-19T14:00:03Z","published":"2024-12-19T14:00:03Z","title":"Hierarchical Subspaces of Policies for Continual Offline Reinforcement\n  Learning","summary":"  In dynamic domains such as autonomous robotics and video game simulations,\nagents must continuously adapt to new tasks while retaining previously acquired\nskills. This ongoing process, known as Continual Reinforcement Learning,\npresents significant challenges, including the risk of forgetting past\nknowledge and the need for scalable solutions as the number of tasks increases.\nTo address these issues, we introduce HIerarchical LOW-rank Subspaces of\nPolicies (HILOW), a novel framework designed for continual learning in offline\nnavigation settings. HILOW leverages hierarchical policy subspaces to enable\nflexible and efficient adaptation to new tasks while preserving existing\nknowledge. We demonstrate, through a careful experimental study, the\neffectiveness of our method in both classical MuJoCo maze environments and\ncomplex video game-like simulations, showcasing competitive performance and\nsatisfying adaptability according to classical continual learning metrics, in\nparticular regarding memory usage. Our work provides a promising framework for\nreal-world applications where continuous learning from pre-collected data is\nessential.\n","authors":["Anthony Kobanda","Rémy Portelas","Odalric-Ambrym Maillard","Ludovic Denoyer"],"pdf_url":"https://arxiv.org/pdf/2412.14865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14854v1","updated":"2024-12-19T13:48:49Z","published":"2024-12-19T13:48:49Z","title":"Surrogate-assisted multi-objective design of complex multibody systems","summary":"  The optimization of large-scale multibody systems is a numerically\nchallenging task, in particular when considering multiple conflicting criteria\nat the same time. In this situation, we need to approximate the Pareto set of\noptimal compromises, which is significantly more expensive than finding a\nsingle optimum in single-objective optimization. To prevent large costs, the\nusage of surrogate models, constructed from a small but informative number of\nexpensive model evaluations, is a very popular and widely studied approach. The\ncentral challenge then is to ensure a high quality (that is, near-optimality)\nof the solutions that were obtained using the surrogate model, which can be\nhard to guarantee with a single pre-computed surrogate. We present a\nback-and-forth approach between surrogate modeling and multi-objective\noptimization to improve the quality of the obtained solutions. Using the\nexample of an expensive-to-evaluate multibody system, we compare different\nstrategies regarding multi-objective optimization, sampling and also surrogate\nmodeling, to identify the most promising approach in terms of computational\nefficiency and solution quality.\n","authors":["Augustina C. Amakor","Manuel B. Berkemeier","Meike Wohlleben","Walter Sextro","Sebastian Peitz"],"pdf_url":"https://arxiv.org/pdf/2412.14854v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2412.01566"},{"id":"http://arxiv.org/abs/2410.10929v6","updated":"2024-12-19T13:39:55Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v6.pdf","comment":"In process to IEEE Intelligent Vehicle Symposium 2025"},{"id":"http://arxiv.org/abs/2408.11778v2","updated":"2024-12-19T13:34:56Z","published":"2024-08-21T17:08:05Z","title":"Sum of Squares Circuits","summary":"  Designing expressive generative models that support exact and efficient\ninference is a core question in probabilistic ML. Probabilistic circuits (PCs)\noffer a framework where this tractability-vs-expressiveness trade-off can be\nanalyzed theoretically. Recently, squared PCs encoding subtractive mixtures via\nnegative parameters have emerged as tractable models that can be exponentially\nmore expressive than monotonic PCs, i.e., PCs with positive parameters only. In\nthis paper, we provide a more precise theoretical characterization of the\nexpressiveness relationships among these models. First, we prove that squared\nPCs can be less expressive than monotonic ones. Second, we formalize a novel\nclass of PCs -- sum of squares PCs -- that can be exponentially more expressive\nthan both squared and monotonic PCs. Around sum of squares PCs, we build an\nexpressiveness hierarchy that allows us to precisely unify and separate\ndifferent tractable model classes such as Born Machines and PSD models, and\nother recently introduced tractable probabilistic models by using complex\nparameters. Finally, we empirically show the effectiveness of sum of squares\ncircuits in performing distribution estimation.\n","authors":["Lorenzo Loconte","Stefan Mengel","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2408.11778v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09116v3","updated":"2024-12-19T13:27:49Z","published":"2024-12-12T09:51:18Z","title":"How to Re-enable PDE Loss for Physical Systems Modeling Under Partial\n  Observation","summary":"  In science and engineering, machine learning techniques are increasingly\nsuccessful in physical systems modeling (predicting future states of physical\nsystems). Effectively integrating PDE loss as a constraint of system transition\ncan improve the model's prediction by overcoming generalization issues due to\ndata scarcity, especially when data acquisition is costly. However, in many\nreal-world scenarios, due to sensor limitations, the data we can obtain is\noften only partial observation, making the calculation of PDE loss seem to be\ninfeasible, as the PDE loss heavily relies on high-resolution states. We\ncarefully study this problem and propose a novel framework named Re-enable PDE\nLoss under Partial Observation (RPLPO). The key idea is that although enabling\nPDE loss to constrain system transition solely is infeasible, we can re-enable\nPDE loss by reconstructing the learnable high-resolution state and constraining\nsystem transition simultaneously. Specifically, RPLPO combines an encoding\nmodule for reconstructing learnable high-resolution states with a transition\nmodule for predicting future states. The two modules are jointly trained by\ndata and PDE loss. We conduct experiments in various physical systems to\ndemonstrate that RPLPO has significant improvement in generalization, even when\nobservation is sparse, irregular, noisy, and PDE is inaccurate.\n","authors":["Haodong Feng","Yue Wang","Dixia Fan"],"pdf_url":"https://arxiv.org/pdf/2412.09116v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2305.09565v2","updated":"2024-12-19T13:27:40Z","published":"2023-05-16T16:02:18Z","title":"Toward Falsifying Causal Graphs Using a Permutation-Based Test","summary":"  Understanding causal relationships among the variables of a system is\nparamount to explain and control its behavior. For many real-world systems,\nhowever, the true causal graph is not readily available and one must resort to\npredictions made by algorithms or domain experts. Therefore, metrics that\nquantitatively assess the goodness of a causal graph provide helpful checks\nbefore using it in downstream tasks. Existing metrics provide an\n$\\textit{absolute}$ number of inconsistencies between the graph and the\nobserved data, and without a baseline, practitioners are left to answer the\nhard question of how many such inconsistencies are acceptable or expected.\nHere, we propose a novel consistency metric by constructing a baseline through\nnode permutations. By comparing the number of inconsistencies with those on the\nbaseline, we derive an interpretable metric that captures whether the graph is\nsignificantly better than random. Evaluating on both simulated and real data\nsets from various domains, including biology and cloud monitoring, we\ndemonstrate that the true graph is not falsified by our metric, whereas the\nwrong graphs given by a hypothetical user are likely to be falsified.\n","authors":["Elias Eulig","Atalanti A. Mastakouri","Patrick Blöbaum","Michaela Hardt","Dominik Janzing"],"pdf_url":"https://arxiv.org/pdf/2305.09565v2.pdf","comment":"Camera-ready version for AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14834v1","updated":"2024-12-19T13:24:01Z","published":"2024-12-19T13:24:01Z","title":"Entropy Regularized Task Representation Learning for Offline\n  Meta-Reinforcement Learning","summary":"  Offline meta-reinforcement learning aims to equip agents with the ability to\nrapidly adapt to new tasks by training on data from a set of different tasks.\nContext-based approaches utilize a history of state-action-reward transitions\n-- referred to as the context -- to infer representations of the current task,\nand then condition the agent, i.e., the policy and value function, on the task\nrepresentations. Intuitively, the better the task representations capture the\nunderlying tasks, the better the agent can generalize to new tasks.\nUnfortunately, context-based approaches suffer from distribution mismatch, as\nthe context in the offline data does not match the context at test time,\nlimiting their ability to generalize to the test tasks. This leads to the task\nrepresentations overfitting to the offline training data. Intuitively, the task\nrepresentations should be independent of the behavior policy used to collect\nthe offline data. To address this issue, we approximately minimize the mutual\ninformation between the distribution over the task representations and behavior\npolicy by maximizing the entropy of behavior policy conditioned on the task\nrepresentations. We validate our approach in MuJoCo environments, showing that\ncompared to baselines, our task representations more faithfully represent the\nunderlying tasks, leading to outperforming prior methods in both\nin-distribution and out-of-distribution tasks.\n","authors":["Mohammadreza nakhaei","Aidan Scannell","Joni Pajarinen"],"pdf_url":"https://arxiv.org/pdf/2412.14834v1.pdf","comment":"7 Pages, Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2406.02765v5","updated":"2024-12-19T13:16:21Z","published":"2024-06-04T20:33:29Z","title":"Discovering Continuous-Time Memory-Based Symbolic Policies using Genetic\n  Programming","summary":"  Artificial intelligence techniques are increasingly being applied to solve\ncontrol problems, but often rely on black-box methods without transparent\noutput generation. To improve the interpretability and transparency in control\nsystems, models can be defined as white-box symbolic policies described by\nmathematical expressions. For better performance in partially observable and\nvolatile environments, the symbolic policies are extended with memory\nrepresented by continuous-time latent variables, governed by differential\nequations. Genetic programming is used for optimisation, resulting in\ninterpretable policies consisting of symbolic expressions. Our results show\nthat symbolic policies with memory compare with black-box policies on a variety\nof control tasks. Furthermore, the benefit of the memory in symbolic policies\nis demonstrated on experiments where memory-less policies fall short. Overall,\nwe present a method for evolving high-performing symbolic policies that offer\ninterpretability and transparency, which lacks in black-box models.\n","authors":["Sigur de Vries","Sander Keemink","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2406.02765v5.pdf","comment":"21 pages including references and appendix, 5 figures, 1 algorithm, 5\n  tables"},{"id":"http://arxiv.org/abs/2412.14814v1","updated":"2024-12-19T13:09:06Z","published":"2024-12-19T13:09:06Z","title":"Answer Set Networks: Casting Answer Set Programming into Deep Learning","summary":"  Although Answer Set Programming (ASP) allows constraining neural-symbolic\n(NeSy) systems, its employment is hindered by the prohibitive costs of\ncomputing stable models and the CPU-bound nature of state-of-the-art solvers.\nTo this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on\nGraph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep\nProbabilistic Logic Programming (DPPL). Specifically, we show how to translate\nASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded\nproblem by leveraging GPU's batching and parallelization capabilities. Our\nexperimental evaluations demonstrate that ASNs outperform state-of-the-art\nCPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following\ntwo contributions based on the strengths of ASNs. Namely, we are the first to\nshow the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs\nto guide the training with logic. Further, we show the \"constitutional\nnavigation\" of drones, i.e., encoding public aviation laws in an ASN for\nrouting Unmanned Aerial Vehicles in uncertain environments.\n","authors":["Arseny Skryagin","Daniel Ochs","Phillip Deibert","Simon Kohaut","Devendra Singh Dhami","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2412.14814v1.pdf","comment":"16 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.10341v2","updated":"2024-12-19T13:03:10Z","published":"2024-12-13T18:38:47Z","title":"Shape error prediction in 5-axis machining using graph neural networks","summary":"  This paper presents an innovative method for predicting shape errors in\n5-axis machining using graph neural networks. The graph structure is defined\nwith nodes representing workpiece surface points and edges denoting the\nneighboring relationships. The dataset encompasses data from a material removal\nsimulation, process data, and post-machining quality information. Experimental\nresults show that the presented approach can generalize the shape error\nprediction for the investigated workpiece geometry. Moreover, by modelling\nspatial and temporal connections within the workpiece, the approach handles a\nlow number of labels compared to non-graphical methods such as Support Vector\nMachines.\n","authors":["Julia Huuk","Abheek Dhingra","Eirini Ntoutsi","Berend Denkena"],"pdf_url":"https://arxiv.org/pdf/2412.10341v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14810v1","updated":"2024-12-19T13:00:03Z","published":"2024-12-19T13:00:03Z","title":"MARIA: a Multimodal Transformer Model for Incomplete Healthcare Data","summary":"  In healthcare, the integration of multimodal data is pivotal for developing\ncomprehensive diagnostic and predictive models. However, managing missing data\nremains a significant challenge in real-world applications. We introduce MARIA\n(Multimodal Attention Resilient to Incomplete datA), a novel transformer-based\ndeep learning model designed to address these challenges through an\nintermediate fusion strategy. Unlike conventional approaches that depend on\nimputation, MARIA utilizes a masked self-attention mechanism, which processes\nonly the available data without generating synthetic values. This approach\nenables it to effectively handle incomplete datasets, enhancing robustness and\nminimizing biases introduced by imputation methods. We evaluated MARIA against\n10 state-of-the-art machine learning and deep learning models across 8\ndiagnostic and prognostic tasks. The results demonstrate that MARIA outperforms\nexisting methods in terms of performance and resilience to varying levels of\ndata incompleteness, underscoring its potential for critical healthcare\napplications.\n","authors":["Camillo Maria Caruso","Paolo Soda","Valerio Guarrasi"],"pdf_url":"https://arxiv.org/pdf/2412.14810v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14802v1","updated":"2024-12-19T12:48:17Z","published":"2024-12-19T12:48:17Z","title":"Stack Trace Deduplication: Faster, More Accurately, and in More\n  Realistic Scenarios","summary":"  In large-scale software systems, there are often no fully-fledged bug reports\nwith human-written descriptions when an error occurs. In this case, developers\nrely on stack traces, i.e., series of function calls that led to the error.\nSince there can be tens and hundreds of thousands of them describing the same\nissue from different users, automatic deduplication into categories is\nnecessary to allow for processing. Recent works have proposed powerful deep\nlearning-based approaches for this, but they are evaluated and compared in\nisolation from real-life workflows, and it is not clear whether they will\nactually work well at scale.\n  To overcome this gap, this work presents three main contributions: a novel\nmodel, an industry-based dataset, and a multi-faceted evaluation. Our model\nconsists of two parts - (1) an embedding model with byte-pair encoding and\napproximate nearest neighbor search to quickly find the most relevant stack\ntraces to the incoming one, and (2) a reranker that re-ranks the most fitting\nstack traces, taking into account the repeated frames between them. To\ncomplement the existing datasets collected from open-source projects, we share\nwith the community SlowOps - a dataset of stack traces from IntelliJ-based\nproducts developed by JetBrains, which has an order of magnitude more stack\ntraces per category. Finally, we carry out an evaluation that strives to be\nrealistic: measuring not only the accuracy of categorization, but also the\noperation time and the ability to create new categories. The evaluation shows\nthat our model strikes a good balance - it outperforms other models on both\nopen-source datasets and SlowOps, while also being faster on time than most. We\nrelease all of our code and data, and hope that our work can pave the way to\nfurther practice-oriented research in the area.\n","authors":["Egor Shibaev","Denis Sushentsev","Yaroslav Golubev","Aleksandr Khvorov"],"pdf_url":"https://arxiv.org/pdf/2412.14802v1.pdf","comment":"Published at SANER'25. 11 pages, 2 figures"},{"id":"http://arxiv.org/abs/2412.14801v1","updated":"2024-12-19T12:47:21Z","published":"2024-12-19T12:47:21Z","title":"Extending TWIG: Zero-Shot Predictive Hyperparameter Selection for KGEs\n  based on Graph Structure","summary":"  Knowledge Graphs (KGs) have seen increasing use across various domains --\nfrom biomedicine and linguistics to general knowledge modelling. In order to\nfacilitate the analysis of knowledge graphs, Knowledge Graph Embeddings (KGEs)\nhave been developed to automatically analyse KGs and predict new facts based on\nthe information in a KG, a task called \"link prediction\". Many existing studies\nhave documented that the structure of a KG, KGE model components, and KGE\nhyperparameters can significantly change how well KGEs perform and what\nrelationships they are able to learn. Recently, the Topologically-Weighted\nIntelligence Generation (TWIG) model has been proposed as a solution to\nmodelling how each of these elements relate. In this work, we extend the\nprevious research on TWIG and evaluate its ability to simulate the output of\nthe KGE model ComplEx in the cross-KG setting. Our results are twofold. First,\nTWIG is able to summarise KGE performance on a wide range of hyperparameter\nsettings and KGs being learned, suggesting that it represents a general\nknowledge of how to predict KGE performance from KG structure. Second, we show\nthat TWIG can successfully predict hyperparameter performance on unseen KGs in\nthe zero-shot setting. This second observation leads us to propose that, with\nadditional research, optimal hyperparameter selection for KGE models could be\ndetermined in a pre-hoc manner using TWIG-like methods, rather than by using a\nfull hyperparameter search.\n","authors":["Jeffrey Sardina","John D. Kelleher","Declan O'Sullivan"],"pdf_url":"https://arxiv.org/pdf/2412.14801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11448v3","updated":"2024-12-19T12:46:27Z","published":"2024-12-16T05:02:50Z","title":"TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated\n  Learning","summary":"  Due to the sensitivity of data, Federated Learning (FL) is employed to enable\ndistributed machine learning while safeguarding data privacy and accommodating\nthe requirements of various devices. However, in the context of\nsemi-decentralized FL, clients' communication and training states are dynamic.\nThis variability arises from local training fluctuations, heterogeneous data\ndistributions, and intermittent client participation. Most existing studies\nprimarily focus on stable client states, neglecting the dynamic challenges\ninherent in real-world scenarios. To tackle this issue, we propose a\nTRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client\nstates and contributions, enhancing model training efficiency through selective\nclient participation. We focus on a semi-decentralized FL framework where edge\nservers and clients train a shared global model using unreliable intra-cluster\nmodel aggregation and inter-cluster model consensus. First, we propose an\nadaptive hidden semi-Markov model to estimate clients' communication states and\ncontributions. Next, we address a client-server association optimization\nproblem to minimize global training loss. Using convergence analysis, we\npropose a greedy client scheduling algorithm. Finally, our experiments\nconducted on real-world datasets demonstrate that TRAIL outperforms\nstate-of-the-art baselines, achieving an improvement of 8.7% in test accuracy\nand a reduction of 15.3% in training loss.\n","authors":["Gangqiang Hu","Jianfeng Lu","Jianmin Han","Shuqin Cao","Jing Liu","Hao Fu"],"pdf_url":"https://arxiv.org/pdf/2412.11448v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05317v3","updated":"2024-12-19T12:38:23Z","published":"2024-10-05T03:47:06Z","title":"Accelerating Diffusion Transformers with Token-wise Feature Caching","summary":"  Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.\n","authors":["Chang Zou","Xuyang Liu","Ting Liu","Siteng Huang","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05317v3.pdf","comment":"In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix"},{"id":"http://arxiv.org/abs/2406.05666v9","updated":"2024-12-19T12:13:26Z","published":"2024-06-09T06:49:22Z","title":"Probability Distribution Learning and Its Application in Deep Learning","summary":"  This paper introduces a novel theoretical learning framework, termed\nprobability distribution learning (PD learning). Departing from the traditional\nstatistical learning framework, PD learning focuses on learning the underlying\nprobability distribution, which is modeled as a random variable within the\nprobability simplex. In this framework, the optimization objective is the\nlearning error, which quantifies the posterior expected discrepancy between the\nmodel's predicted distribution and the underlying true distribution, given\navailable sample data and prior knowledge. To optimize the learning error, this\npaper proposes the necessary conditions for loss functions, models, and\noptimization algorithms, ensuring that these conditions are met in real-world\nmachine learning scenarios. Based on these conditions, the non-convex\noptimization mechanism corresponding to model training can be theoretically\nresolved. Moreover, this paper provides model-dependent and model-independent\nbounds on learning error, offering new insights into the model's fitting and\ngeneralization capabilities. Furthermore, the paper applies the PD learning\nframework to elucidate the mechanisms by which various techniques, including\nrandom parameter initialization, over-parameterization, and dropout, influence\ndeep model training. Finally, the paper substantiates the key conclusions of\nthe proposed framework through experimental results.\n","authors":["Binchuan Qi"],"pdf_url":"https://arxiv.org/pdf/2406.05666v9.pdf","comment":"arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other\n  authors"},{"id":"http://arxiv.org/abs/2412.09265v4","updated":"2024-12-19T12:11:13Z","published":"2024-12-12T13:22:02Z","title":"Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation","summary":"  Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.\n","authors":["Bofang Jia","Pengxiang Ding","Can Cui","Mingyang Sun","Pengfang Qian","Siteng Huang","Zhaoxin Fan","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.09265v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.09826v4","updated":"2024-12-19T12:07:06Z","published":"2023-02-20T08:19:19Z","title":"On the Expressivity of Persistent Homology in Graph Learning","summary":"  Persistent homology, a technique from computational topology, has recently\nshown strong empirical performance in the context of graph classification.\nBeing able to capture long range graph properties via higher-order topological\nfeatures, such as cycles of arbitrary length, in combination with multi-scale\ntopological descriptors, has improved predictive performance for data sets with\nprominent topological structures, such as molecules. At the same time, the\ntheoretical properties of persistent homology have not been formally assessed\nin this context. This paper intends to bridge the gap between computational\ntopology and graph machine learning by providing a brief introduction to\npersistent homology in the context of graphs, as well as a theoretical\ndiscussion and empirical analysis of its expressivity for graph learning tasks.\n","authors":["Rubén Ballester","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2302.09826v4.pdf","comment":"Accepted at the 3rd Learning on Graphs Conference (LoG) 2024"},{"id":"http://arxiv.org/abs/2412.14779v1","updated":"2024-12-19T12:05:13Z","published":"2024-12-19T12:05:13Z","title":"Agent-Temporal Credit Assignment for Optimal Policy Preservation in\n  Sparse Multi-Agent Reinforcement Learning","summary":"  In multi-agent environments, agents often struggle to learn optimal policies\ndue to sparse or delayed global rewards, particularly in long-horizon tasks\nwhere it is challenging to evaluate actions at intermediate time steps. We\nintroduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach\ndesigned to address the agent-temporal credit assignment problem by\nredistributing sparse rewards both temporally and across agents. TAR$^2$\ndecomposes sparse global rewards into time-step-specific rewards and calculates\nagent-specific contributions to these rewards. We theoretically prove that\nTAR$^2$ is equivalent to potential-based reward shaping, ensuring that the\noptimal policy remains unchanged. Empirical results demonstrate that TAR$^2$\nstabilizes and accelerates the learning process. Additionally, we show that\nwhen TAR$^2$ is integrated with single-agent reinforcement learning algorithms,\nit performs as well as or better than traditional multi-agent reinforcement\nlearning methods.\n","authors":["Aditya Kapoor","Sushant Swamy","Kale-ab Tessera","Mayank Baranwal","Mingfei Sun","Harshad Khadilkar","Stefano V. Albrecht"],"pdf_url":"https://arxiv.org/pdf/2412.14779v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.05428v2","updated":"2024-12-19T11:57:19Z","published":"2024-08-10T04:21:04Z","title":"Generalized Encouragement-Based Instrumental Variables for\n  Counterfactual Regression","summary":"  In causal inference, encouragement designs (EDs) are widely used to analyze\ncausal effects, when randomized controlled trials (RCTs) are impractical or\ncompliance to treatment cannot be perfectly enforced. Unlike RCTs, which\ndirectly allocate treatments, EDs randomly assign encouragement policies that\npositively motivate individuals to engage in a specific treatment. These random\nencouragements act as instrumental variables (IVs), facilitating the\nidentification of causal effects through leveraging exogenous perturbations in\ndiscrete treatment scenarios. However, real-world applications of encouragement\ndesigns often face challenges such as incomplete randomization, limited\nexperimental data, and significantly fewer encouragements compared to\ntreatments, hindering precise causal effect estimation. To address this, this\npaper introduces novel theories and algorithms for identifying the Conditional\nAverage Treatment Effect (CATE) using variations in encouragement. Further, by\nleveraging both observational and encouragement data, we propose a generalized\nIV estimator, named Encouragement-based Counterfactual Regression (EnCounteR),\nto effectively estimate the causal effects. Extensive experiments on both\nsynthetic and real-world datasets demonstrate the superiority of EnCounteR over\nexisting methods.\n","authors":["Anpeng Wu","Kun Kuang","Ruoxuan Xiong","Xiangwei Chen","Zexu Sun","Fei Wu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.05428v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14771v1","updated":"2024-12-19T11:55:51Z","published":"2024-12-19T11:55:51Z","title":"ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in\n  Palestine","summary":"  Large Language Models (LLMs) have demonstrated remarkable potential in\ndiverse domains, yet their application in the legal sector, particularly in\nlow-resource contexts, remains limited. This study addresses the challenges of\nadapting LLMs to the Palestinian legal domain, where political instability,\nfragmented legal frameworks, and limited AI resources hinder effective\nmachine-learning applications. We present a fine-tuned model based on a\nquantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set\nderived from Palestinian legal texts. Using smaller-scale models and\nstrategically generated question-answer pairs, we achieve a cost-effective,\nlocally sustainable solution that provides accurate and contextually relevant\nlegal guidance. Our experiments demonstrate promising performance on various\nquery types, ranging from yes/no questions and narrative explanations to\ncomplex legal differentiations, while highlighting areas for improvement, such\nas handling calculation-based inquiries and structured list formatting. This\nwork provides a pathway for the deployment of AI-driven legal assistance tools\ntailored to the needs of resource-constrained environments.\n","authors":["Rabee Qasem","Mohannad Hendi","Banan Tantour"],"pdf_url":"https://arxiv.org/pdf/2412.14771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11654v2","updated":"2024-12-19T11:47:34Z","published":"2024-12-16T10:56:58Z","title":"Smoothness Really Matters: A Simple Yet Effective Approach for\n  Unsupervised Graph Domain Adaptation","summary":"  Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution\nshifts between domains by transferring knowledge from labeled source graphs to\ngiven unlabeled target graphs. Existing UGDA methods primarily focus on\naligning features in the latent space learned by graph neural networks (GNNs)\nacross domains, often overlooking structural shifts, resulting in limited\neffectiveness when addressing structurally complex transfer scenarios. Given\nthe sensitivity of GNNs to local structural features, even slight discrepancies\nbetween source and target graphs could lead to significant shifts in node\nembeddings, thereby reducing the effectiveness of knowledge transfer. To\naddress this issue, we introduce a novel approach for UGDA called Target-Domain\nStructural Smoothing (TDSS). TDSS is a simple and effective method designed to\nperform structural smoothing directly on the target graph, thereby mitigating\nstructural distribution shifts and ensuring the consistency of node\nrepresentations. Specifically, by integrating smoothing techniques with\nneighborhood sampling, TDSS maintains the structural coherence of the target\ngraph while mitigating the risk of over-smoothing. Our theoretical analysis\nshows that TDSS effectively reduces target risk by improving model smoothness.\nEmpirical results on three real-world datasets demonstrate that TDSS\noutperforms recent state-of-the-art baselines, achieving significant\nimprovements across six transfer scenarios. The code is available in\nhttps://github.com/cwei01/TDSS.\n","authors":["Wei Chen","Guo Ye","Yakun Wang","Zhao Zhang","Libang Zhang","Daxin Wang","Zhiqiang Zhang","Fuzhen Zhuang"],"pdf_url":"https://arxiv.org/pdf/2412.11654v2.pdf","comment":"11 pages, Accpected by AAAI2025"},{"id":"http://arxiv.org/abs/2311.07326v2","updated":"2024-12-19T11:41:28Z","published":"2023-11-13T13:27:59Z","title":"MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and\n  Activation Functions","summary":"  Mathematical formulas serve as the means of communication between humans and\nnature, encapsulating the operational laws governing natural phenomena. The\nconcise formulation of these laws is a crucial objective in scientific research\nand an important challenge for artificial intelligence (AI). While traditional\nartificial neural networks (MLP) excel at data fitting, they often yield\nuninterpretable black box results that hinder our understanding of the\nrelationship between variables x and predicted values y. Moreover, the fixed\nnetwork architecture in MLP often gives rise to redundancy in both network\nstructure and parameters. To address these issues, we propose MetaSymNet, a\nnovel neural network that dynamically adjusts its structure in real-time,\nallowing for both expansion and contraction. This adaptive network employs the\nPANGU meta function as its activation function, which is a unique type capable\nof evolving into various basic functions during training to compose\nmathematical formulas tailored to specific needs. We then evolve the neural\nnetwork into a concise, interpretable mathematical expression. To evaluate\nMetaSymNet's performance, we compare it with four state-of-the-art symbolic\nregression algorithms across more than 10 public datasets comprising 222\nformulas. Our experimental results demonstrate that our algorithm outperforms\nothers consistently regardless of noise presence or absence. Furthermore, we\nassess MetaSymNet against MLP and SVM regarding their fitting ability and\nextrapolation capability, these are two essential aspects of machine learning\nalgorithms. The findings reveal that our algorithm excels in both areas.\nFinally, we compared MetaSymNet with MLP using iterative pruning in network\nstructure complexity. The results show that MetaSymNet's network structure\ncomplexity is obviously less than MLP under the same goodness of fit.\n","authors":["Yanjie Li","Weijun Li","Lina Yu","Min Wu","Jinyi Liu","Wenqiang Li","Meilan Hao","Shu Wei","Yusong Deng"],"pdf_url":"https://arxiv.org/pdf/2311.07326v2.pdf","comment":"This work has been accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14753v1","updated":"2024-12-19T11:34:22Z","published":"2024-12-19T11:34:22Z","title":"Opportunities and limitations of explaining quantum machine learning","summary":"  A common trait of many machine learning models is that it is often difficult\nto understand and explain what caused the model to produce the given output.\nWhile the explainability of neural networks has been an active field of\nresearch in the last years, comparably little is known for quantum machine\nlearning models. Despite a few recent works analyzing some specific aspects of\nexplainability, as of now there is no clear big picture perspective as to what\ncan be expected from quantum learning models in terms of explainability. In\nthis work, we address this issue by identifying promising research avenues in\nthis direction and lining out the expected future results. We additionally\npropose two explanation methods designed specifically for quantum machine\nlearning models, as first of their kind to the best of our knowledge. Next to\nour pre-view of the field, we compare both existing and novel methods to\nexplain the predictions of quantum learning models. By studying explainability\nin quantum machine learning, we can contribute to the sustainable development\nof the field, preventing trust issues in the future.\n","authors":["Elies Gil-Fuster","Jonas R. Naujoks","Grégoire Montavon","Thomas Wiegand","Wojciech Samek","Jens Eisert"],"pdf_url":"https://arxiv.org/pdf/2412.14753v1.pdf","comment":"16+16 pages, 3+4 figures"},{"id":"http://arxiv.org/abs/2412.14750v1","updated":"2024-12-19T11:29:57Z","published":"2024-12-19T11:29:57Z","title":"Deep Learning Based Recalibration of SDSS and DESI BAO Alleviates Hubble\n  and Clustering Tensions","summary":"  Conventional calibration of Baryon Acoustic Oscillations (BAO) data relies on\nestimation of the sound horizon at drag epoch $r_d$ from early universe\nobservations by assuming a cosmological model. We present a recalibration of\ntwo independent BAO datasets, SDSS and DESI, by employing deep learning\ntechniques for model-independent estimation of $r_d$, and explore the impacts\non $\\Lambda$CDM cosmological parameters. Significant reductions in both Hubble\n($H_0$) and clustering ($S_8$) tensions are observed for both the recalibrated\ndatasets. Moderate shifts in some other parameters hint towards further\nexploration of such data-driven approaches.\n","authors":["Rahul Shah","Purba Mukherjee","Soumadeep Saha","Utpal Garain","Supratik Pal"],"pdf_url":"https://arxiv.org/pdf/2412.14750v1.pdf","comment":"5 pages, 2 figures, 2 tables. Comments are welcome"},{"id":"http://arxiv.org/abs/2412.14744v1","updated":"2024-12-19T11:22:52Z","published":"2024-12-19T11:22:52Z","title":"A parametric algorithm is optimal for non-parametric regression of\n  smooth functions","summary":"  We address the regression problem for a general function $f:[-1,1]^d\\to\n\\mathbb R$ when the learner selects the training points $\\{x_i\\}_{i=1}^n$ to\nachieve a uniform error bound across the entire domain. In this setting, known\nhistorically as nonparametric regression, we aim to establish a sample\ncomplexity bound that depends solely on the function's degree of smoothness.\nAssuming periodicity at the domain boundaries, we introduce PADUA, an algorithm\nthat, with high probability, provides performance guarantees optimal up to\nconstant or logarithmic factors across all problem parameters. Notably, PADUA\nis the first parametric algorithm with optimal sample complexity for this\nsetting. Due to this feature, we prove that, differently from the\nnon-parametric state of the art, PADUA enjoys optimal space complexity in the\nprediction phase. To validate these results, we perform numerical experiments\nover functions coming from real audio data, where PADUA shows comparable\nperformance to state-of-the-art methods, while requiring only a fraction of the\ncomputational time.\n","authors":["Davide Maran","Marcello Restelli"],"pdf_url":"https://arxiv.org/pdf/2412.14744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14741v1","updated":"2024-12-19T11:17:31Z","published":"2024-12-19T11:17:31Z","title":"Active Inference and Human--Computer Interaction","summary":"  Active Inference is a closed-loop computational theoretical basis for\nunderstanding behaviour, based on agents with internal probabilistic generative\nmodels that encode their beliefs about how hidden states in their environment\ncause their sensations. We review Active Inference and how it could be applied\nto model the human-computer interaction loop. Active Inference provides a\ncoherent framework for managing generative models of humans, their\nenvironments, sensors and interface components. It informs off-line design and\nsupports real-time, online adaptation. It provides model-based explanations for\nbehaviours observed in HCI, and new tools to measure important concepts such as\nagency and engagement. We discuss how Active Inference offers a new basis for a\ntheory of interaction in HCI, tools for design of modern, complex sensor-based\nsystems, and integration of artificial intelligence technologies, enabling it\nto cope with diversity in human users and contexts. We discuss the practical\nchallenges in implementing such Active Inference-based systems.\n","authors":["Roderick Murray-Smith","John H. Williamson","Sebastian Stein"],"pdf_url":"https://arxiv.org/pdf/2412.14741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14739v1","updated":"2024-12-19T11:15:02Z","published":"2024-12-19T11:15:02Z","title":"On the Use of Deep Learning Models for Semantic Clone Detection","summary":"  Detecting and tracking code clones can ease various software development and\nmaintenance tasks when changes in a code fragment should be propagated over all\nits copies. Several deep learning-based clone detection models have appeared in\nthe literature for detecting syntactic and semantic clones, widely evaluated\nwith the BigCloneBench dataset. However, class imbalance and the small number\nof semantic clones make BigCloneBench less ideal for interpreting model\nperformance. Researchers also use other datasets such as GoogleCodeJam,\nOJClone, and SemanticCloneBench to understand model generalizability. To\novercome the limitations of existing datasets, the GPT-assisted semantic and\ncross-language clone dataset GPTCloneBench has been released. However, how\nthese models compare across datasets remains unclear. In this paper, we propose\na multi-step evaluation approach for five state-of-the-art clone detection\nmodels leveraging existing benchmark datasets, including GPTCloneBench, and\nusing mutation operators to study model ability. Specifically, we examine three\nhighly-performing single-language models (ASTNN, GMN, CodeBERT) on\nBigCloneBench, SemanticCloneBench, and GPTCloneBench, testing their robustness\nwith mutation operations. Additionally, we compare them against cross-language\nmodels (C4, CLCDSA) known for detecting semantic clones. While single-language\nmodels show high F1 scores for BigCloneBench, their performance on\nSemanticCloneBench varies (up to 20%). Interestingly, the cross-language model\n(C4) shows superior performance (around 7%) on SemanticCloneBench over other\nmodels and performs similarly on BigCloneBench and GPTCloneBench. On\nmutation-based datasets, C4 has more robust performance (less than 1%\ndifference) compared to single-language models, which show high variability.\n","authors":["Subroto Nag Pinku","Debajyoti Mondal","Chanchal K. Roy"],"pdf_url":"https://arxiv.org/pdf/2412.14739v1.pdf","comment":"Accepted at the 40th IEEE International Conference on Software\n  Maintenance and Evolution (ICSME 2024)"},{"id":"http://arxiv.org/abs/2409.11383v2","updated":"2024-12-19T11:12:30Z","published":"2024-09-17T17:34:24Z","title":"Training Datasets Generation for Machine Learning: Application to Vision\n  Based Navigation","summary":"  Vision Based Navigation consists in utilizing cameras as precision sensors\nfor GNC after extracting information from images. To enable the adoption of\nmachine learning for space applications, one of obstacles is the demonstration\nthat available training datasets are adequate to validate the algorithms. The\nobjective of the study is to generate datasets of images and metadata suitable\nfor training machine learning algorithms. Two use cases were selected and a\nrobust methodology was developed to validate the datasets including the ground\ntruth. The first use case is in-orbit rendezvous with a man-made object: a\nmockup of satellite ENVISAT. The second use case is a Lunar landing scenario.\nDatasets were produced from archival datasets (Chang'e 3), from the laboratory\nat DLR TRON facility and at Airbus Robotic laboratory, from SurRender software\nhigh fidelity image simulator using Model Capture and from Generative\nAdversarial Networks. The use case definition included the selection of\nalgorithms as benchmark: an AI-based pose estimation algorithm and a dense\noptical flow algorithm were selected. Eventually it is demonstrated that\ndatasets produced with SurRender and selected laboratory facilities are\nadequate to train machine learning algorithms.\n","authors":["Jérémy Lebreton","Ingo Ahrns","Roland Brochard","Christoph Haskamp","Hans Krüger","Matthieu Le Goff","Nicolas Menga","Nicolas Ollagnier","Ralf Regele","Francesco Capolupo","Massimo Casasco"],"pdf_url":"https://arxiv.org/pdf/2409.11383v2.pdf","comment":"6 pages, 4 figures, preprint of the proceedings of ESA SPAICE\n  conference 2024"},{"id":"http://arxiv.org/abs/2412.14738v1","updated":"2024-12-19T11:10:48Z","published":"2024-12-19T11:10:48Z","title":"Boosting GNN Performance via Training Sample Selection Based on\n  Adversarial Robustness Evaluation","summary":"  Graph Neural Networks (GNNs) have established themselves as one of the most\npowerful neural network architectures, excelling in leveraging graph topology\nand node features for various tasks. However, GNNs are inherently vulnerable to\nnoise in their inputs. Such noise can significantly degrade their performance.\nTo address this challenge, we propose a novel approach that employs adversarial\nrobustness evaluation techniques to identify nodes in the graph that are most\nsusceptible to noise. By selecting and constructing a training set composed of\nthese particularly noise-prone nodes, we then use them to train a Graph\nConvolutional Network (GCN). Our experimental results demonstrate that this\nstrategy leads to substantial improvements in the GCN's performance.\n","authors":["Yongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10099v2","updated":"2024-12-19T11:06:39Z","published":"2024-04-15T19:15:32Z","title":"Feature selection in linear SVMs via a hard cardinality constraint: a\n  scalable SDP decomposition approach","summary":"  In this paper, we study the embedded feature selection problem in linear\nSupport Vector Machines (SVMs), in which a cardinality constraint is employed,\nleading to an interpretable classification model. The problem is NP-hard due to\nthe presence of the cardinality constraint, even though the original linear SVM\namounts to a problem solvable in polynomial time. To handle the hard problem,\nwe first introduce two mixed-integer formulations for which novel semidefinite\nrelaxations are proposed. Exploiting the sparsity pattern of the relaxations,\nwe decompose the problems and obtain equivalent relaxations in a much smaller\ncone, making the conic approaches scalable. To make the best usage of the\ndecomposed relaxations, we propose heuristics using the information of its\noptimal solution. Moreover, an exact procedure is proposed by solving a\nsequence of mixed-integer decomposed semidefinite optimization problems.\nNumerical results on classical benchmarking datasets are reported, showing the\nefficiency and effectiveness of our approach.\n","authors":["Immanuel Bomze","Federico D'Onofrio","Laura Palagi","Bo Peng"],"pdf_url":"https://arxiv.org/pdf/2404.10099v2.pdf","comment":"Submitted to European Journal of Operational Research. arXiv admin\n  note: text overlap with arXiv:1808.02435 by other authors"},{"id":"http://arxiv.org/abs/2309.11036v2","updated":"2024-12-19T10:58:39Z","published":"2023-09-20T03:31:11Z","title":"Scalable Acceleration for Classification-Based Derivative-Free\n  Optimization","summary":"  Derivative-free optimization algorithms play an important role in scientific\nand engineering design optimization problems, especially when derivative\ninformation is not accessible. In this paper, we study the framework of\nsequential classification-based derivative-free optimization algorithms. By\nintroducing learning theoretic concept hypothesis-target shattering rate, we\nrevisit the computational complexity upper bound of SRACOS (Hu, Qian, and Yu\n2017). Inspired by the revisited upper bound, we propose an algorithm named\nRACE-CARS, which adds a random region-shrinking step compared with SRACOS. We\nfurther establish theorems showing the acceleration by region shrinking.\nExperiments on the synthetic functions as well as black-box tuning for\nlanguage-model-as-a-service demonstrate empirically the efficiency of\nRACE-CARS. An ablation experiment on the introduced hyperparameters is also\nconducted, revealing the mechanism of RACE-CARS and putting forward an\nempirical hyper-parameter tuning guidance.\n","authors":["Tianyi Han","Jingya Li","Zhipeng Guo","Yuan Jin"],"pdf_url":"https://arxiv.org/pdf/2309.11036v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14730v1","updated":"2024-12-19T10:56:18Z","published":"2024-12-19T10:56:18Z","title":"Generative AI for Banks: Benchmarks and Algorithms for Synthetic\n  Financial Transaction Data","summary":"  The banking sector faces challenges in using deep learning due to data\nsensitivity and regulatory constraints, but generative AI may offer a solution.\nThus, this study identifies effective algorithms for generating synthetic\nfinancial transaction data and evaluates five leading models - Conditional\nTabular Generative Adversarial Networks (CTGAN), DoppelGANger (DGAN),\nWasserstein GAN, Financial Diffusion (FinDiff), and Tabular Variational\nAutoEncoders (TVAE) - across five criteria: fidelity, synthesis quality,\nefficiency, privacy, and graph structure. While none of the algorithms is able\nto replicate the real data's graph structure, each excels in specific areas:\nDGAN is ideal for privacy-sensitive tasks, FinDiff and TVAE excel in data\nreplication and augmentation, and CTGAN achieves a balance across all five\ncriteria, making it suitable for general applications with moderate privacy\nconcerns. As a result, our findings offer valuable insights for choosing the\nmost suitable algorithm.\n","authors":["Fabian Sven Karst","Sook-Yee Chong","Abigail A. Antenor","Enyu Lin","Mahei Manhai Li","Jan Marco Leimeister"],"pdf_url":"https://arxiv.org/pdf/2412.14730v1.pdf","comment":"Presented at the 34th Workshop on Information Technologies and\n  Systems (WITS 2024)"},{"id":"http://arxiv.org/abs/2412.14724v1","updated":"2024-12-19T10:47:31Z","published":"2024-12-19T10:47:31Z","title":"FROC: Building Fair ROC from a Trained Classifier","summary":"  This paper considers the problem of fair probabilistic binary classification\nwith binary protected groups. The classifier assigns scores, and a practitioner\npredicts labels using a certain cut-off threshold based on the desired\ntrade-off between false positives vs. false negatives. It derives these\nthresholds from the ROC of the classifier. The resultant classifier may be\nunfair to one of the two protected groups in the dataset. It is desirable that\nno matter what threshold the practitioner uses, the classifier should be fair\nto both the protected groups; that is, the $\\mathcal{L}_p$ norm between FPRs\nand TPRs of both the protected groups should be at most $\\varepsilon$. We call\nsuch fairness on ROCs of both the protected attributes\n$\\varepsilon_p$-Equalized ROC. Given a classifier not satisfying\n$\\varepsilon_1$-Equalized ROC, we aim to design a post-processing method to\ntransform the given (potentially unfair) classifier's output (score) to a\nsuitable randomized yet fair classifier. That is, the resultant classifier must\nsatisfy $\\varepsilon_1$-Equalized ROC. First, we introduce a threshold query\nmodel on the ROC curves for each protected group. The resulting classifier is\nbound to face a reduction in AUC. With the proposed query model, we provide a\nrigorous theoretical analysis of the minimal AUC loss to achieve\n$\\varepsilon_1$-Equalized ROC. To achieve this, we design a linear time\nalgorithm, namely \\texttt{FROC}, to transform a given classifier's output to a\nprobabilistic classifier that satisfies $\\varepsilon_1$-Equalized ROC. We prove\nthat under certain theoretical conditions, \\texttt{FROC}\\ achieves the\ntheoretical optimal guarantees. We also study the performance of our\n\\texttt{FROC}\\ on multiple real-world datasets with many trained classifiers.\n","authors":["Avyukta Manjunatha Vummintala","Shantanu Das","Sujit Gujar"],"pdf_url":"https://arxiv.org/pdf/2412.14724v1.pdf","comment":"51 pages, The 39th Annual AAAI Conference on Artificial Intelligence"},{"id":"http://arxiv.org/abs/2406.02507v3","updated":"2024-12-19T10:43:11Z","published":"2024-06-04T17:25:59Z","title":"Guiding a Diffusion Model with a Bad Version of Itself","summary":"  The primary axes of interest in image-generating diffusion models are image\nquality, the amount of variation in the results, and how well the results align\nwith a given condition, e.g., a class label or a text prompt. The popular\nclassifier-free guidance approach uses an unconditional model to guide a\nconditional model, leading to simultaneously better prompt alignment and\nhigher-quality images at the cost of reduced variation. These effects seem\ninherently entangled, and thus hard to control. We make the surprising\nobservation that it is possible to obtain disentangled control over image\nquality without compromising the amount of variation by guiding generation\nusing a smaller, less-trained version of the model itself rather than an\nunconditional model. This leads to significant improvements in ImageNet\ngeneration, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using\npublicly available networks. Furthermore, the method is also applicable to\nunconditional diffusion models, drastically improving their quality.\n","authors":["Tero Karras","Miika Aittala","Tuomas Kynkäänniemi","Jaakko Lehtinen","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2406.02507v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14719v1","updated":"2024-12-19T10:41:24Z","published":"2024-12-19T10:41:24Z","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","summary":"  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the\nambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to\nidentify the ambiguous sample, categorizing them into distinct sets of\nambiguous samples of false negatives and false positives, considering both\nbody- and action-level categories. \\textbf{Secondly}, we implement an ambiguous\ncontrastive refinement module to calibrate these ambiguous samples by\nregulating the distance between ambiguous samples and their corresponding\nprototypes. This calibration process aims to pull false negative\n($\\mathbb{FN}$) samples closer to their respective prototypes and push false\npositive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. \\textbf{Finally}, we propose a prototype-guided rectification to\nrectify prediction by incorporating the representability of prototypes.\nExtensive experiments conducted on the benchmark dataset demonstrate the\nsuperior performance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14719v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14718v1","updated":"2024-12-19T10:33:19Z","published":"2024-12-19T10:33:19Z","title":"A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical\n  Forecasting Reconciliation and Adjustment","summary":"  Ads demand forecasting for Walmart's ad products plays a critical role in\nenabling effective resource planning, allocation, and management of ads\nperformance. In this paper, we introduce a comprehensive demand forecasting\nsystem that tackles hierarchical time series forecasting in business settings.\nThough traditional hierarchical reconciliation methods ensure forecasting\ncoherence, they often trade off accuracy for coherence especially at lower\nlevels and fail to capture the seasonality unique to each time-series in the\nhierarchy. Thus, we propose a novel framework \"Multi-Stage Hierarchical\nForecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)\" to address\nthe challenges of preserving seasonality, ensuring coherence, and improving\naccuracy. Our system first utilizes diverse models, ensembled through Bayesian\nOptimization (BO), achieving base forecasts. The generated base forecasts are\nthen passed into the Multi-Stage HiFoReAd framework. The initial stage refines\nthe hierarchy using Top-Down forecasts and \"harmonic alignment.\" The second\nstage aligns the higher levels' forecasts using MinTrace algorithm, following\nwhich the last two levels undergo \"harmonic alignment\" and \"stratified\nscaling\", to eventually achieve accurate and coherent forecasts across the\nwhole hierarchy. Our experiments on Walmart's internal Ads-demand dataset and 3\nother public datasets, each with 4 hierarchical levels, demonstrate that the\naverage Absolute Percentage Error from the cross-validation sets improve from\n3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet)\nas well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the\nforecasts at all hierarchical levels are proved to be coherent. The proposed\nframework has been deployed and leveraged by Walmart's ads, sales and\noperations teams to track future demands, make informed decisions and plan\nresources.\n","authors":["Zhengchao Yang","Mithun Ghosh","Anish Saha","Dong Xu","Konstantin Shmakov","Kuang-chih Lee"],"pdf_url":"https://arxiv.org/pdf/2412.14718v1.pdf","comment":"Published in 2024 IEEE International Conference on Big Data (BigData)"},{"id":"http://arxiv.org/abs/2412.11242v2","updated":"2024-12-19T10:33:13Z","published":"2024-12-15T16:47:16Z","title":"TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs","summary":"  Specializing large language models (LLMs) for local deployment in\ndomain-specific use cases is necessary for strong performance while meeting\nlatency and privacy constraints. However, conventional task-specific adaptation\napproaches do not show simultaneous memory saving and inference speedup at\ndeployment time. Practical compression techniques like quantization and pruning\nrequire dedicated hardware or kernel support to achieve measured inference\nspeedup. We develop TrimLLM based on the layer-wise specialization phenomenon\nwe empirically observed and verified on contemporary LLMs. TrimLLM reduces the\ndepth of LLMs via progressive layer dropping. We show it retains LLMs' capacity\nin specific domains and achieves inference speedup irrespective of hardware and\ndeep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for\ninference; models adapted on medical, legal, and financial datasets all\ndemonstrate $2.1-5.7\\times$ inference speedup on consumer GPUs and up to\n$3.1\\times$ speedup on A100 when compared to state-of-the-art model compression\nalgorithms, with no loss in accuracy at 50$\\sim$60\\% model compression ratio.\n","authors":["Lanxiang Hu","Tajana Rosing","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.11242v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14717v1","updated":"2024-12-19T10:31:25Z","published":"2024-12-19T10:31:25Z","title":"Computing Gram Matrix for SMILES Strings using RDKFingerprint and\n  Sinkhorn-Knopp Algorithm","summary":"  In molecular structure data, SMILES (Simplified Molecular Input Line Entry\nSystem) strings are used to analyze molecular structure design. Numerical\nfeature representation of SMILES strings is a challenging task. This work\nproposes a kernel-based approach for encoding and analyzing molecular\nstructures from SMILES strings. The proposed approach involves computing a\nkernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal\ncomponent analysis (PCA) for dimensionality reduction. The resulting\nlow-dimensional embeddings are then used for classification and regression\nanalysis. The kernel matrix is computed by converting the SMILES strings into\nmolecular structures using the Morgan Fingerprint, which computes a fingerprint\nfor each molecule. The distance matrix is computed using the pairwise kernels\nfunction. The Sinkhorn-Knopp algorithm is used to compute the final kernel\nmatrix that satisfies the constraints of a probability distribution. This is\nachieved by iteratively adjusting the kernel matrix until the marginal\ndistributions of the rows and columns match the desired marginal distributions.\nWe provided a comprehensive empirical analysis of the proposed kernel method to\nevaluate its goodness with greater depth. The suggested method is assessed for\ndrug subcategory prediction (classification task) and solubility AlogPS\n``Aqueous solubility and Octanol/Water partition coefficient\" (regression task)\nusing the benchmark SMILES string dataset. The outcomes show the proposed\nmethod outperforms several baseline methods in terms of supervised analysis and\nhas potential uses in molecular design and drug discovery. Overall, the\nsuggested method is a promising avenue for kernel methods-based molecular\nstructure analysis and design.\n","authors":["Sarwan Ali","Haris Mansoor","Prakash Chourasia","Imdad Ullah Khan","Murray Patterson"],"pdf_url":"https://arxiv.org/pdf/2412.14717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14714v1","updated":"2024-12-19T10:25:21Z","published":"2024-12-19T10:25:21Z","title":"Holistic Adversarially Robust Pruning","summary":"  Neural networks can be drastically shrunk in size by removing redundant\nparameters. While crucial for the deployment on resource-constraint hardware,\noftentimes, compression comes with a severe drop in accuracy and lack of\nadversarial robustness. Despite recent advances, counteracting both aspects has\nonly succeeded for moderate compression rates so far. We propose a novel\nmethod, HARP, that copes with aggressive pruning significantly better than\nprior work. For this, we consider the network holistically. We learn a global\ncompression strategy that optimizes how many parameters (compression rate) and\nwhich parameters (scoring connections) to prune specific to each layer\nindividually. Our method fine-tunes an existing model with dynamic\nregularization, that follows a step-wise incremental function balancing the\ndifferent objectives. It starts by favoring robustness before shifting focus on\nreaching the target compression rate and only then handles the objectives\nequally. The learned compression strategies allow us to maintain the\npre-trained model natural accuracy and its adversarial robustness for a\nreduction by 99% of the network original size. Moreover, we observe a crucial\ninfluence of non-uniform compression across layers.\n","authors":["Qi Zhao","Christian Wressnegger"],"pdf_url":"https://arxiv.org/pdf/2412.14714v1.pdf","comment":"Accepted by ICLR 2023"},{"id":"http://arxiv.org/abs/2412.14711v1","updated":"2024-12-19T10:21:20Z","published":"2024-12-19T10:21:20Z","title":"ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing","summary":"  Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to\nscale up model capacity without increasing the computation budget. However,\nvanilla TopK routers are trained in a discontinuous, non-differentiable way,\nlimiting their performance and scalability. To address this issue, we propose\nReMoE, a fully differentiable MoE architecture that offers a simple yet\neffective drop-in replacement for the conventional TopK+Softmax routing,\nutilizing ReLU as the router instead. We further propose methods to regulate\nthe router's sparsity while balancing the load among experts. ReMoE's\ncontinuous nature enables efficient dynamic allocation of computation across\ntokens and layers, while also exhibiting domain specialization. Our experiments\ndemonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across\nvarious model sizes, expert counts, and levels of granularity. Furthermore,\nReMoE exhibits superior scalability with respect to the number of experts,\nsurpassing traditional MoE architectures. The implementation based on\nMegatron-LM is available at https://github.com/thu-ml/ReMoE.\n","authors":["Ziteng Wang","Jianfei Chen","Jun Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.14711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11479v2","updated":"2024-12-19T10:21:11Z","published":"2024-08-21T09:44:43Z","title":"Learning Deep Dissipative Dynamics","summary":"  This study challenges strictly guaranteeing ``dissipativity'' of a dynamical\nsystem represented by neural networks learned from given time-series data.\nDissipativity is a crucial indicator for dynamical systems that generalizes\nstability and input-output stability, known to be valid across various systems\nincluding robotics, biological systems, and molecular dynamics. By analytically\nproving the general solution to the nonlinear Kalman-Yakubovich-Popov (KYP)\nlemma, which is the necessary and sufficient condition for dissipativity, we\npropose a differentiable projection that transforms any dynamics represented by\nneural networks into dissipative ones and a learning method for the transformed\ndynamics. Utilizing the generality of dissipativity, our method strictly\nguarantee stability, input-output stability, and energy conservation of trained\ndynamical systems. Finally, we demonstrate the robustness of our method against\nout-of-domain input through applications to robotic arms and fluid dynamics.\nCode is https://github.com/kojima-r/DeepDissipativeModel\n","authors":["Yuji Okamoto","Ryosuke Kojima"],"pdf_url":"https://arxiv.org/pdf/2408.11479v2.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08555v2","updated":"2024-12-19T10:12:38Z","published":"2024-12-11T17:17:02Z","title":"Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks\n  Defending against Poisoning Attacks","summary":"  Recent studies have revealed the vulnerability of graph neural networks\n(GNNs) to adversarial poisoning attacks on node classification tasks. Current\ndefensive methods require substituting the original GNNs with defense models,\nregardless of the original's type. This approach, while targeting adversarial\nrobustness, compromises the enhancements developed in prior research to boost\nGNNs' practical performance. Here we introduce Grimm, the first plug-and-play\ndefense model. With just a minimal interface requirement for extracting\nfeatures from any layer of the protected GNNs, Grimm is thus enabled to\nseamlessly rectify perturbations. Specifically, we utilize the feature\ntrajectories (FTs) generated by GNNs, as they evolve through epochs, to reflect\nthe training status of the networks. We then theoretically prove that the FTs\nof victim nodes will inevitably exhibit discriminable anomalies. Consequently,\ninspired by the natural parallelism between the biological nervous and immune\nsystems, we construct Grimm, a comprehensive artificial immune system for GNNs.\nGrimm not only detects abnormal FTs and rectifies adversarial edges during\ntraining but also operates efficiently in parallel, thereby mirroring the\nconcurrent functionalities of its biological counterparts. We experimentally\nconfirm that Grimm offers four empirically validated advantages: 1)\nHarmlessness, as it does not actively interfere with GNN training; 2)\nParallelism, ensuring monitoring, detection, and rectification functions\noperate independently of the GNN training process; 3) Generalizability,\ndemonstrating compatibility with mainstream GNNs such as GCN, GAT, and\nGraphSAGE; and 4) Transferability, as the detectors for abnormal FTs can be\nefficiently transferred across different systems for one-step rectification.\n","authors":["Ao Liu","Wenshan Li","Beibei Li","Wengang Ma","Tao Li","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.08555v2.pdf","comment":"19 pages, 13 figures"},{"id":"http://arxiv.org/abs/2202.06374v5","updated":"2024-12-19T10:12:00Z","published":"2022-02-13T18:04:00Z","title":"Holdouts set for safe predictive model updating","summary":"  Predictive risk scores for adverse outcomes are increasingly crucial in\nguiding health interventions. Such scores may need to be periodically updated\ndue to change in the distributions they model. However, directly updating risk\nscores used to guide intervention can lead to biased risk estimates. To address\nthis, we propose updating using a `holdout set' - a subset of the population\nthat does not receive interventions guided by the risk score. Balancing the\nholdout set size is essential to ensure good performance of the updated risk\nscore whilst minimising the number of held out samples. We prove that this\napproach reduces adverse outcome frequency to an asymptotically optimal level\nand argue that often there is no competitive alternative. We describe\nconditions under which an optimal holdout size (OHS) can be readily identified,\nand introduce parametric and semi-parametric algorithms for OHS estimation. We\napply our methods to the ASPRE risk score for pre-eclampsia to recommend a plan\nfor updating it in the presence of change in the underlying data distribution.\nWe show that, in order to minimise the number of pre-eclampsia cases over time,\nthis is best achieved using a holdout set of around 10,000 individuals.\n","authors":["Sami Haidar-Wehbe","Samuel R Emerson","Louis J M Aslett","James Liley"],"pdf_url":"https://arxiv.org/pdf/2202.06374v5.pdf","comment":"Manuscript includes supplementary materials and figures"},{"id":"http://arxiv.org/abs/2412.07675v3","updated":"2024-12-19T10:11:42Z","published":"2024-12-10T17:02:58Z","title":"RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text\n  Rewriting","summary":"  Despite the widespread use of LLMs due to their superior performance in\nvarious tasks, their high computational costs often lead potential users to opt\nfor the pretraining-finetuning pipeline. However, biases prevalent in manually\nconstructed datasets can introduce spurious correlations between tokens and\nlabels, creating so-called shortcuts and hindering the generalizability of\nfine-tuned models. Existing debiasing methods often rely on prior knowledge of\nspecific dataset biases, which is challenging to acquire a priori. We propose\nRAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,\nand data-focused debiasing approach based on text rewriting for shortcut\nmitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text\nsegments by replacing them with heuristically selected alternatives in a\nshortcut space defined by token statistics and positional information. This\nprocess aims to align surface-level text features more closely with diverse\nlabel distributions, thereby promoting the learning of genuine linguistic\npatterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the\nFEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.\nAdditionally, RAZOR effectively mitigates specific known biases, reducing\nbias-related terms by x2 without requiring prior bias information, a result\nthat is on par with SoTA models that leverage prior information. Our work\nprioritizes data manipulation over architectural modifications, emphasizing the\npivotal role of data quality in enhancing model performance and fairness. This\nresearch contributes to developing more robust evaluation benchmarks for\ndebiasing methods by incorporating metrics for bias reduction and overall model\nefficacy.\n","authors":["Shuo Yang","Bardh Prenkaj","Gjergji Kasneci"],"pdf_url":"https://arxiv.org/pdf/2412.07675v3.pdf","comment":"Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117"},{"id":"http://arxiv.org/abs/2412.14701v1","updated":"2024-12-19T10:10:57Z","published":"2024-12-19T10:10:57Z","title":"Taming the Memory Beast: Strategies for Reliable ML Training on\n  Kubernetes","summary":"  Kubernetes offers a powerful orchestration platform for machine learning\ntraining, but memory management can be challenging due to specialized needs and\nresource constraints. This paper outlines how Kubernetes handles memory\nrequests, limits, Quality of Service classes, and eviction policies for ML\nworkloads, with special focus on GPU memory and ephemeral storage. Common\npitfalls such as overcommitment, memory leaks, and ephemeral volume exhaustion\nare examined. We then provide best practices for stable, scalable memory\nutilization to help ML practitioners prevent out-of-memory events and ensure\nhigh-performance ML training pipelines.\n","authors":["Jaideep Ray"],"pdf_url":"https://arxiv.org/pdf/2412.14701v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2412.08160v4","updated":"2024-12-19T10:01:27Z","published":"2024-12-11T07:32:38Z","title":"DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with\n  Selective State Space Models","summary":"  Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns,\nwidely existing in the real world. Nevertheless, the structure incompleteness,\nnoise, and redundancy result in poor robustness for Dynamic Graph Neural\nNetworks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising\nway to optimize graph structures. However, aside from encountering unacceptable\nquadratic complexity, it overly relies on heuristic priors, making it hard to\ndiscover underlying predictive patterns. How to efficiently refine the dynamic\nstructures, capture intrinsic dependencies, and learn robust representations,\nremains under-explored. In this work, we propose the novel DG-Mamba, a robust\nand efficient Dynamic Graph structure learning framework with the Selective\nState Space Models (Mamba). To accelerate the spatio-temporal structure\nlearning, we propose a kernelized dynamic message-passing operator that reduces\nthe quadratic time complexity to linear. To capture global intrinsic dynamics,\nwe establish the dynamic graph as a self-contained system with State Space\nModel. By discretizing the system states with the cross-snapshot graph\nadjacency, we enable the long-distance dependencies capturing with the\nselective snapshot scan. To endow learned dynamic structures more expressive\nwith informativeness, we propose the self-supervised Principle of Relevant\nInformation for DGSL to regularize the most relevant yet least redundant\ninformation, enhancing global robustness. Extensive experiments demonstrate the\nsuperiority of the robustness and efficiency of our DG-Mamba compared with the\nstate-of-the-art baselines against adversarial attacks.\n","authors":["Haonan Yuan","Qingyun Sun","Zhaonan Wang","Xingcheng Fu","Cheng Ji","Yongjian Wang","Bo Jin","Jianxin Li"],"pdf_url":"https://arxiv.org/pdf/2412.08160v4.pdf","comment":"Accepted by the Main Technical Track of the 39th Annual AAAI\n  Conference on Artificial Intelligence (AAAI-2025)"},{"id":"http://arxiv.org/abs/2412.14695v1","updated":"2024-12-19T09:56:01Z","published":"2024-12-19T09:56:01Z","title":"Lorentzian Residual Neural Networks","summary":"  Hyperbolic neural networks have emerged as a powerful tool for modeling\nhierarchical data structures prevalent in real-world datasets. Notably,\nresidual connections, which facilitate the direct flow of information across\nlayers, have been instrumental in the success of deep neural networks. However,\ncurrent methods for constructing hyperbolic residual networks suffer from\nlimitations such as increased model complexity, numerical instability, and\nerrors due to multiple mappings to and from the tangent space. To address these\nlimitations, we introduce LResNet, a novel Lorentzian residual neural network\nbased on the weighted Lorentzian centroid in the Lorentz model of hyperbolic\ngeometry. Our method enables the efficient integration of residual connections\nin Lorentz hyperbolic neural networks while preserving their hierarchical\nrepresentation capabilities. We demonstrate that our method can theoretically\nderive previous methods while offering improved stability, efficiency, and\neffectiveness. Extensive experiments on both graph and vision tasks showcase\nthe superior performance and robustness of our method compared to\nstate-of-the-art Euclidean and hyperbolic alternatives. Our findings highlight\nthe potential of \\method for building more expressive neural networks in\nhyperbolic embedding space as a generally applicable method to multiple\narchitectures, including CNNs, GNNs, and graph Transformers.\n","authors":["Neil He","Menglin Yang","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2412.14695v1.pdf","comment":"12 pages, 3 figures, KDD 2025"},{"id":"http://arxiv.org/abs/2410.05016v2","updated":"2024-12-19T09:49:25Z","published":"2024-10-07T13:15:07Z","title":"T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data","summary":"  Self-supervision is often used for pre-training to foster performance on a\ndownstream task by constructing meaningful representations of samples.\nSelf-supervised learning (SSL) generally involves generating different views of\nthe same sample and thus requires data augmentations that are challenging to\nconstruct for tabular data. This constitutes one of the main challenges of\nself-supervision for structured data. In the present work, we propose a novel\naugmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on\na Joint Embedding Predictive Architecture (JEPA) and is akin to mask\nreconstruction in the latent space. It involves predicting the latent\nrepresentation of one subset of features from the latent representation of a\ndifferent subset within the same sample, thereby learning rich representations\nwithout augmentations. We use our method as a pre-training technique and train\nseveral deep classifiers on the obtained representation. Our experimental\nresults demonstrate a substantial improvement in both classification and\nregression tasks, outperforming models trained directly on samples in their\noriginal data space. Moreover, T-JEPA enables some methods to consistently\noutperform or match the performance of traditional methods likes Gradient\nBoosted Decision Trees. To understand why, we extensively characterize the\nobtained representations and show that T-JEPA effectively identifies relevant\nfeatures for downstream tasks without access to the labels. Additionally, we\nintroduce regularization tokens, a novel regularization method critical for\ntraining of JEPA-based models on structured data.\n","authors":["Hugo Thimonier","José Lucas De Melo Costa","Fabrice Popineau","Arpad Rimmel","Bich-Liên Doan"],"pdf_url":"https://arxiv.org/pdf/2410.05016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14689v1","updated":"2024-12-19T09:43:39Z","published":"2024-12-19T09:43:39Z","title":"How to Synthesize Text Data without Model Collapse?","summary":"  Model collapse in synthetic data indicates that iterative training on\nself-generated data leads to a gradual decline in performance. With the\nproliferation of AI models, synthetic data will fundamentally reshape the web\ndata ecosystem. Future GPT-$\\{n\\}$ models will inevitably be trained on a blend\nof synthetic and human-produced data. In this paper, we focus on two questions:\nwhat is the impact of synthetic data on language model training, and how to\nsynthesize data without model collapse? We first pre-train language models\nacross different proportions of synthetic data, revealing a negative\ncorrelation between the proportion of synthetic data and model performance. We\nfurther conduct statistical analysis on synthetic data to uncover\ndistributional shift phenomenon and over-concentration of n-gram features.\nInspired by the above findings, we propose token editing on human-produced data\nto obtain semi-synthetic data. As a proof of concept, we theoretically\ndemonstrate that token-level editing can prevent model collapse, as the test\nerror is constrained by a finite upper bound. We conduct extensive experiments\non pre-training from scratch, continual pre-training, and supervised\nfine-tuning. The results validate our theoretical proof that token-level\nediting improves data quality and enhances model performance.\n","authors":["Xuekai Zhu","Daixuan Cheng","Hengli Li","Kaiyan Zhang","Ermo Hua","Xingtai Lv","Ning Ding","Zhouhan Lin","Zilong Zheng","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12327v2","updated":"2024-12-19T09:34:30Z","published":"2024-12-16T19:54:57Z","title":"Leveraging Group Classification with Descending Soft Labeling for Deep\n  Imbalanced Regression","summary":"  Deep imbalanced regression (DIR), where the target values have a highly\nskewed distribution and are also continuous, is an intriguing yet\nunder-explored problem in machine learning.\n  While recent works have already shown that incorporating various\nclassification-based regularizers can produce enhanced outcomes, the role of\nclassification remains elusive in DIR.\n  Moreover, such regularizers (e.g., contrastive penalties) merely focus on\nlearning discriminative features of data, which inevitably results in ignorance\nof either continuity or similarity across the data.\n  To address these issues, we first bridge the connection between the\nobjectives of DIR and classification from a Bayesian perspective.\n  Consequently, this motivates us to decompose the objective of DIR into a\ncombination of classification and regression tasks, which naturally guides us\ntoward a divide-and-conquer manner to solve the DIR problem.\n  Specifically, by aggregating the data at nearby labels into the same groups,\nwe introduce an ordinal group-aware contrastive learning loss along with a\nmulti-experts regressor to tackle the different groups of data thereby\nmaintaining the data continuity.\n  Meanwhile, considering the similarity between the groups, we also propose a\nsymmetric descending soft labeling strategy to exploit the intrinsic similarity\nacross the data, which allows classification to facilitate regression more\neffectively.\n  Extensive experiments on real-world datasets also validate the effectiveness\nof our method.\n","authors":["Ruizhi Pu","Gezheng Xu","Ruiyi Fang","Binkun Bao","Charles X. Ling","Boyu Wang"],"pdf_url":"https://arxiv.org/pdf/2412.12327v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.06401v2","updated":"2024-12-19T09:30:05Z","published":"2024-08-12T12:09:25Z","title":"Langevin dynamics for high-dimensional optimization: the case of\n  multi-spiked tensor PCA","summary":"  We study nonconvex optimization in high dimensions through Langevin dynamics,\nfocusing on the multi-spiked tensor PCA problem. This tensor estimation problem\ninvolves recovering $r$ hidden signal vectors (spikes) from noisy Gaussian\ntensor observations using maximum likelihood estimation. We study the number of\nsamples required for Langevin dynamics to efficiently recover the spikes and\ndetermine the necessary separation condition on the signal-to-noise ratios\n(SNRs) for exact recovery, distinguishing the cases $p \\ge 3$ and $p=2$, where\n$p$ denotes the order of the tensor. In particular, we show that the sample\ncomplexity required for recovering the spike associated with the largest SNR\nmatches the well-known algorithmic threshold for the single-spike case, while\nthis threshold degrades when recovering all $r$ spikes. As a key step, we\nprovide a detailed characterization of the trajectory and interactions of\nlow-dimensional projections that capture the high-dimensional dynamics.\n","authors":["Gérard Ben Arous","Cédric Gerbelot","Vanessa Piccolo"],"pdf_url":"https://arxiv.org/pdf/2408.06401v2.pdf","comment":"65 pages"},{"id":"http://arxiv.org/abs/2412.06926v3","updated":"2024-12-19T09:24:39Z","published":"2024-12-09T19:11:54Z","title":"When Every Token Counts: Optimal Segmentation for Low-Resource Language\n  Models","summary":"  Traditional greedy tokenization methods have been a critical step in Natural\nLanguage Processing (NLP), influencing how text is converted into tokens and\ndirectly impacting model performance. While subword tokenizers like Byte-Pair\nEncoding (BPE) are widely used, questions remain about their optimality across\nmodel scales and languages. In this work, we demonstrate through extensive\nexperiments that an optimal BPE configuration significantly reduces token count\ncompared to greedy segmentation, yielding improvements in token-saving\npercentages and performance benefits, particularly for smaller models. We\nevaluate tokenization performance across various intrinsic and extrinsic tasks,\nincluding generation and classification. Our findings suggest that\ncompression-optimized tokenization strategies could provide substantial\nadvantages for multilingual and low-resource language applications,\nhighlighting a promising direction for further research and inclusive NLP.\n","authors":["Bharath Raj S","Garvit Suri","Vikrant Dewangan","Raghav Sonavane"],"pdf_url":"https://arxiv.org/pdf/2412.06926v3.pdf","comment":"LoResLM @ COLING 2025"},{"id":"http://arxiv.org/abs/2412.14668v1","updated":"2024-12-19T09:20:27Z","published":"2024-12-19T09:20:27Z","title":"LoLaFL: Low-Latency Federated Learning via Forward-only Propagation","summary":"  Federated learning (FL) has emerged as a widely adopted paradigm for enabling\nedge learning with distributed data while ensuring data privacy. However, the\ntraditional FL with deep neural networks trained via backpropagation can hardly\nmeet the low-latency learning requirements in the sixth generation (6G) mobile\nnetworks. This challenge mainly arises from the high-dimensional model\nparameters to be transmitted and the numerous rounds of communication required\nfor convergence due to the inherent randomness of the training process. To\naddress this issue, we adopt the state-of-the-art principle of maximal coding\nrate reduction to learn linear discriminative features and extend the resultant\nwhite-box neural network into FL, yielding the novel framework of Low-Latency\nFederated Learning (LoLaFL) via forward-only propagation. LoLaFL enables\nlayer-wise transmissions and aggregation with significantly fewer communication\nrounds, thereby considerably reducing latency. Additionally, we propose two\n\\emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on\nthe proof that the optimal NN parameter aggregation in LoLaFL should be\nharmonic-mean-like. The second scheme further exploits the low-rank structures\nof the features and transmits the low-rank-approximated covariance matrices of\nfeatures to achieve additional latency reduction. Theoretic analysis and\nexperiments are conducted to evaluate the performance of LoLaFL. In comparison\nwith traditional FL, the two nonlinear aggregation schemes for LoLaFL can\nachieve reductions in latency of over 91\\% and 98\\%, respectively, while\nmaintaining comparable accuracies.\n","authors":["Jierui Zhang","Jianhao Huang","Kaibin Huang"],"pdf_url":"https://arxiv.org/pdf/2412.14668v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.14660v1","updated":"2024-12-19T09:10:07Z","published":"2024-12-19T09:10:07Z","title":"Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.\n","authors":["Zijun Chen","Wenbo Hu","Guande He","Zhijie Deng","Zheng Zhang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.14660v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2412.14655v1","updated":"2024-12-19T09:06:39Z","published":"2024-12-19T09:06:39Z","title":"Trainable Adaptive Activation Function Structure (TAAFS) Enhances Neural\n  Network Force Field Performance with Only Dozens of Additional Parameters","summary":"  At the heart of neural network force fields (NNFFs) is the architecture of\nneural networks, where the capacity to model complex interactions is typically\nenhanced through widening or deepening multilayer perceptrons (MLPs) or by\nincreasing layers of graph neural networks (GNNs). These enhancements, while\nimproving the model's performance, often come at the cost of a substantial\nincrease in the number of parameters. By applying the Trainable Adaptive\nActivation Function Structure (TAAFS), we introduce a method that selects\ndistinct mathematical formulations for non-linear activations, thereby\nincreasing the precision of NNFFs with an insignificant addition to the\nparameter count. In this study, we integrate TAAFS into a variety of neural\nnetwork models, resulting in observed accuracy improvements, and further\nvalidate these enhancements through molecular dynamics (MD) simulations using\nDeepMD.\n","authors":["Enji Li"],"pdf_url":"https://arxiv.org/pdf/2412.14655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15031v2","updated":"2024-12-19T09:00:34Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.14650v1","updated":"2024-12-19T08:59:49Z","published":"2024-12-19T08:59:49Z","title":"Permutation recovery of spikes in noisy high-dimensional tensor\n  estimation","summary":"  We study the dynamics of gradient flow in high dimensions for the\nmulti-spiked tensor problem, where the goal is to estimate $r$ unknown signal\nvectors (spikes) from noisy Gaussian tensor observations. Specifically, we\nanalyze the maximum likelihood estimation procedure, which involves optimizing\na highly nonconvex random function. We determine the sample complexity required\nfor gradient flow to efficiently recover all spikes, without imposing any\nassumptions on the separation of the signal-to-noise ratios (SNRs). More\nprecisely, our results provide the sample complexity required to guarantee\nrecovery of the spikes up to a permutation. Our work builds on our companion\npaper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and\ndetermines the sample complexity and separation conditions for the SNRs\nnecessary for ensuring exact recovery of the spikes (where the recovered\npermutation matches the identity). During the recovery process, the\ncorrelations between the estimators and the hidden vectors increase in a\nsequential manner. The order in which these correlations become significant\ndepends on their initial values and the corresponding SNRs, which ultimately\ndetermines the permutation of the recovered spikes.\n","authors":["Gérard Ben Arous","CĆedric Gerbelot","Vanessa Piccolo"],"pdf_url":"https://arxiv.org/pdf/2412.14650v1.pdf","comment":"29 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2408.06401"},{"id":"http://arxiv.org/abs/2406.16606v2","updated":"2024-12-19T08:53:52Z","published":"2024-06-24T12:46:16Z","title":"Cherry on the Cake: Fairness is NOT an Optimization Problem","summary":"  In Fair AI literature, the practice of maliciously creating unfair models\nthat nevertheless satisfy fairness constraints is known as \"cherry-picking\". A\ncherry-picking model is a model that makes mistakes on purpose, selecting bad\nindividuals from a minority class instead of better candidates from the same\nminority. The model literally cherry-picks whom to select to superficially meet\nthe fairness constraints while making minimal changes to the unfair model. This\npractice has been described as \"blatantly unfair\" and has a negative impact on\nalready marginalized communities, undermining the intended purpose of fairness\nmeasures specifically designed to protect these communities. A common\nassumption is that cherry-picking arises solely from malicious intent and that\nmodels designed only to optimize fairness metrics would avoid this behavior. We\nshow that this is not the case: models optimized to minimize fairness metrics\nwhile maximizing performance are often forced to cherry-pick to some degree. In\nother words, cherry-picking might be an inevitable outcome of the optimization\nprocess itself. To demonstrate this, we use tools from fair cake-cutting, a\nmathematical subfield that studies the problem of fairly dividing a resource,\nreferred to as the \"cake,\" among a number of participants. This concept is\nconnected to supervised multi-label classification: any dataset can be thought\nof as a cake that needs to be distributed among different labels, and the model\nis the function that divides the cake. We adapt these classical results for\nmachine learning and demonstrate how this connection can be prolifically used\nfor fairness and classification in general.\n","authors":["Marco Favier","Toon Calders"],"pdf_url":"https://arxiv.org/pdf/2406.16606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14640v1","updated":"2024-12-19T08:51:01Z","published":"2024-12-19T08:51:01Z","title":"Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention\n  for Fine-Grained Few-Shot Learning","summary":"  Few-shot, fine-grained classification in computer vision poses significant\nchallenges due to the need to differentiate subtle class distinctions with\nlimited data. This paper presents a novel method that enhances the Contrastive\nLanguage-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided\nby real-time visual inputs. Unlike existing techniques such as Context\nOptimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by\nstatic prompts or visual token reliance, the proposed approach leverages a\ncross-attention mechanism to dynamically refine text prompts for the image at\nhand. This enables an image-specific alignment of textual features with image\npatches extracted from the Vision Transformer, making the model more effective\nfor datasets with high intra-class variance and low inter-class differences.\nThe method is evaluated on several datasets, including CUBirds, Oxford Flowers,\nand FGVC Aircraft, showing significant performance gains over static prompt\ntuning approaches. To ensure these performance gains translate into trustworthy\npredictions, we integrate Monte-Carlo Dropout in our approach to improve the\nreliability of the model predictions and uncertainty estimates. This\nintegration provides valuable insights into the model's predictive confidence,\nhelping to identify when predictions can be trusted and when additional\nverification is necessary. This dynamic approach offers a robust solution,\nadvancing the state-of-the-art for few-shot fine-grained classification.\n","authors":["Eric Brouwer","Jan Erik van Woerden","Gertjan Burghouts","Matias Valedenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2412.14640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14492v2","updated":"2024-12-19T08:46:02Z","published":"2024-05-23T12:25:22Z","title":"Iterative Methods for Full-Scale Gaussian Process Approximations for\n  Large Spatial Data","summary":"  Gaussian processes are flexible probabilistic regression models which are\nwidely used in statistics and machine learning. However, a drawback is their\nlimited scalability to large data sets. To alleviate this, we consider\nfull-scale approximations (FSAs) that combine predictive process methods and\ncovariance tapering, thus approximating both global and local structures. We\nshow how iterative methods can be used to reduce the computational costs for\ncalculating likelihoods, gradients, and predictive distributions with FSAs. We\nintroduce a novel preconditioner and show that it accelerates the conjugate\ngradient method's convergence speed and mitigates its sensitivity with respect\nto the FSA parameters and the eigenvalue structure of the original covariance\nmatrix, and we demonstrate empirically that it outperforms a state-of-the-art\npivoted Cholesky preconditioner. Further, we present a novel, accurate, and\nfast way to calculate predictive variances relying on stochastic estimations\nand iterative methods. In both simulated and real-world data experiments, we\nfind that our proposed methodology achieves the same accuracy as Cholesky-based\ncomputations with a substantial reduction in computational time. Finally, we\nalso compare different approaches for determining inducing points in predictive\nprocess and FSA models. All methods are implemented in a free C++ software\nlibrary with high-level Python and R packages.\n","authors":["Tim Gyger","Reinhard Furrer","Fabio Sigrist"],"pdf_url":"https://arxiv.org/pdf/2405.14492v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16149v3","updated":"2024-12-19T08:34:15Z","published":"2024-03-24T13:43:43Z","title":"Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a\n  Comprehensive Survey","summary":"  The Consumer Internet of Things (CIoT), a notable segment within the IoT\ndomain, involves the integration of IoT technology into consumer electronics\nand devices, such as smart homes and smart wearables. Compared to traditional\nIoT fields, CIoT differs notably in target users, product types, and design\napproaches. While offering convenience to users, it also raises new security\nand privacy concerns. Network traffic analysis, a widely used technique in the\nsecurity community, has been extensively applied to investigate these concerns\nabout CIoT. Compared to network traffic analysis in other fields such as mobile\napps and websites, CIoT presents unique characteristics, introducing new\nchallenges and research opportunities. Researchers have made significant\ncontributions in this area. To aid researchers in understanding the application\nof traffic analysis tools for studying CIoT security and privacy risks, this\nsurvey reviews 303 publications on traffic analysis within the CIoT security\nand privacy domain from January 2018 to June 2024, focusing on three research\nquestions. Our work: 1) outlines the CIoT traffic analysis process and\nhighlights its differences from general network traffic analysis. 2) summarizes\nand classifies existing research into four categories according to its\napplication objectives: device fingerprinting, user activity inference,\nmalicious traffic detection, and measurement. 3) explores emerging challenges\nand potential future research directions based on each step of the CIoT traffic\nanalysis process. This will provide new insights to the community and guide the\nindustry towards safer product designs.\n","authors":["Yan Jia","Yuxin Song","Zihou Liu","Qingyin Tan","Yang Song","Yu Zhang","Zheli Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16149v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14629v1","updated":"2024-12-19T08:31:42Z","published":"2024-12-19T08:31:42Z","title":"Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix\n  Factorization","summary":"  Robust Principal Component Analysis (RPCA) is a fundamental technique for\ndecomposing data into low-rank and sparse components, which plays a critical\nrole for applications such as image processing and anomaly detection.\nTraditional RPCA methods commonly use $\\ell_1$ norm regularization to enforce\nsparsity, but this approach can introduce bias and result in suboptimal\nestimates, particularly in the presence of significant noise or outliers.\nNon-convex regularization methods have been proposed to mitigate these\nchallenges, but they tend to be complex to optimize and sensitive to initial\nconditions, leading to potential instability in solutions. To overcome these\nchallenges, in this paper, we propose a novel RPCA model that integrates\nadaptive weighted least squares (AWLS) and low-rank matrix factorization\n(LRMF). The model employs a {self-attention-inspired} mechanism in its weight\nupdate process, allowing the weight matrix to dynamically adjust and emphasize\nsignificant components during each iteration. By employing a weighted F-norm\nfor the sparse component, our method effectively reduces bias while simplifying\nthe computational process compared to traditional $\\ell_1$-norm-based methods.\nWe use an alternating minimization algorithm, where each subproblem has an\nexplicit solution, thereby improving computational efficiency. Despite its\nsimplicity, numerical experiments demonstrate that our method outperforms\nexisting non-convex regularization approaches, offering superior performance\nand stability, as well as enhanced accuracy and robustness in practical\napplications.\n","authors":["Kexin Li","You-wei Wen","Xu Xiao","Mingchao Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14628v1","updated":"2024-12-19T08:30:54Z","published":"2024-12-19T08:30:54Z","title":"Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models","summary":"  Diffusion Models (DM) have democratized AI image generation through an\niterative denoising process. Quantization is a major technique to alleviate the\ninference cost and reduce the size of DM denoiser networks. However, as\ndenoisers evolve from variants of convolutional U-Nets toward newer Transformer\narchitectures, it is of growing importance to understand the quantization\nsensitivity of different weight layers, operations and architecture types to\nperformance. In this work, we address this challenge with Qua$^2$SeDiMo, a\nmixed-precision Post-Training Quantization framework that generates explainable\ninsights on the cost-effectiveness of various model weight quantization methods\nfor different denoiser operation types and block structures. We leverage these\ninsights to make high-quality mixed-precision quantization decisions for a\nmyriad of diffusion models ranging from foundational U-Nets to state-of-the-art\nTransformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,\n3.65-bit and 3.7-bit weight quantization on PixArt-${\\alpha}$,\nPixArt-${\\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our\nweight-quantization configurations with 6-bit activation quantization and\noutperform existing approaches in terms of quantitative metrics and generative\nimage quality.\n","authors":["Keith G. Mills","Mohammad Salameh","Ruichen Chen","Negar Hassanpour","Wei Lu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2412.14628v1.pdf","comment":"AAAI 2025; version includes supplementary material; 22 Pages, 18\n  Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2302.09526v4","updated":"2024-12-19T08:22:30Z","published":"2023-02-19T09:55:18Z","title":"Mixed Semi-Supervised Generalized-Linear-Regression with Applications to\n  Deep-Learning and Interpolators","summary":"  We present a methodology for using unlabeled data to design semi supervised\nlearning (SSL) methods that improve the prediction performance of supervised\nlearning for regression tasks. The main idea is to design different mechanisms\nfor integrating the unlabeled data, and include in each of them a mixing\nparameter $\\alpha$, controlling the weight given to the unlabeled data.\nFocusing on Generalized Linear Models (GLM) and linear interpolators classes of\nmodels, we analyze the characteristics of different mixing mechanisms, and\nprove that in all cases, it is invariably beneficial to integrate the unlabeled\ndata with some nonzero mixing ratio $\\alpha>0$, in terms of predictive\nperformance. Moreover, we provide a rigorous framework to estimate the best\nmixing ratio $\\alpha^*$ where mixed SSL delivers the best predictive\nperformance, while using the labeled and unlabeled data on hand.\n  The effectiveness of our methodology in delivering substantial improvement\ncompared to the standard supervised models, in a variety of settings, is\ndemonstrated empirically through extensive simulation, in a manner that\nsupports the theoretical analysis. We also demonstrate the applicability of our\nmethodology (with some intuitive modifications) to improve more complex models,\nsuch as deep neural networks, in real-world regression tasks.\n","authors":["Oren Yuval","Saharon Rosset"],"pdf_url":"https://arxiv.org/pdf/2302.09526v4.pdf","comment":"58 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.14031v2","updated":"2024-12-19T08:21:15Z","published":"2024-12-18T16:51:47Z","title":"Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization\n  Perspective","summary":"  We analyze the convergence of Gauss-Newton dynamics for training neural\nnetworks with smooth activation functions. In the underparameterized regime,\nthe Gauss-Newton gradient flow induces a Riemannian gradient flow on a\nlow-dimensional, smooth, embedded submanifold of the Euclidean output space.\nUsing tools from Riemannian optimization, we prove \\emph{last-iterate}\nconvergence of the Riemannian gradient flow to the optimal in-class predictor\nat an \\emph{exponential rate} that is independent of the conditioning of the\nGram matrix, \\emph{without} requiring explicit regularization. We further\ncharacterize the critical impacts of the neural network scaling factor and the\ninitialization on the convergence behavior. In the overparameterized regime, we\nshow that the Levenberg-Marquardt dynamics with an appropriately chosen damping\nfactor yields robustness to ill-conditioned kernels, analogous to the\nunderparameterized regime. These findings demonstrate the potential of\nGauss-Newton methods for efficiently optimizing neural networks, particularly\nin ill-conditioned problems where kernel and Gram matrices have small singular\nvalues.\n","authors":["Semih Cayci"],"pdf_url":"https://arxiv.org/pdf/2412.14031v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14620v1","updated":"2024-12-19T08:13:20Z","published":"2024-12-19T08:13:20Z","title":"Continuous latent representations for modeling precipitation with deep\n  learning","summary":"  The sparse and spatio-temporally discontinuous nature of precipitation data\npresents significant challenges for simulation and statistical processing for\nbias correction and downscaling. These include incorrect representation of\nintermittency and extreme values (critical for hydrology applications), Gibbs\nphenomenon upon regridding, and lack of fine scales details. To address these\nchallenges, a common approach is to transform the precipitation variable\nnonlinearly into one that is more malleable. In this work, we explore how deep\nlearning can be used to generate a smooth, spatio-temporally continuous\nvariable as a proxy for simulation of precipitation data. We develop a normally\ndistributed field called pseudo-precipitation (PP) as an alternative for\nsimulating precipitation. The practical applicability of this variable is\ninvestigated by applying it for downscaling precipitation from \\(1\\degree\\)\n(\\(\\sim\\) 100 km) to \\(0.25\\degree\\) (\\(\\sim\\) 25 km).\n","authors":["Gokul Radhakrishnan","Rahul Sundar","Nishant Parashar","Antoine Blanchard","Daiwei Wang","Boyko Dodov"],"pdf_url":"https://arxiv.org/pdf/2412.14620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14619v1","updated":"2024-12-19T08:11:42Z","published":"2024-12-19T08:11:42Z","title":"Pitfalls of topology-aware image segmentation","summary":"  Topological correctness, i.e., the preservation of structural integrity and\nspecific characteristics of shape, is a fundamental requirement for medical\nimaging tasks, such as neuron or vessel segmentation. Despite the recent surge\nin topology-aware methods addressing this challenge, their real-world\napplicability is hindered by flawed benchmarking practices. In this paper, we\nidentify critical pitfalls in model evaluation that include inadequate\nconnectivity choices, overlooked topological artifacts in ground truth\nannotations, and inappropriate use of evaluation metrics. Through detailed\nempirical analysis, we uncover these issues' profound impact on the evaluation\nand ranking of segmentation methods. Drawing from our findings, we propose a\nset of actionable recommendations to establish fair and robust evaluation\nstandards for topology-aware medical image segmentation methods.\n","authors":["Alexander H. Berger","Laurin Lux","Alexander Weers","Martin Menten","Daniel Rueckert","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2412.14619v1.pdf","comment":"Code is available at\n  https://github.com/AlexanderHBerger/topo-pitfalls"},{"id":"http://arxiv.org/abs/2412.14602v1","updated":"2024-12-19T07:48:14Z","published":"2024-12-19T07:48:14Z","title":"Towards Scalable and Deep Graph Neural Networks via Noise Masking","summary":"  In recent years, Graph Neural Networks (GNNs) have achieved remarkable\nsuccess in many graph mining tasks. However, scaling them to large graphs is\nchallenging due to the high computational and storage costs of repeated feature\npropagation and non-linear transformation during training. One commonly\nemployed approach to address this challenge is model-simplification, which only\nexecutes the Propagation (P) once in the pre-processing, and Combine (C) these\nreceptive fields in different ways and then feed them into a simple model for\nbetter performance. Despite their high predictive performance and scalability,\nthese methods still face two limitations. First, existing approaches mainly\nfocus on exploring different C methods from the model perspective, neglecting\nthe crucial problem of performance degradation with increasing P depth from the\ndata-centric perspective, known as the over-smoothing problem. Second,\npre-processing overhead takes up most of the end-to-end processing time,\nespecially for large-scale graphs. To address these limitations, we present\nrandom walk with noise masking (RMask), a plug-and-play module compatible with\nthe existing model-simplification works. This module enables the exploration of\ndeeper GNNs while preserving their scalability. Unlike the previous\nmodel-simplification works, we focus on continuous P and found that the noise\nexisting inside each P is the cause of the over-smoothing issue, and use the\nefficient masking mechanism to eliminate them. Experimental results on six\nreal-world datasets demonstrate that model-simplification works equipped with\nRMask yield superior performance compared to their original version and can\nmake a good trade-off between accuracy and efficiency.\n","authors":["Yuxuan Liang","Wentao Zhang","Zeang Sheng","Ling Yang","Quanqing Xu","Jiawei Jiang","Yunhai Tong","Bin Cu"],"pdf_url":"https://arxiv.org/pdf/2412.14602v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14599v1","updated":"2024-12-19T07:42:07Z","published":"2024-12-19T07:42:07Z","title":"Fast inverse lithography based on a model-driven block stacking\n  convolutional neural network","summary":"  In the realm of lithography, Optical Proximity Correction (OPC) is a crucial\nresolution enhancement technique that optimizes the transmission function of\nphotomasks on a pixel-based to effectively counter Optical Proximity Effects\n(OPE). However, conventional pixel-based OPC methods often generate patterns\nthat pose manufacturing challenges, thereby leading to the increased cost in\npractical scenarios. This paper presents a novel inverse lithographic approach\nto OPC, employing a model-driven, block stacking deep learning framework that\nexpedites the generation of masks conducive to manufacturing. This method is\nfounded on vector lithography modelling and streamlines the training process by\neliminating the requirement for extensive labeled datasets. Furthermore,\ndiversity of mask patterns is enhanced by employing a wave function collapse\nalgorithm, which facilitates the random generation of a multitude of target\npatterns, therefore significantly expanding the range of mask paradigm.\nNumerical experiments have substantiated the efficacy of the proposed\nend-to-end approach, highlighting its superior capability to manage mask\ncomplexity within the context of advanced OPC lithography. This advancement is\nanticipated to enhance the feasibility and economic viability of OPC technology\nwithin actual manufacturing environments.\n","authors":["Ruixiang Chen","Yang Zhao","Haoqin Li","Rui Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14599v1.pdf","comment":"21 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.04979v3","updated":"2024-12-19T07:33:48Z","published":"2024-01-10T07:51:02Z","title":"DualDynamics: Synergizing Implicit and Explicit Methods for Robust\n  Irregular Time Series Analysis","summary":"  Real-world time series analysis faces significant challenges when dealing\nwith irregular and incomplete data. While Neural Differential Equation (NDE)\nbased methods have shown promise, they struggle with limited expressiveness,\nscalability issues, and stability concerns. Conversely, Neural Flows offer\nstability but falter with irregular data. We introduce 'DualDynamics', a novel\nframework that synergistically combines NDE-based method and Neural Flow-based\nmethod. This approach enhances expressive power while balancing computational\ndemands, addressing critical limitations of existing techniques. We demonstrate\nDualDynamics' effectiveness across diverse tasks: classification of robustness\nto dataset shift, irregularly-sampled series analysis, interpolation of missing\ndata, and forecasting with partial observations. Our results show consistent\noutperformance over state-of-the-art methods, indicating DualDynamics'\npotential to advance irregular time series analysis significantly.\n","authors":["YongKyung Oh","Dongyoung Lim","Sungil Kim"],"pdf_url":"https://arxiv.org/pdf/2401.04979v3.pdf","comment":"Published at the 39th Annual AAAI Conference on Artificial\n  Intelligence (AAAI 2025)"},{"id":"http://arxiv.org/abs/2412.14596v1","updated":"2024-12-19T07:31:40Z","published":"2024-12-19T07:31:40Z","title":"LDP: Generalizing to Multilingual Visual Information Extraction by\n  Language Decoupled Pretraining","summary":"  Visual Information Extraction (VIE) plays a crucial role in the comprehension\nof semi-structured documents, and several pre-trained models have been\ndeveloped to enhance performance. However, most of these works are monolingual\n(usually English). Due to the extremely unbalanced quantity and quality of\npre-training corpora between English and other languages, few works can extend\nto non-English scenarios. In this paper, we conduct systematic experiments to\nshow that vision and layout modality hold invariance among images with\ndifferent languages. If decoupling language bias from document images, a\nvision-layout-based model can achieve impressive cross-lingual generalization.\nAccordingly, we present a simple but effective multilingual training paradigm\nLDP (Language Decoupled Pre-training) for better utilization of monolingual\npre-training data. Our proposed model LDM (Language Decoupled Model) is first\npre-trained on the language-independent data, where the language knowledge is\ndecoupled by a diffusion model, and then the LDM is fine-tuned on the\ndownstream languages. Extensive experiments show that the LDM outperformed all\nSOTA multilingual pre-trained models, and also maintains competitiveness on\ndownstream monolingual/English benchmarks.\n","authors":["Huawen Shen","Gengluo Li","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14596v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2409.05929v2","updated":"2024-12-19T07:31:18Z","published":"2024-09-09T10:40:50Z","title":"Alt-MoE: Multimodal Alignment via Alternating Optimization of\n  Multi-directional MoE with Unimodal Models","summary":"  Recent Large Multi-Modal Models (LMMs) have made significant advancements in\nmulti-modal alignment by employing lightweight connection modules to facilitate\nthe representation and fusion of knowledge from existing pre-trained uni-modal\nmodels. However, these methods still rely on modality-specific and\ndirection-specific connectors, leading to compartmentalized knowledge\nrepresentations and reduced computational efficiency, which limits the model's\nability to form unified multi-modal representations. To address these issues,\nwe introduce a novel training framework, Alt-MoE, which employs the Mixture of\nExperts (MoE) as a unified multi-directional connector across modalities, and\nemploys a multi-step sequential alternating unidirectional alignment strategy,\nwhich converges to bidirectional alignment over iterations. The extensive\nempirical studies revealed the following key points: 1) Alt-MoE achieves\ncompetitive results by integrating diverse knowledge representations from\nuni-modal models. This approach seamlessly fuses the specialized expertise of\nexisting high-performance uni-modal models, effectively synthesizing their\ndomain-specific knowledge into a cohesive multi-modal representation. 2)\nAlt-MoE efficiently scales to new tasks and modalities without altering its\nmodel architecture or training strategy. Furthermore, Alt-MoE operates in\nlatent space, supporting vector pre-storage and real-time retrieval via\nlightweight multi-directional MoE, thereby facilitating massive data\nprocessing. Our methodology has been validated on several well-performing\nuni-modal models (LLAMA3, Qwen2, and DINOv2), achieving competitive results on\na wide range of downstream tasks and datasets.\n","authors":["Hongyang Lei","Xiaolong Cheng","Dan Wang","Kun Fan","Qi Qin","Huazhen Huang","Yetao Wu","Qingqing Gu","Zhonglin Jiang","Yong Chen","Luo Ji"],"pdf_url":"https://arxiv.org/pdf/2409.05929v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14592v1","updated":"2024-12-19T07:23:17Z","published":"2024-12-19T07:23:17Z","title":"Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,\n  and Internal Properties","summary":"  Object anomaly detection is essential for industrial quality inspection, yet\ntraditional single-sensor methods face critical limitations. They fail to\ncapture the wide range of anomaly types, as single sensors are often\nconstrained to either external appearance, geometric structure, or internal\nproperties. To overcome these challenges, we introduce MulSen-AD, the first\nhigh-resolution, multi-sensor anomaly detection dataset tailored for industrial\napplications. MulSen-AD unifies data from RGB cameras, laser scanners, and\nlock-in infrared thermography, effectively capturing external appearance,\ngeometric deformations, and internal defects. The dataset spans 15 industrial\nproducts with diverse, real-world anomalies. We also present MulSen-AD Bench, a\nbenchmark designed to evaluate multi-sensor methods, and propose\nMulSen-TripleAD, a decision-level fusion algorithm that integrates these three\nmodalities for robust, unsupervised object anomaly detection. Our experiments\ndemonstrate that multi-sensor fusion substantially outperforms single-sensor\napproaches, achieving 96.1% AUROC in object-level detection accuracy. These\nresults highlight the importance of integrating multi-sensor data for\ncomprehensive industrial anomaly detection.\n","authors":["Wenqiao Li","Bozhong Zheng","Xiaohao Xu","Jinye Gan","Fading Lu","Xiang Li","Na Ni","Zheng Tian","Xiaonan Huang","Shenghua Gao","Yingna Wu"],"pdf_url":"https://arxiv.org/pdf/2412.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14590v1","updated":"2024-12-19T07:15:15Z","published":"2024-12-19T07:15:15Z","title":"MixLLM: LLM Quantization with Global Mixed-precision between\n  Output-features and Highly-efficient System Design","summary":"  Quantization has become one of the most effective methodologies to compress\nLLMs into smaller size. However, the existing quantization solutions still show\nlimitations of either non-negligible accuracy drop or system inefficiency. In\nthis paper, we make a comprehensive analysis of the general quantization\nprinciples on their effect to the triangle of accuracy, memory consumption and\nsystem efficiency. We propose MixLLM that explores the new optimization space\nof mixed-precision quantization between output features based on the insight\nthat different output features matter differently in the model. MixLLM\nidentifies the output features with high salience in the global view rather\nthan within each single layer, effectively assigning the larger bit-width to\noutput features that need it most to achieve good accuracy with low memory\nconsumption. We present the sweet spot of quantization configuration of\nalgorithm-system co-design that leads to high accuracy and system efficiency.\nTo address the system challenge, we design the two-step dequantization to make\nuse of the int8 Tensor Core easily and fast data type conversion to reduce\ndequantization overhead significantly, and present the software pipeline to\noverlap the memory access, dequantization and the MatMul to the best. Extensive\nexperiments show that with only 10% more bits, the PPL increasement can be\nreduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on\naverage MMLU-Pro improves by 0.93 over the SOTA of three popular models. In\naddition to its superior accuracy, MixLLM also achieves state-of-the-art system\nefficiency.\n","authors":["Zhen Zheng","Xiaonan Song","Chuanjie Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14590v1.pdf","comment":"The code will be released in the future"},{"id":"http://arxiv.org/abs/2407.02419v3","updated":"2024-12-19T07:07:51Z","published":"2024-07-02T16:44:14Z","title":"Quantum Curriculum Learning","summary":"  Quantum machine learning (QML) requires significant quantum resources to\naddress practical real-world problems. When the underlying quantum information\nexhibits hierarchical structures in the data, limitations persist in training\ncomplexity and generalization. Research should prioritize both the efficient\ndesign of quantum architectures and the development of learning strategies to\noptimize resource usage. We propose a framework called quantum curriculum\nlearning (Q-CurL) for quantum data, where the curriculum introduces simpler\ntasks or data to the learning model before progressing to more challenging\nones. Q-CurL exhibits robustness to noise and data limitations, which is\nparticularly relevant for current and near-term noisy intermediate-scale\nquantum devices. We achieve this through a curriculum design based on quantum\ndata density ratios and a dynamic learning schedule that prioritizes the most\ninformative quantum data. Empirical evidence shows that Q-CurL significantly\nenhances training convergence and generalization for unitary learning and\nimproves the robustness of quantum phase recognition tasks. Q-CurL is effective\nwith broad physical learning applications in condensed matter physics and\nquantum chemistry.\n","authors":["Quoc Hoan Tran","Yasuhiro Endo","Hirotaka Oshima"],"pdf_url":"https://arxiv.org/pdf/2407.02419v3.pdf","comment":"main 6 pages, supplementary materials 11 pages (update the\n  supplementary materials with more explanation on data-based Q-CurL)"},{"id":"http://arxiv.org/abs/2302.03390v5","updated":"2024-12-19T07:05:03Z","published":"2023-02-07T10:51:53Z","title":"Learning Discretized Neural Networks under Ricci Flow","summary":"  In this paper, we study Discretized Neural Networks (DNNs) composed of\nlow-precision weights and activations, which suffer from either infinite or\nzero gradients due to the non-differentiable discrete function during training.\nMost training-based DNNs in such scenarios employ the standard Straight-Through\nEstimator (STE) to approximate the gradient w.r.t. discrete values. However,\nthe use of STE introduces the problem of gradient mismatch, arising from\nperturbations in the approximated gradient. To address this problem, this paper\nreveals that this mismatch can be interpreted as a metric perturbation in a\nRiemannian manifold, viewed through the lens of duality theory. Building on\ninformation geometry, we construct the Linearly Nearly Euclidean (LNE) manifold\nfor DNNs, providing a background for addressing perturbations. By introducing a\npartial differential equation on metrics, i.e., the Ricci flow, we establish\nthe dynamical stability and convergence of the LNE metric with the $L^2$-norm\nperturbation. In contrast to previous perturbation theories with convergence\nrates in fractional powers, the metric perturbation under the Ricci flow\nexhibits exponential decay in the LNE manifold. Experimental results across\nvarious datasets demonstrate that our method achieves superior and more stable\nperformance for DNNs compared to other representative training-based methods.\n","authors":["Jun Chen","Hanwen Chen","Mengmeng Wang","Guang Dai","Ivor W. Tsang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2302.03390v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.00284v3","updated":"2024-12-19T06:52:07Z","published":"2023-02-01T07:31:25Z","title":"Selective Uncertainty Propagation in Offline RL","summary":"  We consider the finite-horizon offline reinforcement learning (RL) setting,\nand are motivated by the challenge of learning the policy at any step h in\ndynamic programming (DP) algorithms. To learn this, it is sufficient to\nevaluate the treatment effect of deviating from the behavioral policy at step h\nafter having optimized the policy for all future steps. Since the policy at any\nstep can affect next-state distributions, the related distributional shift\nchallenges can make this problem far more statistically hard than estimating\nsuch treatment effects in the stochastic contextual bandit setting. However,\nthe hardness of many real-world RL instances lies between the two regimes. We\ndevelop a flexible and general method called selective uncertainty propagation\nfor confidence interval construction that adapts to the hardness of the\nassociated distribution shift challenges. We show benefits of our approach on\ntoy environments and demonstrate the benefits of these techniques for offline\npolicy learning.\n","authors":["Sanath Kumar Krishnamurthy","Tanmay Gangwani","Sumeet Katariya","Branislav Kveton","Shrey Modi","Anshuka Rangi"],"pdf_url":"https://arxiv.org/pdf/2302.00284v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07728v3","updated":"2024-12-19T06:51:17Z","published":"2024-03-12T15:07:20Z","title":"CAP: A General Algorithm for Online Selective Conformal Prediction with\n  FCR Control","summary":"  We study the problem of post-selection predictive inference in an online\nfashion. To avoid devoting resources to unimportant units, a preliminary\nselection of the current individual before reporting its prediction interval is\ncommon and meaningful in online predictive tasks. Since the online selection\ncauses a temporal multiplicity in the selected prediction intervals, it is\nimportant to control the real-time false coverage-statement rate (FCR) which\nmeasures the overall miscoverage level. We develop a general framework named\nCAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on\nhistorical data to construct a calibration set if the current individual is\nselected and then outputs a conformal prediction interval for the unobserved\nlabel. We provide tractable procedures for constructing the calibration set for\npopular online selection rules. We proved that CAP can achieve an exact\nselection-conditional coverage guarantee in the finite-sample and\ndistribution-free regimes. To account for the distribution shift in online\ndata, we also embed CAP into some recent dynamic conformal prediction\nalgorithms and show that the proposed method can deliver long-run FCR control.\nNumerical results on both synthetic and real data corroborate that CAP can\neffectively control FCR around the target level and yield more narrowed\nprediction intervals over existing baselines across various settings.\n","authors":["Yajie Bao","Yuyang Huo","Haojie Ren","Changliang Zou"],"pdf_url":"https://arxiv.org/pdf/2403.07728v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14572v1","updated":"2024-12-19T06:42:57Z","published":"2024-12-19T06:42:57Z","title":"Accelerated Patient-Specific Calibration via Differentiable Hemodynamics\n  Simulations","summary":"  One of the goals of personalized medicine is to tailor diagnostics to\nindividual patients. Diagnostics are performed in practice by measuring\nquantities, called biomarkers, that indicate the existence and progress of a\ndisease. In common cardiovascular diseases, such as hypertension, biomarkers\nthat are closely related to the clinical representation of a patient can be\npredicted using computational models. Personalizing computational models\ntranslates to considering patient-specific flow conditions, for example, the\ncompliance of blood vessels that cannot be a priori known and quantities such\nas the patient geometry that can be measured using imaging. Therefore, a\npatient is identified by a set of measurable and nonmeasurable parameters\nneeded to well-define a computational model; else, the computational model is\nnot personalized, meaning it is prone to large prediction errors. Therefore, to\npersonalize a computational model, sufficient information needs to be extracted\nfrom the data. The current methods by which this is done are either\ninefficient, due to relying on slow-converging optimization methods, or hard to\ninterpret, due to using `black box` deep-learning algorithms. We propose a\npersonalized diagnostic procedure based on a differentiable 0D-1D Navier-Stokes\nreduced order model solver and fast parameter inference methods that take\nadvantage of gradients through the solver. By providing a faster method for\nperforming parameter inference and sensitivity analysis through\ndifferentiability while maintaining the interpretability of well-understood\nmathematical models and numerical methods, the best of both worlds is combined.\nThe performance of the proposed solver is validated against a well-established\nprocess on different geometries, and different parameter inference processes\nare successfully performed.\n","authors":["Diego Renner","Georgios Kissas"],"pdf_url":"https://arxiv.org/pdf/2412.14572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14569v1","updated":"2024-12-19T06:40:21Z","published":"2024-12-19T06:40:21Z","title":"Global Spatio-Temporal Fusion-based Traffic Prediction Algorithm with\n  Anomaly Aware","summary":"  Traffic prediction is an indispensable component of urban planning and\ntraffic management. Achieving accurate traffic prediction hinges on the ability\nto capture the potential spatio-temporal relationships among road sensors.\nHowever, the majority of existing works focus on local short-term\nspatio-temporal correlations, failing to fully consider the interactions of\ndifferent sensors in the long-term state. In addition, these works do not\nanalyze the influences of anomalous factors, or have insufficient ability to\nextract personalized features of anomalous factors, which make them\nineffectively capture their spatio-temporal influences on traffic prediction.\nTo address the aforementioned issues, We propose a global spatio-temporal\nfusion-based traffic prediction algorithm that incorporates anomaly awareness.\nInitially, based on the designed anomaly detection network, we construct an\nefficient anomalous factors impacting module (AFIM), to evaluate the\nspatio-temporal impact of unexpected external events on traffic prediction.\nFurthermore, we propose a multi-scale spatio-temporal feature fusion module\n(MTSFFL) based on the transformer architecture, to obtain all possible both\nlong and short term correlations among different sensors in a wide-area traffic\nenvironment for accurate prediction of traffic flow. Finally, experiments are\nimplemented based on real-scenario public transportation datasets (PEMS04 and\nPEMS08) to demonstrate that our approach can achieve state-of-the-art\nperformance.\n","authors":["Chaoqun Liu","Xuanpeng Li","Chen Gong","Guangyu Li"],"pdf_url":"https://arxiv.org/pdf/2412.14569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21853v2","updated":"2024-12-19T06:39:24Z","published":"2024-10-29T08:28:23Z","title":"Learning Infinitesimal Generators of Continuous Symmetries from Data","summary":"  Exploiting symmetry inherent in data can significantly improve the sample\nefficiency of a learning procedure and the generalization of learned models.\nWhen data clearly reveals underlying symmetry, leveraging this symmetry can\nnaturally inform the design of model architectures or learning strategies. Yet,\nin numerous real-world scenarios, identifying the specific symmetry within a\ngiven data distribution often proves ambiguous. To tackle this, some existing\nworks learn symmetry in a data-driven manner, parameterizing and learning\nexpected symmetry through data. However, these methods often rely on explicit\nknowledge, such as pre-defined Lie groups, which are typically restricted to\nlinear or affine transformations. In this paper, we propose a novel symmetry\nlearning algorithm based on transformations defined with one-parameter groups,\ncontinuously parameterized transformations flowing along the directions of\nvector fields called infinitesimal generators. Our method is built upon minimal\ninductive biases, encompassing not only commonly utilized symmetries rooted in\nLie groups but also extending to symmetries derived from nonlinear generators.\nTo learn these symmetries, we introduce a notion of a validity score that\nexamine whether the transformed data is still valid for the given task. The\nvalidity score is designed to be fully differentiable and easily computable,\nenabling effective searches for transformations that achieve symmetries innate\nto the data. We apply our method mainly in two domains: image data and partial\ndifferential equations, and demonstrate its advantages. Our codes are available\nat \\url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.\n","authors":["Gyeonghoon Ko","Hyunsu Kim","Juho Lee"],"pdf_url":"https://arxiv.org/pdf/2410.21853v2.pdf","comment":"Neurips 2024"},{"id":"http://arxiv.org/abs/2412.14566v1","updated":"2024-12-19T06:35:54Z","published":"2024-12-19T06:35:54Z","title":"AIArena: A Blockchain-Based Decentralized AI Training Platform","summary":"  The rapid advancement of AI has underscored critical challenges in its\ndevelopment and implementation, largely due to centralized control by a few\nmajor corporations. This concentration of power intensifies biases within AI\nmodels, resulting from inadequate governance and oversight mechanisms.\nAdditionally, it limits public involvement and heightens concerns about the\nintegrity of model generation. Such monopolistic control over data and AI\noutputs threatens both innovation and fair data usage, as users inadvertently\ncontribute data that primarily benefits these corporations. In this work, we\npropose AIArena, a blockchain-based decentralized AI training platform designed\nto democratize AI development and alignment through on-chain incentive\nmechanisms. AIArena fosters an open and collaborative environment where\nparticipants can contribute models and computing resources. Its on-chain\nconsensus mechanism ensures fair rewards for participants based on their\ncontributions. We instantiate and implement AIArena on the public Base\nblockchain Sepolia testnet, and the evaluation results demonstrate the\nfeasibility of AIArena in real-world applications.\n","authors":["Zhipeng Wang","Rui Sun","Elizabeth Lui","Tuo Zhou","Yizhe Wen","Jiahao Sun"],"pdf_url":"https://arxiv.org/pdf/2412.14566v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.08282v2","updated":"2024-12-19T06:35:21Z","published":"2024-12-11T10:57:16Z","title":"How Does the Smoothness Approximation Method Facilitate Generalization\n  for Federated Adversarial Learning?","summary":"  Federated Adversarial Learning (FAL) is a robust framework for resisting\nadversarial attacks on federated learning. Although some FAL studies have\ndeveloped efficient algorithms, they primarily focus on convergence performance\nand overlook generalization. Generalization is crucial for evaluating algorithm\nperformance on unseen data. However, generalization analysis is more\nchallenging due to non-smooth adversarial loss functions. A common approach to\naddressing this issue is to leverage smoothness approximation. In this paper,\nwe develop algorithm stability measures to evaluate the generalization\nperformance of two popular FAL algorithms: \\textit{Vanilla FAL (VFAL)} and {\\it\nSlack FAL (SFAL)}, using three different smooth approximation methods: 1)\n\\textit{Surrogate Smoothness Approximation (SSA)}, (2) \\textit{Randomized\nSmoothness Approximation (RSA)}, and (3) \\textit{Over-Parameterized Smoothness\nApproximation (OPSA)}. Based on our in-depth analysis, we answer the question\nof how to properly set the smoothness approximation method to mitigate\ngeneralization error in FAL. Moreover, we identify RSA as the most effective\nmethod for reducing generalization error. In highly data-heterogeneous\nscenarios, we also recommend employing SFAL to mitigate the deterioration of\ngeneralization performance caused by heterogeneity. Based on our theoretical\nresults, we provide insights to help develop more efficient FAL algorithms,\nsuch as designing new metrics and dynamic aggregation rules to mitigate\nheterogeneity.\n","authors":["Wenjun Ding","Ying An","Lixing Chen","Shichao Kan","Fan Wu","Zhe Qu"],"pdf_url":"https://arxiv.org/pdf/2412.08282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11973v6","updated":"2024-12-19T06:29:38Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) and Task-agnostic\nIncremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning\n(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is\ndesigned to prevent overfitting when the data samples are scarce. Furthermore,\nthe sparse reuse of WSN weights is considered for Video Incremental Learning\n(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It\nenables compact encoding of videos and identifies reusable subnetworks across\nvarying bandwidths. We have integrated FSO into different architectural\nframeworks for continual learning, including VIL, TIL, and FSCIL. Our\ncomprehensive experiments demonstrate FSO's effectiveness, significantly\nimproving task performance at various convolutional representational levels.\nSpecifically, FSO enhances higher-layer performance in TIL and FSCIL and\nlower-layer performance in VIL.\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v6.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (T-PAMI)"},{"id":"http://arxiv.org/abs/2412.14561v1","updated":"2024-12-19T06:26:16Z","published":"2024-12-19T06:26:16Z","title":"GBRIP: Granular Ball Representation for Imbalanced Partial Label\n  Learning","summary":"  Partial label learning (PLL) is a complicated weakly supervised\nmulti-classification task compounded by class imbalance. Currently, existing\nmethods only rely on inter-class pseudo-labeling from inter-class features,\noften overlooking the significant impact of the intra-class imbalanced features\ncombined with the inter-class. To address these limitations, we introduce\nGranular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for\nimbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and\nmulti-center loss to construct a granular ball-based nfeature space through\nunsupervised learning, effectively capturing the feature distribution within\neach class. GBRIP mitigates the impact of confusing features by systematically\nrefining label disambiguation and estimating imbalance distributions. The novel\nmulti-center loss function enhances learning by emphasizing the relationships\nbetween samples and their respective centers within the granular balls.\nExtensive experiments on standard benchmarks demonstrate that GBRIP outperforms\nexisting state-of-the-art methods, offering a robust solution to the challenges\nof imbalanced PLL.\n","authors":["Jintao Huang","Yiu-ming Cheung","Chi-man Vong","Wenbin Qian"],"pdf_url":"https://arxiv.org/pdf/2412.14561v1.pdf","comment":"AAAI25"},{"id":"http://arxiv.org/abs/2403.10650v3","updated":"2024-12-19T06:25:45Z","published":"2024-03-15T19:35:10Z","title":"PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time\n  Adaptation","summary":"  Real-world vision models in dynamic environments face rapid shifts in domain\ndistributions, leading to decreased recognition performance. Using unlabeled\ntest data, continuous test-time adaptation (CTTA) directly adjusts a\npre-trained source discriminative model to these changing domains. A highly\neffective CTTA method involves applying layer-wise adaptive learning rates for\nselectively adapting pre-trained layers. However, it suffers from the poor\nestimation of domain shift and the inaccuracies arising from the pseudo-labels.\nThis work aims to overcome these limitations by identifying layers for\nadaptation via quantifying model prediction uncertainty without relying on\npseudo-labels. We utilize the magnitude of gradients as a metric, calculated by\nbackpropagating the KL divergence between the softmax output and a uniform\ndistribution, to select layers for further adaptation. Subsequently, for the\nparameters exclusively belonging to these selected layers, with the remaining\nones frozen, we evaluate their sensitivity to approximate the domain shift and\nadjust their learning rates accordingly. We conduct extensive image\nclassification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,\ndemonstrating the superior efficacy of our method compared to prior approaches.\n","authors":["Sarthak Kumar Maharana","Baoming Zhang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10650v3.pdf","comment":"AAAI 2025"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2412.15216v1","updated":"2024-12-19T18:59:58Z","published":"2024-12-19T18:59:58Z","title":"UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit\n  Consistency","summary":"  We propose an unsupervised model for instruction-based image editing that\neliminates the need for ground-truth edited images during training. Existing\nsupervised methods depend on datasets containing triplets of input image,\nedited image, and edit instruction. These are generated by either existing\nediting methods or human-annotations, which introduce biases and limit their\ngeneralization ability. Our method addresses these challenges by introducing a\nnovel editing mechanism called Cycle Edit Consistency (CEC), which applies\nforward and backward edits in one training step and enforces consistency in\nimage and attention spaces. This allows us to bypass the need for ground-truth\nedited images and unlock training for the first time on datasets comprising\neither real image-caption pairs or image-caption-edit triplets. We empirically\nshow that our unsupervised technique performs better across a broader range of\nedits with high fidelity and precision. By eliminating the need for\npre-existing datasets of triplets, reducing biases associated with supervised\nmethods, and proposing CEC, our work represents a significant advancement in\nunblocking scaling of instruction-based image editing.\n","authors":["Enis Simsar","Alessio Tonioni","Yongqin Xian","Thomas Hofmann","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2412.15216v1.pdf","comment":"Project page: https://enis.dev/uip2p/"},{"id":"http://arxiv.org/abs/2412.15215v1","updated":"2024-12-19T18:59:57Z","published":"2024-12-19T18:59:57Z","title":"EnvGS: Modeling View-Dependent Appearance with Environment Gaussian","summary":"  Reconstructing complex reflections in real-world scenes from 2D images is\nessential for achieving photorealistic novel view synthesis. Existing methods\nthat utilize environment maps to model reflections from distant lighting often\nstruggle with high-frequency reflection details and fail to account for\nnear-field reflections. In this work, we introduce EnvGS, a novel approach that\nemploys a set of Gaussian primitives as an explicit 3D representation for\ncapturing reflections of environments. These environment Gaussian primitives\nare incorporated with base Gaussian primitives to model the appearance of the\nwhole scene. To efficiently render these environment Gaussian primitives, we\ndeveloped a ray-tracing-based renderer that leverages the GPU's RT core for\nfast rendering. This allows us to jointly optimize our model for high-quality\nreconstruction while maintaining real-time rendering speeds. Results from\nmultiple real-world and synthetic datasets demonstrate that our method produces\nsignificantly more detailed reflections, achieving the best rendering quality\nin real-time novel view synthesis.\n","authors":["Tao Xie","Xi Chen","Zhen Xu","Yiman Xie","Yudong Jin","Yujun Shen","Sida Peng","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15215v1.pdf","comment":"Project page: https://zju3dv.github.io/envgs/"},{"id":"http://arxiv.org/abs/2412.15213v1","updated":"2024-12-19T18:59:56Z","published":"2024-12-19T18:59:56Z","title":"Flowing from Words to Pixels: A Framework for Cross-Modality Evolution","summary":"  Diffusion models, and their generalization, flow matching, have had a\nremarkable impact on the field of media generation. Here, the conventional\napproach is to learn the complex mapping from a simple source distribution of\nGaussian noise to the target media distribution. For cross-modal tasks such as\ntext-to-image generation, this same mapping from noise to image is learnt\nwhilst including a conditioning mechanism in the model. One key and thus far\nrelatively unexplored feature of flow matching is that, unlike Diffusion\nmodels, they are not constrained for the source distribution to be noise.\nHence, in this paper, we propose a paradigm shift, and ask the question of\nwhether we can instead train flow matching models to learn a direct mapping\nfrom the distribution of one modality to the distribution of another, thus\nobviating the need for both the noise distribution and conditioning mechanism.\nWe present a general and simple framework, CrossFlow, for cross-modal flow\nmatching. We show the importance of applying Variational Encoders to the input\ndata, and introduce a method to enable Classifier-free guidance. Surprisingly,\nfor text-to-image, CrossFlow with a vanilla transformer without cross attention\nslightly outperforms standard flow matching, and we show that it scales better\nwith training steps and model size, while also allowing for interesting latent\narithmetic which results in semantically meaningful edits in the output space.\nTo demonstrate the generalizability of our approach, we also show that\nCrossFlow is on par with or outperforms the state-of-the-art for various\ncross-modal / intra-modal mapping tasks, viz. image captioning, depth\nestimation, and image super-resolution. We hope this paper contributes to\naccelerating progress in cross-modal media generation.\n","authors":["Qihao Liu","Xi Yin","Alan Yuille","Andrew Brown","Mannat Singh"],"pdf_url":"https://arxiv.org/pdf/2412.15213v1.pdf","comment":"Project page: https://cross-flow.github.io/"},{"id":"http://arxiv.org/abs/2412.15214v1","updated":"2024-12-19T18:59:56Z","published":"2024-12-19T18:59:56Z","title":"LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis","summary":"  The intuitive nature of drag-based interaction has led to its growing\nadoption for controlling object trajectories in image-to-video synthesis.\nStill, existing methods that perform dragging in the 2D space usually face\nambiguity when handling out-of-plane movements. In this work, we augment the\ninteraction with a new dimension, i.e., the depth dimension, such that users\nare allowed to assign a relative depth for each point on the trajectory. That\nway, our new interaction paradigm not only inherits the convenience from 2D\ndragging, but facilitates trajectory control in the 3D space, broadening the\nscope of creativity. We propose a pioneering method for 3D trajectory control\nin image-to-video synthesis by abstracting object masks into a few cluster\npoints. These points, accompanied by the depth information and the instance\ninformation, are finally fed into a video diffusion model as the control\nsignal. Extensive experiments validate the effectiveness of our approach,\ndubbed LeviTor, in precisely manipulating the object movements when producing\nphoto-realistic videos from static images. Project page:\nhttps://ppetrichor.github.io/levitor.github.io/\n","authors":["Hanlin Wang","Hao Ouyang","Qiuyu Wang","Wen Wang","Ka Leong Cheng","Qifeng Chen","Yujun Shen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.15214v1.pdf","comment":"Project page available at\n  https://ppetrichor.github.io/levitor.github.io/"},{"id":"http://arxiv.org/abs/2412.15211v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Generative Multiview Relighting for 3D Reconstruction under Extreme\n  Illumination Variation","summary":"  Reconstructing the geometry and appearance of objects from photographs taken\nin different environments is difficult as the illumination and therefore the\nobject appearance vary across captured images. This is particularly challenging\nfor more specular objects whose appearance strongly depends on the viewing\ndirection. Some prior approaches model appearance variation across images using\na per-image embedding vector, while others use physically-based rendering to\nrecover the materials and per-image illumination. Such approaches fail at\nfaithfully recovering view-dependent appearance given significant variation in\ninput illumination and tend to produce mostly diffuse results. We present an\napproach that reconstructs objects from images taken under different\nilluminations by first relighting the images under a single reference\nillumination with a multiview relighting diffusion model and then\nreconstructing the object's geometry and appearance with a radiance field\narchitecture that is robust to the small remaining inconsistencies among the\nrelit images. We validate our proposed approach on both synthetic and real\ndatasets and demonstrate that it greatly outperforms existing techniques at\nreconstructing high-fidelity appearance from images taken under extreme\nillumination variation. Moreover, our approach is particularly effective at\nrecovering view-dependent \"shiny\" appearance which cannot be reconstructed by\nprior methods.\n","authors":["Hadi Alzayer","Philipp Henzler","Jonathan T. Barron","Jia-Bin Huang","Pratul P. Srinivasan","Dor Verbin"],"pdf_url":"https://arxiv.org/pdf/2412.15211v1.pdf","comment":"Project page: https://relight-to-reconstruct.github.io/"},{"id":"http://arxiv.org/abs/2412.15212v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Scaling 4D Representations","summary":"  Scaling has not yet been convincingly demonstrated for pure self-supervised\nlearning from video. However, prior work has focused evaluations on\nsemantic-related tasks $\\unicode{x2013}$ action classification, ImageNet\nclassification, etc. In this paper we focus on evaluating self-supervised\nlearning on non-semantic vision tasks that are more spatial (3D) and temporal\n(+1D = 4D), such as camera pose estimation, point and object tracking, and\ndepth estimation. We show that by learning from very large video datasets,\nmasked auto-encoding (MAE) with transformer video models actually scales,\nconsistently improving performance on these 4D tasks, as model size increases\nfrom 20M all the way to the largest by far reported self-supervised video model\n$\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with\nmany recent image and video models demonstrates the benefits of scaling 4D\nrepresentations.\n","authors":["João Carreira","Dilara Gokay","Michael King","Chuhan Zhang","Ignacio Rocco","Aravindh Mahendran","Thomas Albert Keck","Joseph Heyward","Skanda Koppula","Etienne Pot","Goker Erdogan","Yana Hasson","Yi Yang","Klaus Greff","Guillaume Le Moing","Sjoerd van Steenkiste","Daniel Zoran","Drew A. Hudson","Pedro Vélez","Luisa Polanía","Luke Friedman","Chris Duvarney","Ross Goroshin","Kelsey Allen","Jacob Walker","Rishabh Kabra","Eric Aboussouan","Jennifer Sun","Thomas Kipf","Carl Doersch","Viorica Pătrăucean","Dima Damen","Pauline Luc","Mehdi S. M. Sajjadi","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2412.15212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15209v1","updated":"2024-12-19T18:59:44Z","published":"2024-12-19T18:59:44Z","title":"PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation","summary":"  Despite significant advancements in Large Vision-Language Models (LVLMs),\nexisting pixel-grounding models operate on single-image settings, limiting\ntheir ability to perform detailed, fine-grained comparisons across multiple\nimages. Conversely, current multi-image understanding models lack pixel-level\ngrounding. Our work addresses this gap by introducing the task of multi-image\npixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates\npixel-level grounding with robust multi-image reasoning capabilities to produce\ncontextually rich, pixel-grounded explanations. Central to PRIMA is an\nefficient vision module that queries fine-grained visual representations across\nmultiple images, reducing TFLOPs by $25.3\\%$. To support training and\nevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark\nconsisting of $\\sim$224K question-answer pairs that require fine-grained visual\nunderstanding across multiple images. Experimental results demonstrate PRIMA\noutperforms state-of-the-art baselines.\n","authors":["Muntasir Wahed","Kiet A. Nguyen","Adheesh Sunil Juvekar","Xinzhuo Li","Xiaona Zhou","Vedant Shah","Tianjiao Yu","Pinar Yanardag","Ismini Lourentzou"],"pdf_url":"https://arxiv.org/pdf/2412.15209v1.pdf","comment":"Project page: https://plan-lab.github.io/prima"},{"id":"http://arxiv.org/abs/2412.15208v1","updated":"2024-12-19T18:59:40Z","published":"2024-12-19T18:59:40Z","title":"OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving","summary":"  Since the advent of Multimodal Large Language Models (MLLMs), they have made\na significant impact across a wide range of real-world applications,\nparticularly in Autonomous Driving (AD). Their ability to process complex\nvisual data and reason about intricate driving scenarios has paved the way for\na new paradigm in end-to-end AD systems. However, the progress of developing\nend-to-end models for AD has been slow, as existing fine-tuning methods demand\nsubstantial resources, including extensive computational power, large-scale\ndatasets, and significant funding. Drawing inspiration from recent advancements\nin inference computing, we propose OpenEMMA, an open-source end-to-end\nframework based on MLLMs. By incorporating the Chain-of-Thought reasoning\nprocess, OpenEMMA achieves significant improvements compared to the baseline\nwhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates\neffectiveness, generalizability, and robustness across a variety of challenging\ndriving scenarios, offering a more efficient and effective approach to\nautonomous driving. We release all the codes in\nhttps://github.com/taco-group/OpenEMMA.\n","authors":["Shuo Xing","Chengyuan Qian","Yuping Wang","Hongyuan Hua","Kexin Tian","Yang Zhou","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15206v1","updated":"2024-12-19T18:59:33Z","published":"2024-12-19T18:59:33Z","title":"AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models\n  for Autonomous Driving","summary":"  Recent advancements in large vision language models (VLMs) tailored for\nautonomous driving (AD) have shown strong scene understanding and reasoning\ncapabilities, making them undeniable candidates for end-to-end driving systems.\nHowever, limited work exists on studying the trustworthiness of DriveVLMs -- a\ncritical factor that directly impacts public transportation safety. In this\npaper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for\nlarge vision-language models in autonomous driving (DriveVLMs), considering\ndiverse perspectives -- including trustfulness, safety, robustness, privacy,\nand fairness. We constructed the largest visual question-answering dataset for\ninvestigating trustworthiness issues in driving scenarios, comprising over 10k\nunique scenes and 18k queries. We evaluated six publicly available VLMs,\nspanning from generalist to specialist, from open-source to commercial models.\nOur exhaustive evaluations have unveiled previously undiscovered\nvulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found\nthat the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform\nspecialized models fine-tuned for driving in terms of overall trustworthiness.\nDriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing\nsensitive information. Additionally, both generalist and specialist VLMs remain\nsusceptible to adversarial attacks and struggle to ensure unbiased\ndecision-making across diverse environments and populations. Our findings call\nfor immediate and decisive action to address the trustworthiness of DriveVLMs\n-- an issue of critical importance to public safety and the welfare of all\ncitizens relying on autonomous transportation systems. Our benchmark is\npublicly available at \\url{https://github.com/taco-group/AutoTrust}, and the\nleaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.\n","authors":["Shuo Xing","Hongyuan Hua","Xiangbo Gao","Shenzhe Zhu","Renjie Li","Kexin Tian","Xiaopeng Li","Heng Huang","Tianbao Yang","Zhangyang Wang","Yang Zhou","Huaxiu Yao","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15206v1.pdf","comment":"55 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.15205v1","updated":"2024-12-19T18:59:31Z","published":"2024-12-19T18:59:31Z","title":"FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching","summary":"  Autoregressive (AR) modeling has achieved remarkable success in natural\nlanguage processing by enabling models to generate text with coherence and\ncontextual understanding through next token prediction. Recently, in image\ngeneration, VAR proposes scale-wise autoregressive modeling, which extends the\nnext token prediction to the next scale prediction, preserving the 2D structure\nof images. However, VAR encounters two primary challenges: (1) its complex and\nrigid scale design limits generalization in next scale prediction, and (2) the\ngenerator's dependence on a discrete tokenizer with the same complex scale\nstructure restricts modularity and flexibility in updating the tokenizer. To\naddress these limitations, we introduce FlowAR, a general next scale prediction\nmethod featuring a streamlined scale design, where each subsequent scale is\nsimply double the previous one. This eliminates the need for VAR's intricate\nmulti-scale residual tokenizer and enables the use of any off-the-shelf\nVariational AutoEncoder (VAE). Our simplified design enhances generalization in\nnext scale prediction and facilitates the integration of Flow Matching for\nhigh-quality image synthesis. We validate the effectiveness of FlowAR on the\nchallenging ImageNet-256 benchmark, demonstrating superior generation\nperformance compared to previous methods. Codes will be available at\n\\url{https://github.com/OliverRensu/FlowAR}.\n","authors":["Sucheng Ren","Qihang Yu","Ju He","Xiaohui Shen","Alan Yuille","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15200v1","updated":"2024-12-19T18:58:46Z","published":"2024-12-19T18:58:46Z","title":"DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation\n  for High-quality 3D Asset Creation","summary":"  Procedural Content Generation (PCG) is powerful in creating high-quality 3D\ncontents, yet controlling it to produce desired shapes is difficult and often\nrequires extensive parameter tuning. Inverse Procedural Content Generation aims\nto automatically find the best parameters under the input condition. However,\nexisting sampling-based and neural network-based methods still suffer from\nnumerous sample iterations or limited controllability. In this work, we present\nDI-PCG, a novel and efficient method for Inverse PCG from general image\nconditions. At its core is a lightweight diffusion transformer model, where PCG\nparameters are directly treated as the denoising target and the observed images\nas conditions to control parameter generation. DI-PCG is efficient and\neffective. With only 7.6M network parameters and 30 GPU hours to train, it\ndemonstrates superior performance in recovering parameters accurately, and\ngeneralizing well to in-the-wild images. Quantitative and qualitative\nexperiment results validate the effectiveness of DI-PCG in inverse PCG and\nimage-to-3D generation tasks. DI-PCG offers a promising approach for efficient\ninverse PCG and represents a valuable exploration step towards a 3D generation\npath that models how to construct a 3D asset using parametric models.\n","authors":["Wang Zhao","Yan-Pei Cao","Jiale Xu","Yuejiang Dong","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.15200v1.pdf","comment":"Project page: https://thuzhaowang.github.io/projects/DI-PCG/"},{"id":"http://arxiv.org/abs/2412.15199v1","updated":"2024-12-19T18:58:36Z","published":"2024-12-19T18:58:36Z","title":"LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation","summary":"  This paper targets the challenge of real-time LiDAR re-simulation in dynamic\ndriving scenarios. Recent approaches utilize neural radiance fields combined\nwith the physical modeling of LiDAR sensors to achieve high-fidelity\nre-simulation results. Unfortunately, these methods face limitations due to\nhigh computational demands in large-scale scenes and cannot perform real-time\nLiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel\nframework that supports real-time, physically accurate LiDAR re-simulation for\ndriving scenes. Our primary contribution is the development of an efficient and\neffective rendering pipeline, which integrates Gaussian primitives and\nhardware-accelerated ray tracing technology. Specifically, we model the\nphysical properties of LiDAR sensors using Gaussian primitives with learnable\nparameters and incorporate scene graphs to handle scene dynamics. Building upon\nthis scene representation, our framework first constructs a bounding volume\nhierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views\nthrough a differentiable rendering algorithm. Importantly, our framework\nsupports realistic rendering with flexible scene editing operations and various\nsensor configurations. Extensive experiments across multiple public benchmarks\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nrendering quality and efficiency. Our project page is at\nhttps://zju3dv.github.io/lidar-rt.\n","authors":["Chenxu Zhou","Lvchang Fu","Sida Peng","Yunzhi Yan","Zhanhua Zhang","Yong Chen","Jiazhi Xia","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15199v1.pdf","comment":"Project page: https://zju3dv.github.io/lidar-rt"},{"id":"http://arxiv.org/abs/2412.15195v1","updated":"2024-12-19T18:58:14Z","published":"2024-12-19T18:58:14Z","title":"Preventing Local Pitfalls in Vector Quantization via Optimal Transport","summary":"  Vector-quantized networks (VQNs) have exhibited remarkable performance across\nvarious tasks, yet they are prone to training instability, which complicates\nthe training process due to the necessity for techniques such as subtle\ninitialization and model distillation. In this study, we identify the local\nminima issue as the primary cause of this instability. To address this, we\nintegrate an optimal transport method in place of the nearest neighbor search\nto achieve a more globally informed assignment. We introduce OptVQ, a novel\nvector quantization method that employs the Sinkhorn algorithm to optimize the\noptimal transport problem, thereby enhancing the stability and efficiency of\nthe training process. To mitigate the influence of diverse data distributions\non the Sinkhorn algorithm, we implement a straightforward yet effective\nnormalization strategy. Our comprehensive experiments on image reconstruction\ntasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses\ncurrent state-of-the-art VQNs in reconstruction quality.\n","authors":["Borui Zhang","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.15195v1.pdf","comment":"Code is available at https://github.com/zbr17/OptVQ"},{"id":"http://arxiv.org/abs/2412.15191v1","updated":"2024-12-19T18:57:21Z","published":"2024-12-19T18:57:21Z","title":"AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation","summary":"  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video\ngeneration that leverages the activations of frozen video and audio diffusion\nmodels for temporally-aligned cross-modal conditioning. The key to our\nframework is a Fusion Block that enables bidirectional information exchange\nbetween our backbone video and audio diffusion models through a\ntemporally-aligned self attention operation. Unlike prior work that uses\nfeature extractors pretrained for other tasks for the conditioning signal,\nAV-Link can directly leverage features obtained by the complementary modality\nin a single framework i.e. video features to generate audio, or audio features\nto generate video. We extensively evaluate our design choices and demonstrate\nthe ability of our method to achieve synchronized and high-quality audiovisual\ncontent, showcasing its potential for applications in immersive media\ngeneration. Project Page: snap-research.github.io/AVLink/\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Ivan Skorokhodov","Alper Canberk","Kwot Sin Lee","Vicente Ordonez","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2412.15191v1.pdf","comment":"Project Page: snap-research.github.io/AVLink/"},{"id":"http://arxiv.org/abs/2412.15190v1","updated":"2024-12-19T18:57:13Z","published":"2024-12-19T18:57:13Z","title":"EarthDial: Turning Multi-sensory Earth Observations to Interactive\n  Dialogues","summary":"  Automated analysis of vast Earth observation data via interactive\nVision-Language Models (VLMs) can unlock new opportunities for environmental\nmonitoring, disaster response, and resource management. Existing generic VLMs\ndo not perform well on Remote Sensing data, while the recent Geo-spatial VLMs\nremain restricted to a fixed resolution and few sensor modalities. In this\npaper, we introduce EarthDial, a conversational assistant specifically designed\nfor Earth Observation (EO) data, transforming complex, multi-sensory Earth\nobservations into interactive, natural language dialogues. EarthDial supports\nmulti-spectral, multi-temporal, and multi-resolution imagery, enabling a wide\nrange of remote sensing tasks, including classification, detection, captioning,\nquestion answering, visual reasoning, and visual grounding. To achieve this, we\nintroduce an extensive instruction tuning dataset comprising over 11.11M\ninstruction pairs covering RGB, Synthetic Aperture Radar (SAR), and\nmultispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,\nEarthDial handles bi-temporal and multi-temporal sequence analysis for\napplications like change detection. Our extensive experimental results on 37\ndownstream applications demonstrate that EarthDial outperforms existing generic\nand domain-specific models, achieving better generalization across various EO\ntasks.\n","authors":["Sagar Soni","Akshay Dudhane","Hiyam Debary","Mustansar Fiaz","Muhammad Akhtar Munir","Muhammad Sohail Danish","Paolo Fraccaro","Campbell D Watson","Levente J Klein","Fahad Shahbaz Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2412.15190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15188v1","updated":"2024-12-19T18:56:24Z","published":"2024-12-19T18:56:24Z","title":"LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation","summary":"  We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.\n","authors":["Weijia Shi","Xiaochuang Han","Chunting Zhou","Weixin Liang","Xi Victoria Lin","Luke Zettlemoyer","Lili Yu"],"pdf_url":"https://arxiv.org/pdf/2412.15188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15185v1","updated":"2024-12-19T18:55:25Z","published":"2024-12-19T18:55:25Z","title":"Tiled Diffusion","summary":"  Image tiling -- the seamless connection of disparate images to create a\ncoherent visual field -- is crucial for applications such as texture creation,\nvideo game asset development, and digital art. Traditionally, tiles have been\nconstructed manually, a method that poses significant limitations in\nscalability and flexibility. Recent research has attempted to automate this\nprocess using generative models. However, current approaches primarily focus on\ntiling textures and manipulating models for single-image generation, without\ninherently supporting the creation of multiple interconnected tiles across\ndiverse domains. This paper presents Tiled Diffusion, a novel approach that\nextends the capabilities of diffusion models to accommodate the generation of\ncohesive tiling patterns across various domains of image synthesis that require\ntiling. Our method supports a wide range of tiling scenarios, from self-tiling\nto complex many-to-many connections, enabling seamless integration of multiple\nimages. Tiled Diffusion automates the tiling process, eliminating the need for\nmanual intervention and enhancing creative possibilities in various\napplications, such as seamlessly tiling of existing images, tiled texture\ncreation, and 360{\\deg} synthesis.\n","authors":["Or Madar","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2412.15185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07449v2","updated":"2024-12-19T18:51:28Z","published":"2024-11-12T00:20:11Z","title":"Tracing the Roots: Leveraging Temporal Dynamics in Diffusion\n  Trajectories for Origin Attribution","summary":"  Diffusion models have revolutionized image synthesis, garnering significant\nresearch interest in recent years. Diffusion is an iterative algorithm in which\nsamples are generated step-by-step, starting from pure noise. This process\nintroduces the notion of diffusion trajectories, i.e., paths from the standard\nGaussian distribution to the target image distribution. In this context, we\nstudy discriminative algorithms operating on these trajectories. Specifically,\ngiven a pre-trained diffusion model, we consider the problem of classifying\nimages as part of the training dataset, generated by the model or originating\nfrom an external source. Our approach demonstrates the presence of patterns\nacross steps that can be leveraged for classification. We also conduct ablation\nstudies, which reveal that using higher-order gradient features to characterize\nthe trajectories leads to significant performance gains and more robust\nalgorithms.\n","authors":["Andreas Floros","Seyed-Mohsen Moosavi-Dezfooli","Pier Luigi Dragotti"],"pdf_url":"https://arxiv.org/pdf/2411.07449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15171v1","updated":"2024-12-19T18:46:55Z","published":"2024-12-19T18:46:55Z","title":"SqueezeMe: Efficient Gaussian Avatars for VR","summary":"  Gaussian Splatting has enabled real-time 3D human avatars with unprecedented\nlevels of visual quality. While previous methods require a desktop GPU for\nreal-time inference of a single avatar, we aim to squeeze multiple Gaussian\navatars onto a portable virtual reality headset with real-time drivable\ninference. We begin by training a previous work, Animatable Gaussians, on a\nhigh quality dataset captured with 512 cameras. The Gaussians are animated by\ncontrolling base set of Gaussians with linear blend skinning (LBS) motion and\nthen further adjusting the Gaussians with a neural network decoder to correct\ntheir appearance. When deploying the model on a Meta Quest 3 VR headset, we\nfind two major computational bottlenecks: the decoder and the rendering. To\naccelerate the decoder, we train the Gaussians in UV-space instead of\npixel-space, and we distill the decoder to a single neural network layer.\nFurther, we discover that neighborhoods of Gaussians can share a single\ncorrective from the decoder, which provides an additional speedup. To\naccelerate the rendering, we develop a custom pipeline in Vulkan that runs on\nthe mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently\nat 72 FPS on a VR headset. Demo videos are at\nhttps://forresti.github.io/squeezeme.\n","authors":["Shunsuke Saito","Stanislav Pidhorskyi","Igor Santesteban","Forrest Iandola","Divam Gupta","Anuj Pahuja","Nemanja Bartolovic","Frank Yu","Emanuel Garbin","Tomas Simon"],"pdf_url":"https://arxiv.org/pdf/2412.15171v1.pdf","comment":"Initial version"},{"id":"http://arxiv.org/abs/2412.15159v1","updated":"2024-12-19T18:34:50Z","published":"2024-12-19T18:34:50Z","title":"OnlineVPO: Align Video Diffusion Model with Online Video-Centric\n  Preference Optimization","summary":"  In recent years, the field of text-to-video (T2V) generation has made\nsignificant strides. Despite this progress, there is still a gap between\ntheoretical advancements and practical application, amplified by issues like\ndegraded image quality and flickering artifacts. Recent advancements in\nenhancing the video diffusion model (VDM) through feedback learning have shown\npromising results. However, these methods still exhibit notable limitations,\nsuch as misaligned feedback and inferior scalability. To tackle these issues,\nwe introduce OnlineVPO, a more efficient preference learning approach tailored\nspecifically for video diffusion models. Our method features two novel designs,\nfirstly, instead of directly using image-based reward feedback, we leverage the\nvideo quality assessment (VQA) model trained on synthetic data as the reward\nmodel to provide distribution and modality-aligned feedback on the video\ndiffusion model. Additionally, we introduce an online DPO algorithm to address\nthe off-policy optimization and scalability issue in existing video preference\nlearning frameworks. By employing the video reward model to offer concise video\nfeedback on the fly, OnlineVPO offers effective and efficient preference\nguidance. Extensive experiments on the open-source video-diffusion model\ndemonstrate OnlineVPO as a simple yet effective and more importantly scalable\npreference learning algorithm for video diffusion models, offering valuable\ninsights for future advancements in this domain.\n","authors":["Jiacheng Zhang","Jie Wu","Weifeng Chen","Yatai Ji","Xuefeng Xiao","Weilin Huang","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.15159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15156v1","updated":"2024-12-19T18:32:21Z","published":"2024-12-19T18:32:21Z","title":"Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM","summary":"  Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.\n","authors":["Yatai Ji","Jiacheng Zhang","Jie Wu","Shilong Zhang","Shoufa Chen","Chongjian GE","Peize Sun","Weifeng Chen","Wenqi Shao","Xuefeng Xiao","Weilin Huang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2412.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15150v1","updated":"2024-12-19T18:28:37Z","published":"2024-12-19T18:28:37Z","title":"Leveraging Color Channel Independence for Improved Unsupervised Object\n  Detection","summary":"  Object-centric architectures can learn to extract distinct object\nrepresentations from visual scenes, enabling downstream applications on the\nobject level. Similarly to autoencoder-based image models, object-centric\napproaches have been trained on the unsupervised reconstruction loss of images\nencoded by RGB color spaces. In our work, we challenge the common assumption\nthat RGB images are the optimal color space for unsupervised learning in\ncomputer vision. We discuss conceptually and empirically that other color\nspaces, such as HSV, bear essential characteristics for object-centric\nrepresentation learning, like robustness to lighting conditions. We further\nshow that models improve when requiring them to predict additional color\nchannels. Specifically, we propose to transform the predicted targets to the\nRGB-S space, which extends RGB with HSV's saturation component and leads to\nmarkedly better reconstruction and disentanglement for five common evaluation\ndatasets. The use of composite color spaces can be implemented with basically\nno computational overhead, is agnostic of the models' architecture, and is\nuniversally applicable across a wide range of visual computing tasks and\ntraining types. The findings of our approach encourage additional\ninvestigations in computer vision tasks beyond object-centric learning.\n","authors":["Bastian Jäckl","Yannick Metz","Udo Schlegel","Daniel A. Keim","Maximilian T. Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.15150v1.pdf","comment":"38 pages incl. references, 16 figures"},{"id":"http://arxiv.org/abs/2412.15129v1","updated":"2024-12-19T18:09:42Z","published":"2024-12-19T18:09:42Z","title":"Jet: A Modern Transformer-Based Normalizing Flow","summary":"  In the past, normalizing generative flows have emerged as a promising class\nof generative models for natural images. This type of model has many modeling\nadvantages: the ability to efficiently compute log-likelihood of the input\ndata, fast generation and simple overall structure. Normalizing flows remained\na topic of active research but later fell out of favor, as visual quality of\nthe samples was not competitive with other model classes, such as GANs,\nVQ-VAE-based approaches or diffusion models. In this paper we revisit the\ndesign of the coupling-based normalizing flow models by carefully ablating\nprior design choices and using computational blocks based on the Vision\nTransformer architecture, not convolutional neural networks. As a result, we\nachieve state-of-the-art quantitative and qualitative performance with a much\nsimpler architecture. While the overall visual quality is still behind the\ncurrent state-of-the-art models, we argue that strong normalizing flow models\ncan help advancing research frontier by serving as building components of more\npowerful generative models.\n","authors":["Alexander Kolesnikov","André Susano Pinto","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2412.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15119v1","updated":"2024-12-19T17:59:54Z","published":"2024-12-19T17:59:54Z","title":"Parallelized Autoregressive Visual Generation","summary":"  Autoregressive models have emerged as a powerful approach for visual\ngeneration but suffer from slow inference speed due to their sequential\ntoken-by-token prediction process. In this paper, we propose a simple yet\neffective approach for parallelized autoregressive visual generation that\nimproves generation efficiency while preserving the advantages of\nautoregressive modeling. Our key insight is that parallel generation depends on\nvisual token dependencies-tokens with weak dependencies can be generated in\nparallel, while strongly dependent adjacent tokens are difficult to generate\ntogether, as their independent sampling may lead to inconsistencies. Based on\nthis observation, we develop a parallel generation strategy that generates\ndistant tokens with weak dependencies in parallel while maintaining sequential\ngeneration for strongly dependent local tokens. Our approach can be seamlessly\nintegrated into standard autoregressive models without modifying the\narchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that\nour method achieves a 3.6x speedup with comparable quality and up to 9.5x\nspeedup with minimal quality degradation across both image and video generation\ntasks. We hope this work will inspire future research in efficient visual\ngeneration and unified autoregressive modeling. Project page:\nhttps://epiphqny.github.io/PAR-project.\n","authors":["Yuqing Wang","Shuhuai Ren","Zhijie Lin","Yujin Han","Haoyuan Guo","Zhenheng Yang","Difan Zou","Jiashi Feng","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2412.15119v1.pdf","comment":"Project page: https://epiphqny.github.io/PAR-project"},{"id":"http://arxiv.org/abs/2412.11917v3","updated":"2024-12-19T17:57:59Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v3.pdf","comment":"AAAI-25 (extended version), Code: https://github.com/CompVis/DisCLIP"},{"id":"http://arxiv.org/abs/2412.15106v1","updated":"2024-12-19T17:51:49Z","published":"2024-12-19T17:51:49Z","title":"Knowing Where to Focus: Attention-Guided Alignment for Text-based Person\n  Search","summary":"  In the realm of Text-Based Person Search (TBPS), mainstream methods aim to\nexplore more efficient interaction frameworks between text descriptions and\nvisual data. However, recent approaches encounter two principal challenges.\nFirstly, the widely used random-based Masked Language Modeling (MLM) considers\nall the words in the text equally during training. However, massive\nsemantically vacuous words ('with', 'the', etc.) be masked fail to contribute\nefficient interaction in the cross-modal MLM and hampers the representation\nalignment. Secondly, manual descriptions in TBPS datasets are tedious and\ninevitably contain several inaccuracies. To address these issues, we introduce\nan Attention-Guided Alignment (AGA) framework featuring two innovative\ncomponents: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module\n(TEM). AGM dynamically masks semantically meaningful words by aggregating the\nattention weight derived from the text encoding process, thereby cross-modal\nMLM can capture information related to the masked word from text context and\nimages and align their representations. Meanwhile, TEM alleviates low-quality\nrepresentations caused by repetitive and erroneous text descriptions by\nreplacing those semantically meaningful words with MLM's prediction. It not\nonly enriches text descriptions but also prevents overfitting. Extensive\nexperiments across three challenging benchmarks demonstrate the effectiveness\nof our AGA, achieving new state-of-the-art results with Rank-1 accuracy\nreaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid,\nrespectively.\n","authors":["Lei Tan","Weihao Li","Pingyang Dai","Jie Chen","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2412.15106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13788v2","updated":"2024-12-19T17:51:42Z","published":"2024-03-20T17:51:53Z","title":"DepthFM: Fast Monocular Depth Estimation with Flow Matching","summary":"  Current discriminative depth estimation methods often produce blurry\nartifacts, while generative approaches suffer from slow sampling due to\ncurvatures in the noise-to-depth transport. Our method addresses these\nchallenges by framing depth estimation as a direct transport between image and\ndepth distributions. We are the first to explore flow matching in this field,\nand we demonstrate that its interpolation trajectories enhance both training\nand sampling efficiency while preserving high performance. While generative\nmodels typically require extensive training data, we mitigate this dependency\nby integrating external knowledge from a pre-trained image diffusion model,\nenabling effective transfer even across differing objectives. To further boost\nour model performance, we employ synthetic data and utilize image-depth pairs\ngenerated by a discriminative model on an in-the-wild image dataset. As a\ngenerative model, our model can reliably estimate depth confidence, which\nprovides an additional advantage. Our approach achieves competitive zero-shot\nperformance on standard benchmarks of complex natural scenes while improving\nsampling efficiency and only requiring minimal synthetic data for training.\n","authors":["Ming Gui","Johannes Schusterbauer","Ulrich Prestel","Pingchuan Ma","Dmytro Kotovenko","Olga Grebenkova","Stefan Andreas Baumann","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13788v2.pdf","comment":"AAAI 2025, Project Page: https://github.com/CompVis/depth-fm"},{"id":"http://arxiv.org/abs/2412.15095v1","updated":"2024-12-19T17:45:08Z","published":"2024-12-19T17:45:08Z","title":"A Full Transformer-based Framework for Automatic Pain Estimation using\n  Videos","summary":"  The automatic estimation of pain is essential in designing an optimal pain\nmanagement system offering reliable assessment and reducing the suffering of\npatients. In this study, we present a novel full transformer-based framework\nconsisting of a Transformer in Transformer (TNT) model and a Transformer\nleveraging cross-attention and self-attention blocks. Elaborating on videos\nfrom the BioVid database, we demonstrate state-of-the-art performances, showing\nthe efficacy, efficiency, and generalization capability across all the primary\npain estimation tasks.\n","authors":["Stefanos Gkikas","Manolis Tsiknakis"],"pdf_url":"https://arxiv.org/pdf/2412.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15077v1","updated":"2024-12-19T17:26:07Z","published":"2024-12-19T17:26:07Z","title":"Till the Layers Collapse: Compressing a Deep Neural Network through the\n  Lenses of Batch Normalization Layers","summary":"  Today, deep neural networks are widely used since they can handle a variety\nof complex tasks. Their generality makes them very powerful tools in modern\ntechnology. However, deep neural networks are often overparameterized. The\nusage of these large models consumes a lot of computation resources. In this\npaper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers\n\\textbf{C}ollapse (TLC), which compresses deep neural networks through the\nlenses of batch normalization layers. By reducing the depth of these networks,\nour method decreases deep neural networks' computational requirements and\noverall latency. We validate our method on popular models such as Swin-T,\nMobileNet-V2, and RoBERTa, across both image classification and natural\nlanguage processing (NLP) tasks.\n","authors":["Zhu Liao","Nour Hezbri","Victor Quétu","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2412.15077v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15058v1","updated":"2024-12-19T17:06:53Z","published":"2024-12-19T17:06:53Z","title":"MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging\n  Datasets with In-Context Guidance","summary":"  Medical researchers and clinicians often need to perform novel segmentation\ntasks on a set of related images. Existing methods for segmenting a new dataset\nare either interactive, requiring substantial human effort for each image, or\nrequire an existing set of manually labeled images. We introduce a system,\nMultiverSeg, that enables practitioners to rapidly segment an entire new\ndataset without requiring access to any existing labeled data from that task or\ndomain. Along with the image to segment, the model takes user interactions such\nas clicks, bounding boxes or scribbles as input, and predicts a segmentation.\nAs the user segments more images, those images and segmentations become\nadditional inputs to the model, providing context. As the context set of\nlabeled images grows, the number of interactions required to segment each new\nimage decreases. We demonstrate that MultiverSeg enables users to interactively\nsegment new datasets efficiently, by amortizing the number of interactions per\nimage to achieve an accurate segmentation. Compared to using a state-of-the-art\ninteractive segmentation method, using MultiverSeg reduced the total number of\nscribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images\nfrom unseen tasks. We release code and model weights at\nhttps://multiverseg.csail.mit.edu\n","authors":["Hallee E. Wong","Jose Javier Gonzalez Ortiz","John Guttag","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2412.15058v1.pdf","comment":"Project Website: https://multiverseg.csail.mit.edu Keywords:\n  interactive segmentation, in-context learning, medical image analysis,\n  biomedical imaging, image annotation, visual prompting"},{"id":"http://arxiv.org/abs/2412.15054v1","updated":"2024-12-19T17:02:03Z","published":"2024-12-19T17:02:03Z","title":"GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and\n  Facilitative Playbacks Evaluation","summary":"  The advances in the development of Facilitative Playbacks extracted from\nHigh-Speed videoendoscopic sequences of the vocal folds are hindered by a\nnotable lack of publicly available datasets annotated with the semantic\nsegmentations corresponding to the area of the glottal gap. This fact also\nlimits the reproducibility and further exploration of existing research in this\nfield.\n  To address this gap, GIRAFE is a data repository designed to facilitate the\ndevelopment of advanced techniques for the semantic segmentation, analysis, and\nfast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The\nrepository includes 65 high-speed videoendoscopic recordings from a cohort of\n50 patients (30 female, 20 male). The dataset comprises 15 recordings from\nhealthy controls, 26 from patients with diagnosed voice disorders, and 24 with\nan unknown health condition. All of them were manually annotated by an expert,\nincluding the masks corresponding to the semantic segmentation of the glottal\ngap. The repository is also complemented with the automatic segmentation of the\nglottal area using different state-of-the-art approaches.\n  This data set has already supported several studies, which demonstrates its\nusefulness for the development of new glottal gap segmentation algorithms from\nHigh-Speed-Videoendoscopic sequences to improve or create new Facilitative\nPlaybacks. Despite these advances and others in the field, the broader\nchallenge of performing an accurate and completely automatic semantic\nsegmentation method of the glottal area remains open.\n","authors":["G. Andrade-Miranda","K. Chatzipapas","J. D. Arias-Londoño","J. I. Godino-Llorente"],"pdf_url":"https://arxiv.org/pdf/2412.15054v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.15050v1","updated":"2024-12-19T16:57:45Z","published":"2024-12-19T16:57:45Z","title":"Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion","summary":"  Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area.\n","authors":["Zhifei Chen","Tianshuo Xu","Wenhang Ge","Leyi Wu","Dongyu Yan","Jing He","Luozhou Wang","Lu Zeng","Shunsi Zhang","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03767v2","updated":"2024-12-19T16:45:52Z","published":"2023-01-10T03:10:32Z","title":"Metric Compatible Training for Online Backfilling in Large-Scale\n  Retrieval","summary":"  Backfilling is the process of re-extracting all gallery embeddings from\nupgraded models in image retrieval systems. It inevitably requires a\nprohibitively large amount of computational cost and even entails the downtime\nof the service. Although backward-compatible learning sidesteps this challenge\nby tackling query-side representations, this leads to suboptimal solutions in\nprinciple because gallery embeddings cannot benefit from model upgrades. We\naddress this dilemma by introducing an online backfilling algorithm, which\nenables us to achieve a progressive performance improvement during the\nbackfilling process while not sacrificing the final performance of new model\nafter the completion of backfilling. To this end, we first propose a simple\ndistance rank merge technique for online backfilling. Then, we incorporate a\nreverse transformation module for more effective and efficient merging, which\nis further enhanced by adopting a metric-compatible contrastive learning\napproach. These two components help to make the distances of old and new models\ncompatible, resulting in desirable merge results during backfilling with no\nextra computational overhead. Extensive experiments show the effectiveness of\nour framework on four standard benchmarks in various settings.\n","authors":["Seonguk Seo","Mustafa Gokhan Uzunbas","Bohyung Han","Sara Cao","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2301.03767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15032v1","updated":"2024-12-19T16:44:01Z","published":"2024-12-19T16:44:01Z","title":"DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT\n  Space","summary":"  This paper explores image modeling from the frequency space and introduces\nDCTdiff, an end-to-end diffusion generative paradigm that efficiently models\nimages in the discrete cosine transform (DCT) space. We investigate the design\nspace of DCTdiff and reveal the key design factors. Experiments on different\nframeworks (UViT, DiT), generation tasks, and various diffusion samplers\ndemonstrate that DCTdiff outperforms pixel-based diffusion models regarding\ngenerative quality and training efficiency. Remarkably, DCTdiff can seamlessly\nscale up to high-resolution generation without using the latent diffusion\nparadigm. Finally, we illustrate several intriguing properties of DCT image\nmodeling. For example, we provide a theoretical proof of why `image diffusion\ncan be seen as spectral autoregression', bridging the gap between diffusion and\nautoregressive models. The effectiveness of DCTdiff and the introduced\nproperties suggest a promising direction for image modeling in the frequency\nspace. The code is at \\url{https://github.com/forever208/DCTdiff}.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Haozhe Jia","Lanmiao Liu","Martin Beneš","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2412.15032v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2302.10634v2","updated":"2024-12-19T16:41:57Z","published":"2023-02-21T12:48:44Z","title":"A Deep Learning-Based and Fully Automated Pipeline for Regurgitant\n  Mitral Valve Anatomy Analysis from 3D Echocardiography","summary":"  3D transesophageal echocardiography (3DTEE), is the recommended method for\ndiagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of\nthe mitral valve (MV), allowing for precise segmentation and measurement of the\nregurgitant valve anatomy. However, manual TEE segmentations are time-consuming\nand prone to intra-operator variability, affecting the reliability of the\nmeasurements. To address this, we developed a fully automated pipeline using a\n3D convolutional neural network (CNN) to segment MV substructures (annulus,\nanterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN,\nbased on a multi-decoder residual U-Net architecture, was trained and tested on\na dataset comprising 100 3DTEE images with corresponding segmentations. Within\nthe pipeline, a custom algorithm refines the CNN-based segmentations and\nextracts MV models, from which anatomical landmarks and features are\nquantified. The accuracy of the proposed method was assessed using Dice score\nand mean surface distance (MSD) against ground truth segmentations, and the\nextracted anatomical parameters were compared against a semiautomated\ncommercial software TomTec Image Arena. The trained 3D CNN achieved an average\nDice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the\nannulus, anterior and posterior leaflet. The proposed CNN architecture\noutperformed a baseline residual U-Net architecture in MV substructure\nsegmentation, and the refinement of the predicted annulus segmentation improved\nMSD by 8.36%. The annular and leaflet linear measurements differed by less than\n7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained\nwith TomTec Image Arena. The proposed pipeline was faster than the commercial\nsoftware, with a modeling time of 12.54 s and a quantification time of 54.42 s.\n","authors":["Riccardo Munafò","Simone Saitta","Giacomo Ingallina","Paolo Denti","Francesco Maisano","Eustachio Agricola","Alberto Redaelli","Emiliano Votta"],"pdf_url":"https://arxiv.org/pdf/2302.10634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15023v1","updated":"2024-12-19T16:37:19Z","published":"2024-12-19T16:37:19Z","title":"Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and\n  Semantic Controls","summary":"  Sound designers and Foley artists usually sonorize a scene, such as from a\nmovie or video game, by manually annotating and sonorizing each action of\ninterest in the video. In our case, the intent is to leave full creative\ncontrol to sound designers with a tool that allows them to bypass the more\nrepetitive parts of their work, thus being able to focus on the creative\naspects of sound production. We achieve this presenting Stable-V2A, a two-stage\nmodel consisting of: an RMS-Mapper that estimates an envelope representative of\nthe audio characteristics associated with the input video; and Stable-Foley, a\ndiffusion model based on Stable Audio Open that generates audio semantically\nand temporally aligned with the target video. Temporal alignment is guaranteed\nby the use of the envelope as a ControlNet input, while semantic alignment is\nachieved through the use of sound representations chosen by the designer as\ncross-attention conditioning of the diffusion process. We train and test our\nmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. In\naddition, to test our model on a case study of interest, we introduce Walking\nThe Maps, a dataset of videos extracted from video games depicting animated\ncharacters walking in different locations. Samples and code available on our\ndemo page at https://ispamm.github.io/Stable-V2A.\n","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunità","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.15023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15010v1","updated":"2024-12-19T16:22:37Z","published":"2024-12-19T16:22:37Z","title":"Robust Federated Learning in the Face of Covariate Shift: A Magnitude\n  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation","summary":"  The development of highly sophisticated neural networks has allowed for fast\nprogress in every field of computer vision, however, applications where\nannotated data is prohibited due to privacy or security concerns remain\nchallenging. Federated Learning (FL) offers a promising framework for\nindividuals aiming to collaboratively develop a shared model while preserving\ndata privacy. Nevertheless, our findings reveal that variations in data\ndistribution among clients can profoundly affect FL methodologies, primarily\ndue to instabilities in the aggregation process. We also propose a novel FL\nframework to mitigate the adverse effects of covariate shifts among federated\nclients by combining individual parameter pruning and regularization techniques\nto improve the robustness of individual clients' models to aggregate. Each\nclient's model is optimized through magnitude-based pruning and the addition of\ndropout and noise injection layers to build more resilient decision pathways in\nthe networks and improve the robustness of the model's parameter aggregation\nstep. The proposed framework is capable of extracting robust representations\neven in the presence of very large covariate shifts among client data\ndistributions and in the federation of a small number of clients. Empirical\nfindings substantiate the effectiveness of our proposed methodology across\ncommon benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.\nFurthermore, we introduce the CelebA-Gender dataset, specifically designed to\nevaluate performance on a more realistic domain. The proposed method is capable\nof extracting robust representations even in the presence of both high and low\ncovariate shifts among client data distributions.\n","authors":["Ozgu Goksu","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2412.15010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14988v1","updated":"2024-12-19T16:00:10Z","published":"2024-12-19T16:00:10Z","title":"Stitch Contrast and Segment_Learning a Human Action Segmentation Model\n  Using Trimmed Skeleton Videos","summary":"  Existing skeleton-based human action classification models rely on\nwell-trimmed action-specific skeleton videos for both training and testing,\nprecluding their scalability to real-world applications where untrimmed videos\nexhibiting concatenated actions are predominant. To overcome this limitation,\nrecently introduced skeleton action segmentation models involve un-trimmed\nskeleton videos into end-to-end training. The model is optimized to provide\nframe-wise predictions for any length of testing videos, simultaneously\nrealizing action localization and classification. Yet, achieving such an\nimprovement im-poses frame-wise annotated skeleton videos, which remains\ntime-consuming in practice. This paper features a novel framework for\nskeleton-based action segmentation trained on short trimmed skeleton videos,\nbut that can run on longer un-trimmed videos. The approach is implemented in\nthree steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral\nskeleton stitching scheme that treats trimmed skeleton videos as elementary\nhuman motions that compose a semantic space and can be sampled to generate\nmulti-action stitched se-quences. Contrast learns contrastive representations\nfrom stitched sequences with a novel discrimination pretext task that enables a\nskeleton encoder to learn meaningful action-temporal contexts to improve action\nsegmentation. Finally, Segment relates the proposed method to action\nsegmentation by learning a segmentation layer while handling particular da-ta\navailability. Experiments involve a trimmed source dataset and an untrimmed\ntarget dataset in an adaptation formulation for real-world skeleton-based human\naction segmentation to evaluate the effectiveness of the proposed method.\n","authors":["Haitao Tian","Pierre Payeur"],"pdf_url":"https://arxiv.org/pdf/2412.14988v1.pdf","comment":"Accepted as AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08941v3","updated":"2024-12-19T15:59:19Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14974v1","updated":"2024-12-19T15:48:51Z","published":"2024-12-19T15:48:51Z","title":"Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse\n  Articulated Objects with Rich Annotations","summary":"  The acquisition of substantial volumes of 3D articulated object data is\nexpensive and time-consuming, and consequently the scarcity of 3D articulated\nobject data becomes an obstacle for deep learning methods to achieve remarkable\nperformance in various articulated object understanding tasks. Meanwhile,\npairing these object data with detailed annotations to enable training for\nvarious tasks is also difficult and labor-intensive to achieve. In order to\nexpeditiously gather a significant number of 3D articulated objects with\ncomprehensive and detailed annotations for training, we propose Articulated\nObject Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox\nconsists of i) descriptions of articulated objects by means of a generalized\nstructure program along with their analytic correspondence to the objects'\npoint cloud, ii) procedural rules about manipulations on the structure program\nto synthesize large-scale and diverse new articulated objects, and iii)\nmathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to\nprovide annotations to the synthesized object. Arti-PG has two appealing\nproperties for providing training data for articulated object understanding\ntasks: i) objects are created with unlimited variations in shape through\nprogram-oriented structure manipulation, ii) Arti-PG is widely applicable to\ndiverse tasks by easily providing comprehensive and detailed annotations.\nArti-PG now supports the procedural generation of 26 categories of articulate\nobjects and provides annotations across a wide range of both vision and\nmanipulation tasks, and we provide exhaustive experiments which fully\ndemonstrate its advantages. We will make Arti-PG toolbox publicly available for\nthe community to use.\n","authors":["Jianhua Sun","Yuxuan Li","Jiude Wei","Longfei Xu","Nange Wang","Yining Zhang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2412.14974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14969v1","updated":"2024-12-19T15:47:31Z","published":"2024-12-19T15:47:31Z","title":"PhotoHolmes: a Python library for forgery detection in digital images","summary":"  In this paper, we introduce PhotoHolmes, an open-source Python library\ndesigned to easily run and benchmark forgery detection methods on digital\nimages. The library includes implementations of popular and state-of-the-art\nmethods, dataset integration tools, and evaluation metrics. Utilizing the\nBenchmark tool in PhotoHolmes, users can effortlessly compare various methods.\nThis facilitates an accurate and reproducible comparison between their own\nmethods and those in the existing literature. Furthermore, PhotoHolmes includes\na command-line interface (CLI) to easily run the methods implemented in the\nlibrary on any suspicious image. As such, image forgery methods become more\naccessible to the community. The library has been built with extensibility and\nmodularity in mind, which makes adding new methods, datasets and metrics to the\nlibrary a straightforward process. The source code is available at\nhttps://github.com/photoholmes/photoholmes.\n","authors":["Julián O'Flaherty","Rodrigo Paganini","Juan Pablo Sotelo","Julieta Umpiérrez","Marina Gardella","Matías Tailanian","Pablo Musé"],"pdf_url":"https://arxiv.org/pdf/2412.14969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14965v1","updated":"2024-12-19T15:44:04Z","published":"2024-12-19T15:44:04Z","title":"Movie2Story: A framework for understanding videos and telling stories in\n  the form of novel text","summary":"  Multimodal video-to-text models have made considerable progress, primarily in\ngenerating brief descriptions of video content. However, there is still a\ndeficiency in generating rich long-form text descriptions that integrate both\nvideo and audio. In this paper, we introduce a framework called M2S, designed\nto generate novel-length text by combining audio, video, and character\nrecognition. M2S includes modules for video long-form text description and\ncomprehension, audio-based analysis of emotion, speech rate, and character\nalignment, and visual-based character recognition alignment. By integrating\nmultimodal information using the large language model GPT4o, M2S stands out in\nthe field of multimodal text generation. We demonstrate the effectiveness and\naccuracy of M2S through comparative experiments and human evaluation.\nAdditionally, the model framework has good scalability and significant\npotential for future research.\n","authors":["Kangning Li","Zheyang Jia","Anyu Ying"],"pdf_url":"https://arxiv.org/pdf/2412.14965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14963v1","updated":"2024-12-19T15:43:05Z","published":"2024-12-19T15:43:05Z","title":"IDOL: Instant Photorealistic 3D Human Creation from a Single Image","summary":"  Creating a high-fidelity, animatable 3D full-body avatar from a single image\nis a challenging task due to the diverse appearance and poses of humans and the\nlimited availability of high-quality training data. To achieve fast and\nhigh-quality human reconstruction, this work rethinks the task from the\nperspectives of dataset, model, and representation. First, we introduce a\nlarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K\ndiverse, photorealistic sets of human images. Each set contains 24-view frames\nin specific human poses, generated using a pose-controllable\nimage-to-multi-view model. Next, leveraging the diversity in views, poses, and\nappearances within HuGe100K, we develop a scalable feed-forward transformer\nmodel to predict a 3D human Gaussian representation in a uniform space from a\ngiven human image. This model is trained to disentangle human pose, body shape,\nclothing geometry, and texture. The estimated Gaussians can be animated without\npost-processing. We conduct comprehensive experiments to validate the\neffectiveness of the proposed dataset and method. Our model demonstrates the\nability to efficiently reconstruct photorealistic humans at 1K resolution from\na single input image using a single GPU instantly. Additionally, it seamlessly\nsupports various applications, as well as shape and texture editing tasks.\n","authors":["Yiyu Zhuang","Jiaxi Lv","Hao Wen","Qing Shuai","Ailing Zeng","Hao Zhu","Shifeng Chen","Yujiu Yang","Xun Cao","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14963v1.pdf","comment":"21 pages, 15 figures, includes main content, supplementary materials,\n  and references"},{"id":"http://arxiv.org/abs/2412.14961v1","updated":"2024-12-19T15:42:21Z","published":"2024-12-19T15:42:21Z","title":"TDCNet: Transparent Objects Depth Completion with CNN-Transformer\n  Dual-Branch Parallel Network","summary":"  The sensing and manipulation of transparent objects present a critical\nchallenge in industrial and laboratory robotics. Conventional sensors face\nchallenges in obtaining the full depth of transparent objects due to the\nrefraction and reflection of light on their surfaces and their lack of visible\ntexture. Previous research has attempted to obtain complete depth maps of\ntransparent objects from RGB and damaged depth maps (collected by depth sensor)\nusing deep learning models. However, existing methods fail to fully utilize the\noriginal depth map, resulting in limited accuracy for deep completion. To solve\nthis problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel\nnetwork for transparent object depth completion. The proposed framework\nconsists of two different branches: one extracts features from partial depth\nmaps, while the other processes RGB-D images. Experimental results demonstrate\nthat our model achieves state-of-the-art performance across multiple public\ndatasets. Our code and the pre-trained model are publicly available at\nhttps://github.com/XianghuiFan/TDCNet.\n","authors":["Xianghui Fan","Chao Ye","Anping Deng","Xiaotian Wu","Mengyang Pan","Hang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14957v1","updated":"2024-12-19T15:38:15Z","published":"2024-12-19T15:38:15Z","title":"Dream to Manipulate: Compositional World Models Empowering Robot\n  Imitation Learning with Imagination","summary":"  A world model provides an agent with a representation of its environment,\nenabling it to predict the causal consequences of its actions. Current world\nmodels typically cannot directly and explicitly imitate the actual environment\nin front of a robot, often resulting in unrealistic behaviors and\nhallucinations that make them unsuitable for real-world applications. In this\npaper, we introduce a new paradigm for constructing world models that are\nexplicit representations of the real world and its dynamics. By integrating\ncutting-edge advances in real-time photorealism with Gaussian Splatting and\nphysics simulators, we propose the first compositional manipulation world\nmodel, which we call DreMa. DreMa replicates the observed world and its\ndynamics, allowing it to imagine novel configurations of objects and predict\nthe future consequences of robot actions. We leverage this capability to\ngenerate new data for imitation learning by applying equivariant\ntransformations to a small set of demonstrations. Our evaluations across\nvarious settings demonstrate significant improvements in both accuracy and\nrobustness by incrementing actions and object distributions, reducing the data\nneeded to learn a policy and improving the generalization of the agents. As a\nhighlight, we show that a real Franka Emika Panda robot, powered by DreMa's\nimagination, can successfully learn novel physical tasks from just a single\nexample per task variation (one-shot policy learning). Our project page and\nsource code can be found in https://leobarcellona.github.io/DreamToManipulate/\n","authors":["Leonardo Barcellona","Andrii Zadaianchuk","Davide Allegro","Samuele Papa","Stefano Ghidoni","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2412.14957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13647v2","updated":"2024-12-19T15:37:55Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14954v1","updated":"2024-12-19T15:36:30Z","published":"2024-12-19T15:36:30Z","title":"Corn Ear Detection and Orientation Estimation Using Deep Learning","summary":"  Monitoring growth behavior of maize plants such as the development of ears\ncan give key insights into the plant's health and development. Traditionally,\nthe measurement of the angle of ears is performed manually, which can be\ntime-consuming and prone to human error. To address these challenges, this\npaper presents a computer vision-based system for detecting and tracking ears\nof corn in an image sequence. The proposed system could accurately detect,\ntrack, and predict the ear's orientation, which can be useful in monitoring\ntheir growth behavior. This can significantly save time compared to manual\nmeasurement and enables additional areas of ear orientation research and\npotential increase in efficiencies for maize production. Using an object\ndetector with keypoint detection, the algorithm proposed could detect 90\npercent of all ears. The cardinal estimation had a mean absolute error (MAE) of\n18 degrees, compared to a mean 15 degree difference between two people\nmeasuring by hand. These results demonstrate the feasibility of using computer\nvision techniques for monitoring maize growth and can lead to further research\nin this area.\n","authors":["Nathan Sprague","John Evans","Michael Mardikes"],"pdf_url":"https://arxiv.org/pdf/2412.14954v1.pdf","comment":"22 pages;15 figures"},{"id":"http://arxiv.org/abs/2406.16710v2","updated":"2024-12-19T15:28:26Z","published":"2024-06-24T15:11:35Z","title":"ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait\n  Image","summary":"  While recent works have achieved great success on image-to-3D object\ngeneration, high quality and fidelity 3D head generation from a single image\nremains a great challenge. Previous text-based methods for generating 3D heads\nwere limited by text descriptions and image-based methods struggled to produce\nhigh-quality head geometry. To handle this challenging problem, we propose a\nnovel framework, ID-Sculpt, to generate high-quality 3D heads while preserving\ntheir identities. Our work incorporates the identity information of the\nportrait image into three parts: 1) geometry initialization, 2) geometry\nsculpting, and 3) texture generation stages. Given a reference portrait image,\nwe first align the identity features with text features to realize ID-aware\nguidance enhancement, which contains the control signals representing the face\ninformation. We then use the canny map, ID features of the portrait image, and\na pre-trained text-to-normal/depth diffusion model to generate ID-aware\ngeometry supervision, and 3D-GAN inversion is employed to generate ID-aware\ngeometry initialization. Furthermore, with the ability to inject identity\ninformation into 3D head generation, we use ID-aware guidance to calculate\nID-aware Score Distillation (ISD) for geometry sculpting. For texture\ngeneration, we adopt the ID Consistent Texture Inpainting and Refinement which\nprogressively expands the view for texture inpainting to obtain an\ninitialization UV texture map. We then use the ID-aware guidance to provide\nimage-level supervision for noisy multi-view images to obtain a refined texture\nmap. Extensive experiments demonstrate that we can generate high-quality 3D\nheads with accurate geometry and texture from a single in-the-wild portrait\nimage.\n","authors":["Jinkun Hao","Junshu Tang","Jiangning Zhang","Ran Yi","Yijia Hong","Moran Li","Weijian Cao","Yating Wang","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2406.16710v2.pdf","comment":"Accepted by AAAI 2025; Project page:\n  https://jinkun-hao.github.io/ID-Sculpt/"},{"id":"http://arxiv.org/abs/2411.10958v2","updated":"2024-12-19T15:26:20Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrixes $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$\nto enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS)\nof SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on\nRTX4090, respectively. Comprehensive experiments confirm that our approach\nincurs negligible end-to-end metrics loss across diverse models, including\nthose for large language processing, image generation, and video generation.\nThe codes are available at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14939v1","updated":"2024-12-19T15:15:03Z","published":"2024-12-19T15:15:03Z","title":"GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction","summary":"  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n","authors":["Zesong Yang","Ru Zhang","Jiale Shi","Zixiang Ai","Boming Zhao","Hujun Bao","Luwei Yang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.14939v1.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://zju3dv.github.io/gurecon/"},{"id":"http://arxiv.org/abs/2302.11947v2","updated":"2024-12-19T15:13:46Z","published":"2023-02-23T11:44:43Z","title":"Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight\n  Convolutional Neural Networks","summary":"  The health and safety hazards posed by worn crane lifting ropes mandate\nperiodic inspection for damage. This task is time-consuming, prone to human\nerror, halts operation, and may result in the premature disposal of ropes.\nTherefore, we propose using efficient deep learning and computer vision methods\nto automate the process of detecting damaged ropes. Specifically, we present a\nvision-based system for detecting damage in synthetic fiber rope images using\nlightweight convolutional neural networks. We develop a camera-based apparatus\nto photograph the lifting rope's surface, while in operation, and capture the\nprogressive wear-and-tear as well as the more significant degradation in the\nrope's health state. Experts from Konecranes annotate the collected images in\naccordance with the rope's condition; normal or damaged. Then, we pre-process\nthe images, systematically design a deep learning model, evaluate its detection\nand prediction performance, analyze its computational complexity, and compare\nit with various other models. Experimental results show the proposed model\noutperforms other similar techniques with 96.5% accuracy, 94.8% precision,\n98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the\nmodel's real-time operation, low memory footprint, robustness to various\nenvironmental and operational conditions, and adequacy for deployment in\nindustrial applications such as lifting, mooring, towing, climbing, and\nsailing.\n","authors":["Tuomas Jalonen","Mohammad Al-Sa'd","Roope Mellanen","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2302.11947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14925v1","updated":"2024-12-19T15:02:50Z","published":"2024-12-19T15:02:50Z","title":"Automatic Spectral Calibration of Hyperspectral Images:Method, Dataset\n  and Benchmark","summary":"  Hyperspectral image (HSI) densely samples the world in both the space and\nfrequency domain and therefore is more distinctive than RGB images. Usually,\nHSI needs to be calibrated to minimize the impact of various illumination\nconditions. The traditional way to calibrate HSI utilizes a physical reference,\nwhich involves manual operations, occlusions, and/or limits camera mobility.\nThese limitations inspire this paper to automatically calibrate HSIs using a\nlearning-based method. Towards this goal, a large-scale HSI calibration dataset\nis created, which has 765 high-quality HSI pairs covering diversified natural\nscenes and illuminations. The dataset is further expanded to 7650 pairs by\ncombining with 10 different physically measured illuminations. A spectral\nillumination transformer (SIT) together with an illumination attention module\nis proposed. Extensive benchmarks demonstrate the SoTA performance of the\nproposed SIT. The benchmarks also indicate that low-light conditions are more\nchallenging than normal conditions. The dataset and codes are available\nonline:https://github.com/duranze/Automatic-spectral-calibration-of-HSI\n","authors":["Zhuoran Du","Shaodi You","Cheng Cheng","Shikui Wei"],"pdf_url":"https://arxiv.org/pdf/2412.14925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04272v2","updated":"2024-12-19T15:02:37Z","published":"2024-09-06T13:28:05Z","title":"Cycle Pixel Difference Network for Crisp Edge Detection","summary":"  Edge detection, as a fundamental task in computer vision, has garnered\nincreasing attention. The advent of deep learning has significantly advanced\nthis field. However, recent deep learning-based methods generally face two\nsignificant issues: 1) reliance on large-scale pre-trained weights, and 2)\ngeneration of thick edges. We construct a U-shape encoder-decoder model named\nCPD-Net that successfully addresses these two issues simultaneously. In\nresponse to issue 1), we propose a novel cycle pixel difference convolution\n(CPDC), which effectively integrates edge prior knowledge with modern\nconvolution operations, consequently successfully eliminating the dependence on\nlarge-scale pre-trained weights. As for issue 2), we construct a multi-scale\ninformation enhancement module (MSEM) and a dual residual connection-based\n(DRC) decoder to enhance the edge location ability of the model, thereby\ngenerating crisp and clean contour maps. Comprehensive experiments conducted on\nfour standard benchmarks demonstrate that our method achieves competitive\nperformance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760\nand AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our\napproach provides a novel perspective for addressing these challenges in edge\ndetection.\n","authors":["Changsong Liu","Wei Zhang","Yanyan Liu","Mingyang Li","Wenlin Li","Yimeng Fan","Xiangnan Bai","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.04272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16571v4","updated":"2024-12-19T14:47:05Z","published":"2024-04-25T12:34:23Z","title":"MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth\n  Estimation of Endoscopic Images","summary":"  Photometric constraint is indispensable for self-supervised monocular depth\nestimation. It involves warping a source image onto a target view using\nestimated depth&pose, and then minimizing the difference between the warped and\ntarget images. However, the endoscopic built-in light causes significant\nbrightness fluctuations, and thus makes the photometric constraint unreliable.\nPrevious efforts only mitigate this relying on extra models to calibrate image\nbrightness. In this paper, we propose MonoPCC to address the brightness\ninconsistency radically by reshaping the photometric constraint into a cycle\nform. Instead of only warping the source image, MonoPCC constructs a closed\nloop consisting of two opposite forward-backward warping paths: from target to\nsource and then back to target. Thus, the target image finally receives an\nimage cycle-warped from itself, which naturally makes the constraint invariant\nto brightness changes. Moreover, MonoPCC transplants the source image's\nphase-frequency into the intermediate warped image to avoid structure lost, and\nalso stabilizes the training via an exponential moving average (EMA) strategy\nto avoid frequent changes in the forward warping. The comprehensive and\nextensive experimental results on four endoscopic datasets demonstrate that our\nproposed MonoPCC shows a great robustness to the brightness inconsistency, and\nexceeds other state-of-the-arts by reducing the absolute relative error by at\nleast 7.27%, 9.38%, 9.90% and 3.17%, respectively.\n","authors":["Zhiwei Wang","Ying Zhou","Shiquan He","Ting Li","Fan Huang","Qiang Ding","Xinxia Feng","Mei Liu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2404.16571v4.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.18512v2","updated":"2024-12-19T14:46:05Z","published":"2023-11-30T12:40:23Z","title":"Union-over-Intersections: Object Detection beyond Winner-Takes-All","summary":"  This paper revisits the problem of predicting box locations in object\ndetection architectures. Typically, each box proposal or box query aims to\ndirectly maximize the intersection-over-union score with the ground truth,\nfollowed by a winner-takes-all non-maximum suppression where only the highest\nscoring box in each region is retained. We observe that both steps are\nsub-optimal: the first involves regressing proposals to the entire ground\ntruth, which is a difficult task even with large receptive fields, and the\nsecond neglects valuable information from boxes other than the top candidate.\nInstead of regressing proposals to the whole ground truth, we propose a simpler\napproach: regress only to the area of intersection between the proposal and the\nground truth. This avoids the need for proposals to extrapolate beyond their\nvisual scope, improving localization accuracy. Rather than adopting a\nwinner-takes-all strategy, we take the union over the regressed intersections\nof all boxes in a region to generate the final box outputs. Our plug-and-play\nmethod integrates seamlessly into proposal-based, grid-based, and query-based\ndetection architectures with minimal modifications, consistently improving\nobject localization and instance segmentation. We demonstrate its broad\napplicability and versatility across various detection and segmentation tasks.\n","authors":["Aritra Bhowmik","Pascal Mettes","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2311.18512v2.pdf","comment":"17 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2312.06259v2","updated":"2024-12-19T14:38:47Z","published":"2023-12-11T09:57:09Z","title":"Point Cloud Semantic Segmentation with Sparse and Inhomogeneous\n  Annotations","summary":"  Utilizing uniformly distributed sparse annotations, weakly supervised\nlearning alleviates the heavy reliance on fine-grained annotations in point\ncloud semantic segmentation tasks. However, few works discuss the inhomogeneity\nof sparse annotations, albeit it is common in real-world scenarios. Therefore,\nthis work introduces the probability density function into the gradient\nsampling approximation method to qualitatively analyze the impact of annotation\nsparsity and inhomogeneity under weakly supervised learning. Based on our\nanalysis, we propose an Adaptive Annotation Distribution Network (AADNet)\ncapable of robust learning on arbitrarily distributed sparse annotations.\nSpecifically, we propose a label-aware point cloud downsampling strategy to\nincrease the proportion of annotations involved in the training stage.\nFurthermore, we design the multiplicative dynamic entropy as the gradient\ncalibration function to mitigate the gradient bias caused by non-uniformly\ndistributed sparse annotations and explicitly reduce the epistemic uncertainty.\nWithout any prior restrictions and additional information, our proposed method\nachieves comprehensive performance improvements at multiple label rates and\ndifferent annotation distributions.\n","authors":["Zhiyi Pan","Nan Zhang","Wei Gao","Shan Liu","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2312.06259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14902v1","updated":"2024-12-19T14:32:11Z","published":"2024-12-19T14:32:11Z","title":"MagicNaming: Consistent Identity Generation by Finding a \"Name Space\" in\n  T2I Diffusion Models","summary":"  Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable\nof generating famous persons by simply referring to their names. Is it possible\nto make such models generate generic identities as simple as the famous ones,\ne.g., just use a name? In this paper, we explore the existence of a \"Name\nSpace\", where any point in the space corresponds to a specific identity.\nFortunately, we find some clues in the feature space spanned by text embedding\nof celebrities' names. Specifically, we first extract the embeddings of\ncelebrities' names in the Laion5B dataset with the text encoder of diffusion\nmodels. Such embeddings are used as supervision to learn an encoder that can\npredict the name (actually an embedding) of a given face image. We\nexperimentally find that such name embeddings work well in promising the\ngenerated image with good identity consistency. Note that like the names of\ncelebrities, our predicted name embeddings are disentangled from the semantics\nof text inputs, making the original generation capability of text-to-image\nmodels well-preserved. Moreover, by simply plugging such name embeddings, all\nvariants (e.g., from Civitai) derived from the same base model (i.e., SDXL)\nreadily become identity-aware text-to-image models. Project homepage:\n\\url{https://magicfusion.github.io/MagicNaming/}.\n","authors":["Jing Zhao","Heliang Zheng","Chaoyue Wang","Long Lan","Wanrong Hunag","Yuhua Tang"],"pdf_url":"https://arxiv.org/pdf/2412.14902v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13099v2","updated":"2024-12-19T14:23:45Z","published":"2024-12-17T17:10:02Z","title":"Accuracy Limits as a Barrier to Biometric System Security","summary":"  Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.\n","authors":["Axel Durbet","Paul-Marie Grollemund","Pascal Lafourcade","Kevin Thiry-Atighehchi"],"pdf_url":"https://arxiv.org/pdf/2412.13099v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.14118v2","updated":"2024-12-19T14:18:57Z","published":"2024-12-18T18:04:12Z","title":"GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images","summary":"  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n","authors":["Ziyang Xu","Huangxuan Zhao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14118v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2411.13093v2","updated":"2024-12-19T14:17:13Z","published":"2024-11-20T07:44:34Z","title":"Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension","summary":"  Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.\n","authors":["Yongdong Luo","Xiawu Zheng","Xiao Yang","Guilin Li","Haojia Lin","Jinfa Huang","Jiayi Ji","Fei Chao","Jiebo Luo","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.13093v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.14880v1","updated":"2024-12-19T14:17:09Z","published":"2024-12-19T14:17:09Z","title":"Multimodal Hypothetical Summary for Retrieval-based Multi-image Question\n  Answering","summary":"  Retrieval-based multi-image question answering (QA) task involves retrieving\nmultiple question-related images and synthesizing these images to generate an\nanswer. Conventional \"retrieve-then-answer\" pipelines often suffer from\ncascading errors because the training objective of QA fails to optimize the\nretrieval stage. To address this issue, we propose a novel method to\neffectively introduce and reference retrieved information into the QA. Given\nthe image set to be retrieved, we employ a multimodal large language model\n(visual perspective) and a large language model (textual perspective) to obtain\nmultimodal hypothetical summary in question-form and description-form. By\ncombining visual and textual perspectives, MHyS captures image content more\nspecifically and replaces real images in retrieval, which eliminates the\nmodality gap by transforming into text-to-text retrieval and helps improve\nretrieval. To more advantageously introduce retrieval with QA, we employ\ncontrastive learning to align queries (questions) with MHyS. Moreover, we\npropose a coarse-to-fine strategy for calculating both sentence-level and\nword-level similarity scores, to further enhance retrieval and filter out\nirrelevant details. Our approach achieves a 3.7% absolute improvement over\nstate-of-the-art methods on RETVQA and a 14.5% improvement over CLIP.\nComprehensive experiments and detailed ablation studies demonstrate the\nsuperiority of our method.\n","authors":["Peize Li","Qingyi Si","Peng Fu","Zheng Lin","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14880v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14873v1","updated":"2024-12-19T14:11:49Z","published":"2024-12-19T14:11:49Z","title":"Zero-Shot Artifact2Artifact: Self-incentive artifact removal for\n  photoacoustic imaging without any data","summary":"  Photoacoustic imaging (PAI) uniquely combines optical contrast with the\npenetration depth of ultrasound, making it critical for clinical applications.\nHowever, the quality of 3D PAI is often degraded due to reconstruction\nartifacts caused by the sparse and angle-limited configuration of detector\narrays. Existing iterative or deep learning-based methods are either\ntime-consuming or require large training datasets, significantly limiting their\npractical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a\nzero-shot self-supervised artifact removal method based on a super-lightweight\nnetwork, which leverages the fact that reconstruction artifacts are sensitive\nto irregularities caused by data loss. By introducing random perturbations to\nthe acquired PA data, it spontaneously generates subset data, which in turn\nstimulates the network to learn the artifact patterns in the reconstruction\nresults, thus enabling zero-shot artifact removal. This approach requires\nneither training data nor prior knowledge of the artifacts, and is capable of\nartifact removal for 3D PAI. For maximum amplitude projection (MAP) images or\nslice images in 3D PAI acquired with arbitrarily sparse or angle-limited\ndetector arrays, ZS-A2A employs a self-incentive strategy to complete artifact\nremoval and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in\nboth simulation study and $ in\\ vivo $ animal experiments. Results demonstrate\nthat ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing\nzero-shot methods, and for the $ in\\ vivo $ rat liver, ZS-A2A improves CNR from\n17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in\nthe following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.\n","authors":["Shuang Li","Qian Chen","Chulhong Kim","Seongwook Choi","Yibing Wang","Yu Zhang","Changhui Li"],"pdf_url":"https://arxiv.org/pdf/2412.14873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13620v3","updated":"2024-12-19T14:08:42Z","published":"2022-04-28T16:35:04Z","title":"Generative Adversarial Networks for Image Super-Resolution: A Survey","summary":"  Single image super-resolution (SISR) has played an important role in the\nfield of image processing. Recent generative adversarial networks (GANs) can\nachieve excellent results on low-resolution images with small samples. However,\nthere are little literatures summarizing different GANs in SISR. In this paper,\nwe conduct a comparative study of GANs from different perspectives. We first\ntake a look at developments of GANs. Second, we present popular architectures\nfor GANs in big and small samples for image applications. Then, we analyze\nmotivations, implementations and differences of GANs based optimization methods\nand discriminative learning for image super-resolution in terms of supervised,\nsemi-supervised and unsupervised manners, where these GANs are analyzed via\nintegrating different network architectures, prior knowledge, loss functions\nand multiple tasks. Next, we compare performance of these popular GANs on\npublic datasets via quantitative and qualitative analysis in SISR. Finally, we\nhighlight challenges of GANs and potential research points for SISR.\n","authors":["Chunwei Tian","Xuanyu Zhang","Qi Zhu","Bob Zhang","Jerry Chun-Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2204.13620v3.pdf","comment":"31pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.07024v3","updated":"2024-12-19T14:07:44Z","published":"2024-07-09T16:44:04Z","title":"Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization","summary":"  The vocabulary size in temporal action localization (TAL) is limited by the\nscarcity of large-scale annotated datasets. To overcome this, recent works\nintegrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL\n(OV-TAL). However, despite the success of VLMs trained on extensive datasets,\nexisting OV-TAL methods still rely on human-labeled TAL datasets of limited\nsize to train action localizers, limiting their generalizability. In this\npaper, we explore the scalability of self-training with unlabeled YouTube\nvideos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic\naction localizer is trained on a human-labeled TAL dataset to generate\npseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled\ndataset is then used to train the localizer. Extensive experiments demonstrate\nthat leveraging web-scale videos in self-training significantly enhances the\ngeneralizability of an action localizer. Additionally, we identify limitations\nin existing OV-TAL evaluation schemes and propose a new benchmark for thorough\nassessment. Finally, we showcase the TAL performance of the large multimodal\nmodel Gemini-1.5 on our new benchmark. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL.\n","authors":["Jeongseok Hyun","Su Ho Han","Hyolim Kang","Joon-Young Lee","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07024v3.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2412.14870v1","updated":"2024-12-19T14:06:56Z","published":"2024-12-19T14:06:56Z","title":"Large-scale School Mapping using Weakly Supervised Deep Learning for\n  Universal School Connectivity","summary":"  Improving global school connectivity is critical for ensuring inclusive and\nequitable quality education. To reliably estimate the cost of connecting\nschools, governments and connectivity providers require complete and accurate\nschool location data - a resource that is often scarce in many low- and\nmiddle-income countries. To address this challenge, we propose a\ncost-effective, scalable approach to locating schools in high-resolution\nsatellite images using weakly supervised deep learning techniques. Our best\nmodels, which combine vision transformers and convolutional neural networks,\nachieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging\nexplainable AI techniques, our approach can approximate the precise\ngeographical coordinates of the school locations using only low-cost,\nclassification-level annotations. To demonstrate the scalability of our method,\nwe generate nationwide maps of school location predictions in African countries\nand present a detailed analysis of our results, using Senegal as our case\nstudy. Finally, we demonstrate the immediate usability of our work by\nintroducing an interactive web mapping tool to streamline human-in-the-loop\nmodel validation efforts by government partners. This work successfully\nshowcases the real-world utility of deep learning and satellite images for\nplanning regional infrastructure and accelerating universal school\nconnectivity.\n","authors":["Isabelle Tingzon","Utku Can Ozturk","Ivan Dotu"],"pdf_url":"https://arxiv.org/pdf/2412.14870v1.pdf","comment":"Accepted at AAAI-25 Special Track on AI for Social Impact (AISI)"},{"id":"http://arxiv.org/abs/2412.14869v1","updated":"2024-12-19T14:06:44Z","published":"2024-12-19T14:06:44Z","title":"AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional\n  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature\n  Screening","summary":"  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood\nwithin the skull, which occurs due to the rupture of blood vessels in or around\nthe brain. If this condition is not diagnosed in a timely manner and\nappropriately treated, it can lead to serious complications such as decreased\nconsciousness, permanent neurological disabilities, or even death.The primary\naim of this study is to detect the occurrence or non-occurrence of ICH,\nfollowed by determining the type of subdural hemorrhage (SDH). These tasks are\nframed as two separate binary classification problems. By adding two layers to\nthe co-scale convolutional attention (CCA) classifier architecture, we\nintroduce a novel approach for ICH detection. In the first layer, after\nextracting features from different slices of computed tomography (CT) scan\nimages, we combine these features and select the 50 components that capture the\nhighest variance in the data, considering them as informative features. We then\nassess the discriminative power of these features using the bootstrap forest\nalgorithm, discarding those that lack sufficient discriminative ability between\ndifferent classes. This algorithm explicitly determines the contribution of\neach feature to the final prediction, assisting us in developing an explainable\nAI model. The features feed into a boosting neural network as a latent feature\nspace. In the second layer, we introduce a novel uncertainty-based fuzzy\nintegral operator to fuse information from different CT scan slices. This\noperator, by accounting for the dependencies between consecutive slices,\nsignificantly improves detection accuracy.\n","authors":["Mehdi Hosseini Chagahi","Md. Jalil Piran","Niloufar Delfan","Behzad Moshiri","Jaber Hatam Parikhan"],"pdf_url":"https://arxiv.org/pdf/2412.14869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20213v4","updated":"2024-12-19T13:46:40Z","published":"2024-03-29T14:50:43Z","title":"VHM: Versatile and Honest Vision Language Model for Remote Sensing Image\n  Analysis","summary":"  This paper develops a Versatile and Honest vision language Model (VHM) for\nremote sensing image analysis. VHM is built on a large-scale remote sensing\nimage-text dataset with rich-content captions (VersaD), and an honest\ninstruction dataset comprising both factual and deceptive questions (HnstD).\nUnlike prevailing remote sensing image-text datasets, in which image captions\nfocus on a few prominent objects and their relationships, VersaD captions\nprovide detailed information about image properties, object attributes, and the\noverall scene. This comprehensive captioning enables VHM to thoroughly\nunderstand remote sensing images and perform diverse remote sensing tasks.\nMoreover, different from existing remote sensing instruction datasets that only\ninclude factual questions, HnstD contains additional deceptive questions\nstemming from the non-existence of objects. This feature prevents VHM from\nproducing affirmative answers to nonsense queries, thereby ensuring its\nhonesty. In our experiments, VHM significantly outperforms various vision\nlanguage models on common tasks of scene classification, visual question\nanswering, and visual grounding. Additionally, VHM achieves competent\nperformance on several unexplored tasks, such as building vectorizing,\nmulti-label classification and honest question answering. We will release the\ncode, data and model weights at https://github.com/opendatalab/VHM .\n","authors":["Chao Pang","Xingxing Weng","Jiang Wu","Jiayu Li","Yi Liu","Jiaxing Sun","Weijia Li","Shuai Wang","Litong Feng","Gui-Song Xia","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2403.20213v4.pdf","comment":"Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding\n  author: Gui-Song Xia, Conghui He"},{"id":"http://arxiv.org/abs/2411.04865v4","updated":"2024-12-19T13:45:39Z","published":"2024-11-07T16:58:18Z","title":"ZAHA: Introducing the Level of Facade Generalization and the Large-Scale\n  Point Cloud Facade Semantic Segmentation Benchmark Dataset","summary":"  Facade semantic segmentation is a long-standing challenge in photogrammetry\nand computer vision. Although the last decades have witnessed the influx of\nfacade segmentation methods, there is a lack of comprehensive facade classes\nand data covering the architectural variability. In ZAHA, we introduce Level of\nFacade Generalization (LoFG), novel hierarchical facade classes designed based\non international urban modeling standards, ensuring compatibility with\nreal-world challenging classes and uniform methods' comparison. Realizing the\nLoFG, we present to date the largest semantic 3D facade segmentation dataset,\nproviding 601 million annotated points at five and 15 classes of LoFG2 and\nLoFG3, respectively. Moreover, we analyze the performance of baseline semantic\nsegmentation methods on our introduced LoFG classes and data, complementing it\nwith a discussion on the unresolved challenges for facade segmentation. We\nfirmly believe that ZAHA shall facilitate further development of 3D facade\nsemantic segmentation methods, enabling robust segmentation indispensable in\ncreating urban digital twins.\n","authors":["Olaf Wysocki","Yue Tan","Thomas Froech","Yan Xia","Magdalena Wysocki","Ludwig Hoegner","Daniel Cremers","Christoph Holst"],"pdf_url":"https://arxiv.org/pdf/2411.04865v4.pdf","comment":"Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV))"},{"id":"http://arxiv.org/abs/2412.13913v2","updated":"2024-12-19T13:41:08Z","published":"2024-12-18T14:53:38Z","title":"A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye\n  View Detection","summary":"  Camera-based Bird's Eye View (BEV) perception models receive increasing\nattention for their crucial role in autonomous driving, a domain where concerns\nabout the robustness and reliability of deep learning have been raised. While\nonly a few works have investigated the effects of randomly generated semantic\nperturbations, aka natural corruptions, on the multi-view BEV detection task,\nwe develop a black-box robustness evaluation framework that adversarially\noptimises three common semantic perturbations: geometric transformation, colour\nshifting, and motion blur, to deceive BEV models, serving as the first approach\nin this emerging field. To address the challenge posed by optimising the\nsemantic perturbation, we design a smoothed, distance-based surrogate function\nto replace the mAP metric and introduce SimpleDIRECT, a deterministic\noptimisation algorithm that utilises observed slopes to guide the optimisation\nprocess. By comparing with randomised perturbation and two optimisation\nbaselines, we demonstrate the effectiveness of the proposed framework.\nAdditionally, we provide a benchmark on the semantic robustness of ten recent\nBEV models. The results reveal that PolarFormer, which emphasises geometric\ninformation from multi-view images, exhibits the highest robustness, whereas\nBEVDet is fully compromised, with its precision reduced to zero.\n","authors":["Fu Wang","Yanghao Zhang","Xiangyu Yin","Guangliang Cheng","Zeyu Fu","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.13913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10929v6","updated":"2024-12-19T13:39:55Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v6.pdf","comment":"In process to IEEE Intelligent Vehicle Symposium 2025"},{"id":"http://arxiv.org/abs/2412.14846v1","updated":"2024-12-19T13:38:20Z","published":"2024-12-19T13:38:20Z","title":"Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy\n  with Pre-training, Data Augmentation and Dual Flow UNet","summary":"  Head and neck tumors and metastatic lymph nodes are crucial for treatment\nplanning and prognostic analysis. Accurate segmentation and quantitative\nanalysis of these structures require pixel-level annotation, making automated\nsegmentation techniques essential for the diagnosis and treatment of head and\nneck cancer. In this study, we investigated the effects of multiple strategies\non the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)\nimages. For the segmentation of pre-RT images, we utilized: 1) a fully\nsupervised learning approach, and 2) the same approach enhanced with\npre-trained weights and the MixUp data augmentation technique. For mid-RT\nimages, we introduced a novel computational-friendly network architecture that\nfeatures separate encoders for mid-RT images and registered pre-RT images with\ntheir labels. The mid-RT encoder branch integrates information from pre-RT\nimages and labels progressively during the forward propagation. We selected the\nhighest-performing model from each fold and used their predictions to create an\nensemble average for inference. In the final test, our models achieved a\nsegmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on\naggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at\nhttps://github.com/WltyBY/HNTS-MRG2024_train_code.\n","authors":["Litingyu Wang","Wenjun Liao","Shichuan Zhang","Guotai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14837v1","updated":"2024-12-19T13:27:58Z","published":"2024-12-19T13:27:58Z","title":"ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects","summary":"  3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models.\n","authors":["Qihang Cao","Huangxun Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14837v1.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2412.14835v1","updated":"2024-12-19T13:25:39Z","published":"2024-12-19T13:25:39Z","title":"Progressive Multimodal Reasoning via Active Retrieval","summary":"  Multi-step multimodal reasoning tasks pose significant challenges for\nmultimodal large language models (MLLMs), and finding effective ways to enhance\ntheir performance in such scenarios remains an unresolved issue. In this paper,\nwe propose AR-MCTS, a universal framework designed to progressively improve the\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\nTree Search (MCTS). Our approach begins with the development of a unified\nretrieval module that retrieves key supporting insights for solving complex\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\nautomated multimodal reasoning verification, we employ the MCTS algorithm\ncombined with an active retrieval mechanism, which enables the automatic\ngeneration of step-wise annotations. This strategy dynamically retrieves key\ninsights for each reasoning step, moving beyond traditional beam search\nsampling to improve the diversity and reliability of the reasoning space.\nAdditionally, we introduce a process reward model that aligns progressively to\nsupport the automatic verification of multimodal reasoning tasks. Experimental\nresults across three complex multimodal reasoning benchmarks confirm the\neffectiveness of the AR-MCTS framework in enhancing the performance of various\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\nsampling diversity and accuracy, yielding reliable multimodal reasoning.\n","authors":["Guanting Dong","Chenghao Zhang","Mengjie Deng","Yutao Zhu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.14835v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.14833v1","updated":"2024-12-19T13:21:04Z","published":"2024-12-19T13:21:04Z","title":"Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action\n  Recognition","summary":"  Skeleton-based action recognition using GCNs has achieved remarkable\nperformance, but recognizing ambiguous actions, such as \"waving\" and\n\"saluting\", remains a significant challenge. Existing methods typically rely on\na serial combination of GCNs and TCNs, where spatial and temporal features are\nextracted independently, leading to an unbalanced spatial-temporal information,\nwhich hinders accurate action recognition. Moreover, existing methods for\nambiguous actions often overemphasize local details, resulting in the loss of\ncrucial global context, which further complicates the task of differentiating\nambiguous actions. To address these challenges, we propose a lightweight\nplug-and-play module called Synchronized and Fine-grained Head (SF-Head),\ninserted between GCN and TCN layers. SF-Head first conducts Synchronized\nSpatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL),\nensuring a balanced interaction between the two types of features. It then\nperforms Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature\nConsistency Loss (F-CL), which aligns the aggregated feature with their\noriginal spatial-temporal feature. This aggregation step effectively combines\nboth global context and local details. Experimental results on NTU RGB+D 60,\nNTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in\ndistinguishing ambiguous actions. Our code will be made available at\nhttps://github.com/HaoHuang2003/SFHead.\n","authors":["Hao Huang","Yujie Lin","Siyu Chen","Haiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14833v1.pdf","comment":"20pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.14821v1","updated":"2024-12-19T13:12:15Z","published":"2024-12-19T13:12:15Z","title":"PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation","summary":"  Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}\n","authors":["Shoumeng Qiu","Xinrun Li","XiangYang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2412.14821v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14819v1","updated":"2024-12-19T13:10:38Z","published":"2024-12-19T13:10:38Z","title":"Multi-Level Embedding and Alignment Network with Consistency and\n  Invariance Learning for Cross-View Geo-Localization","summary":"  Cross-View Geo-Localization (CVGL) involves determining the localization of\ndrone images by retrieving the most similar GPS-tagged satellite images.\nHowever, the imaging gaps between platforms are often significant and the\nvariations in viewpoints are substantial, which limits the ability of existing\nmethods to effectively associate cross-view features and extract consistent and\ninvariant characteristics. Moreover, existing methods often overlook the\nproblem of increased computational and storage requirements when improving\nmodel performance. To handle these limitations, we propose a lightweight\nenhanced alignment network, called the Multi-Level Embedding and Alignment\nNetwork (MEAN). The MEAN network uses a progressive multi-level enhancement\nstrategy, global-to-local associations, and cross-domain alignment, enabling\nfeature communication across levels. This allows MEAN to effectively connect\nfeatures at different levels and learn robust cross-view consistent mappings\nand modality-invariant features. Moreover, MEAN adopts a shallow backbone\nnetwork combined with a lightweight branch design, effectively reducing\nparameter count and computational complexity. Experimental results on the\nUniversity-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter\ncount by 62.17% and computational complexity by 70.99% compared to\nstate-of-the-art models, while maintaining competitive or even superior\nperformance. The codes will be released soon.\n","authors":["Zhongwei Chen","Zhao-Xu Yang","Hai-Jun Rong"],"pdf_url":"https://arxiv.org/pdf/2412.14819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14816v1","updated":"2024-12-19T13:10:03Z","published":"2024-12-19T13:10:03Z","title":"Explainable Tampered Text Detection via Multimodal Large Models","summary":"  Recently, tampered text detection has attracted increasing attention due to\nits essential role in information security. Although existing methods can\ndetect the tampered text region, the interpretation of such detection remains\nunclear, making the prediction unreliable. To address this black-box problem,\nwe propose to explain the basis of tampered text detection with natural\nlanguage via large multimodal models. To fill the data gap for this task, we\npropose a large-scale, comprehensive dataset, ETTD, which contains both\npixel-level annotations indicating the tampered text region and natural\nlanguage annotations describing the anomaly of the tampered text. Multiple\nmethods are employed to improve the quality of the proposed data. For example,\na fused mask prompt is proposed to reduce confusion when querying GPT4o to\ngenerate anomaly descriptions. By weighting the input image with the mask\nannotation, the tampered region can be clearly indicated and the content in and\naround the tampered region can also be preserved. We also propose prompting\nGPT4o to recognize tampered texts and filtering out the responses with low OCR\naccuracy, which can effectively improve annotation quality in an automatic\nmanner. To further improve explainable tampered text detection, we propose a\nsimple yet effective model called TTD, which benefits from improved\nfine-grained perception by paying attention to the suspected region with\nauxiliary reference grounding query. Extensive experiments on both the ETTD\ndataset and the public dataset have verified the effectiveness of the proposed\nmethods. In-depth analysis is also provided to inspire further research. The\ndataset and code will be made publicly available.\n","authors":["Chenfan Qu","Jian Liu","Haoxing Chen","Baihan Yu","Jingjing Liu","Weiqiang Wang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.14816v1.pdf","comment":"The first work for explainable tampered text detection"},{"id":"http://arxiv.org/abs/2304.02488v4","updated":"2024-12-19T13:00:35Z","published":"2023-04-05T15:02:30Z","title":"SCB-dataset: A Dataset for Detecting Student Classroom Behavior","summary":"  The use of deep learning methods for automatic detection of students'\nclassroom behavior is a promising approach to analyze their class performance\nand enhance teaching effectiveness. However, the lack of publicly available\ndatasets on student behavior poses a challenge for researchers in this field.\nTo address this issue, we propose a Student Classroom Behavior dataset\n(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248\nlabels and 4,003 images, with a focus on hand-raising behavior. We evaluated\nthe dataset using the YOLOv7 algorithm, achieving a mean average precision\n(map) of up to 85.3%. We believe that our dataset can serve as a robust\nfoundation for future research in the field of student behavior detection and\npromote further advancements in this area.Our SCB-dataset can be downloaded\nfrom: https://github.com/Whiffe/SCB-dataset\n","authors":["Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2304.02488v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13735v2","updated":"2024-12-19T12:59:31Z","published":"2024-12-18T11:14:01Z","title":"3D Registration in 30 Years: A Survey","summary":"  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n","authors":["Jiaqi Yang","Chu'ai Zhang","Zhengbao Wang","Xinyue Cao","Xuan Ouyang","Xiyu Zhang","Zhenxuan Zeng","Zhao Zeng","Borui Lu","Zhiyi Xia","Qian Zhang","Yulan Guo","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14803v1","updated":"2024-12-19T12:48:40Z","published":"2024-12-19T12:48:40Z","title":"Video Prediction Policy: A Generalist Robot Policy with Predictive\n  Visual Representations","summary":"  Recent advancements in robotics have focused on developing generalist\npolicies capable of performing multiple tasks. Typically, these policies\nutilize pre-trained vision encoders to capture crucial information from current\nobservations. However, previous vision encoders, which trained on two-image\ncontrastive learning or single-image reconstruction, can not perfectly capture\nthe sequential information essential for embodied tasks. Recently, video\ndiffusion models (VDMs) have demonstrated the capability to accurately predict\nfuture image sequences, exhibiting a good understanding of physical dynamics.\nMotivated by the strong visual prediction capabilities of VDMs, we hypothesize\nthat they inherently possess visual representations that reflect the evolution\nof the physical world, which we term predictive visual representations.\nBuilding on this hypothesis, we propose the Video Prediction Policy (VPP), a\ngeneralist robotic policy conditioned on the predictive visual representations\nfrom VDMs. To further enhance these representations, we incorporate diverse\nhuman or robotic manipulation datasets, employing unified video-generation\ntraining objectives. VPP consistently outperforms existing methods across two\nsimulated and two real-world benchmarks. Notably, it achieves a 28.1\\% relative\nimprovement in the Calvin ABC-D benchmark compared to the previous\nstate-of-the-art and delivers a 28.8\\% increase in success rates for complex\nreal-world dexterous manipulation tasks.\n","authors":["Yucheng Hu","Yanjiang Guo","Pengchao Wang","Xiaoyu Chen","Yen-Jen Wang","Jianke Zhang","Koushil Sreenath","Chaochao Lu","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14803v1.pdf","comment":"The first two authors contribute equally. Project Page at\n  https://video-prediction-policy.github.io/"},{"id":"http://arxiv.org/abs/2410.05317v3","updated":"2024-12-19T12:38:23Z","published":"2024-10-05T03:47:06Z","title":"Accelerating Diffusion Transformers with Token-wise Feature Caching","summary":"  Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.\n","authors":["Chang Zou","Xuyang Liu","Ting Liu","Siteng Huang","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05317v3.pdf","comment":"In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix"},{"id":"http://arxiv.org/abs/2412.13803v2","updated":"2024-12-19T12:31:34Z","published":"2024-12-18T12:50:11Z","title":"M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation","summary":"  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M$^3$-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M$^3$-VOS, yielding several key insights. Notably, current\nappearancebased approaches show significant room for improvement when handling\nobjects with phase transitions. The inherent changes in disorder suggest that\nthe predictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-andplay model that improves its performance by\nreversal refinement. Our data and code will be publicly available at\nhttps://zixuan-chen.github.io/M-cubeVOS.github.io/.\n","authors":["Zixuan Chen","Jiaxin Li","Liming Tan","Yejie Guo","Junxuan Liang","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2412.13803v2.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.14790v1","updated":"2024-12-19T12:29:31Z","published":"2024-12-19T12:29:31Z","title":"YOLOv11 Optimization for Efficient Resource Utilization","summary":"  The objective of this research is to optimize the eleventh iteration of You\nOnly Look Once (YOLOv11) by developing size-specific modified versions of the\narchitecture. These modifications involve pruning unnecessary layers and\nreconfiguring the main architecture of YOLOv11. Each proposed version is\ntailored to detect objects of specific size ranges, from small to large. To\nensure proper model selection based on dataset characteristics, we introduced\nan object classifier program. This program identifies the most suitable\nmodified version for a given dataset. The proposed models were evaluated on\nvarious datasets and compared with the original YOLOv11 and YOLOv8 models. The\nexperimental results highlight significant improvements in computational\nresource efficiency, with the proposed models maintaining the accuracy of the\noriginal YOLOv11. In some cases, the modified versions outperformed the\noriginal model regarding detection performance. Furthermore, the proposed\nmodels demonstrated reduced model sizes and faster inference times. Models\nweights and the object size classifier can be found in this repository\n","authors":["Areeg Fagad Rasheed","M. Zarkoosh"],"pdf_url":"https://arxiv.org/pdf/2412.14790v1.pdf","comment":"12 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.09401v2","updated":"2024-12-19T12:23:39Z","published":"2024-12-12T16:08:03Z","title":"SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos","summary":"  In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM\nsystem for real-time and high-quality dense 3D reconstruction. SLAM3R provides\nan end-to-end solution by seamlessly integrating local 3D reconstruction and\nglobal coordinate registration through feed-forward neural networks. Given an\ninput video, the system first converts it into overlapping clips using a\nsliding window mechanism. Unlike traditional pose optimization-based methods,\nSLAM3R directly regresses 3D pointmaps from RGB images in each window and\nprogressively aligns and deforms these local pointmaps to create a globally\nconsistent scene reconstruction - all without explicitly solving any camera\nparameters. Experiments across datasets consistently show that SLAM3R achieves\nstate-of-the-art reconstruction accuracy and completeness while maintaining\nreal-time performance at 20+ FPS. Code and weights at:\nhttps://github.com/PKU-VCL-3DV/SLAM3R.\n","authors":["Yuzheng Liu","Siyan Dong","Shuzhe Wang","Yanchao Yang","Qingnan Fan","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16302v2","updated":"2024-12-19T12:14:03Z","published":"2024-07-23T08:57:11Z","title":"DeepClean: Integrated Distortion Identification and Algorithm Selection\n  for Rectifying Image Corruptions","summary":"  Distortion identification and rectification in images and videos is vital for\nachieving good performance in downstream vision applications. Instead of\nrelying on fixed trial-and-error based image processing pipelines, we propose a\ntwo-level sequential planning approach for automated image distortion\nclassification and rectification. At the higher level it detects the class of\ncorruptions present in the input image, if any. The lower level selects a\nspecific algorithm to be applied, from a set of externally provided candidate\nalgorithms. The entire two-level setup runs in the form of a single forward\npass during inference and it is to be queried iteratively until the retrieval\nof the original image. We demonstrate improvements compared to three baselines\non the object detection task on COCO image dataset with rich set of\ndistortions. The advantage of our approach is its dynamic reconfiguration,\nconditioned on the input image and generalisability to unseen candidate\nalgorithms at inference time, since it relies only on the comparison of their\noutput of the image embeddings.\n","authors":["Aditya Kapoor","Harshad Khadilkar","Jayvardhana Gubbi"],"pdf_url":"https://arxiv.org/pdf/2407.16302v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.14168v2","updated":"2024-12-19T11:59:46Z","published":"2024-12-18T18:59:50Z","title":"FashionComposer: Compositional Fashion Image Generation","summary":"  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n","authors":["Sihui Ji","Yiyang Wang","Xi Chen","Xiaogang Xu","Hao Luo","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14168v2.pdf","comment":"https://sihuiji.github.io/FashionComposer-Page"},{"id":"http://arxiv.org/abs/2412.14768v1","updated":"2024-12-19T11:51:45Z","published":"2024-12-19T11:51:45Z","title":"FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal\n  Keypoint Transformers for Pedestrian Fall Detection in Smart Cities","summary":"  In smart cities, detecting pedestrian falls is a major challenge to ensure\nthe safety and quality of life of citizens. In this study, we propose a novel\nfall detection system using FLAMe (Federated Learning with Attention\nMechanism), a federated learning (FL) based algorithm. FLAMe trains around\nimportant keypoint information and only transmits the trained important weights\nto the server, reducing communication costs and preserving data privacy.\nFurthermore, the lightweight keypoint transformer model is integrated into the\nFL framework to effectively learn spatio-temporal features. We validated the\nexperiment using 22,672 video samples from the \"Fall Accident Risk Behavior\nVideo-Sensor Pair data\" dataset from AI-Hub. As a result of the experiment, the\nFLAMe-based system achieved an accuracy of 94.02% with about 190,000\ntransmission parameters, maintaining performance similar to that of existing\ncentralized learning while maximizing efficiency by reducing communication\ncosts by about 40% compared to the existing FL algorithm, FedAvg. Therefore,\nthe FLAMe algorithm has demonstrated that it provides robust performance in the\ndistributed environment of smart cities and is a practical and effective\nsolution for public safety.\n","authors":["Byeonghun Kim","Byeongjoon Noh"],"pdf_url":"https://arxiv.org/pdf/2412.14768v1.pdf","comment":"8 pages, 7 figures, AAAI 2025 FLUID Workshop"},{"id":"http://arxiv.org/abs/2408.01812v3","updated":"2024-12-19T11:29:09Z","published":"2024-08-03T15:43:56Z","title":"SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and\n  BEV Paradigm","summary":"  Ground-to-aerial image synthesis focuses on generating realistic aerial\nimages from corresponding ground street view images while maintaining\nconsistent content layout, simulating a top-down view. The significant\nviewpoint difference leads to domain gaps between views, and dense urban scenes\nlimit the visible range of street views, making this cross-view generation task\nparticularly challenging. In this paper, we introduce SkyDiffusion, a novel\ncross-view generation method for synthesizing aerial images from street view\nimages, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The\nCurved-BEV method in SkyDiffusion converts street-view images into a BEV\nperspective, effectively bridging the domain gap, and employs a \"multi-to-one\"\nmapping strategy to address occlusion issues in dense urban scenes. Next,\nSkyDiffusion designed a BEV-guided diffusion model to generate\ncontent-consistent and realistic aerial images. Additionally, we introduce a\nnovel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image\nsynthesis applications, including disaster scene aerial synthesis, historical\nhigh-resolution satellite image synthesis, and low-altitude UAV image synthesis\ntasks. Experimental results demonstrate that SkyDiffusion outperforms\nstate-of-the-art methods on cross-view datasets across natural (CVUSA),\nsuburban (CVACT), urban (VIGOR-Chicago), and various application scenarios\n(G2A-3), achieving realistic and content-consistent aerial image generation.\nMore result and dataset information can be found at\nhttps://opendatalab.github.io/skydiffusion/ .\n","authors":["Junyan Ye","Jun He","Weijia Li","Zhutao Lv","Yi Lin","Jinhua Yu","Haote Yang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2408.01812v3.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.17981v3","updated":"2024-12-19T11:25:34Z","published":"2024-01-31T16:38:32Z","title":"From Training-Free to Adaptive: Empirical Insights into MLLMs'\n  Understanding of Detection Information","summary":"  Despite the impressive capabilities of Multimodal Large Language Models\n(MLLMs) in integrating text and image modalities, challenges remain in\naccurately interpreting detailed visual elements. Vision detection models excel\nat recognizing fine-grained image details, prompting researchers to use them to\nenhance MLLMs. One effective strategy is to infuse detection information in\ntext format, which has proven simple and effective. However, most studies\nutilize this method without training, leaving the potential of adaptive\ntraining largely unexplored. Adaptive training could significantly enhance\nMLLMs' comprehension of unique inputs while filtering out irrelevant\ninformation. This paper addresses the crucial question: How does training\nimpact MLLMs' understanding of infused textual detection information? We\nsystematically experiment with various representative models to evaluate the\neffects of training-free, retraining, and fine-tuning strategies. We also\nexamine the influence of training on MLLMs' original abilities and the\ninterchangeability of detection models. Our findings indicate that fine-tuning\na pre-trained MLLM to incorporate textual detection information delivers\nsuperior results compared to training-free and retraining methods, improving\nperformance by 6.71% across 10 widely recognized benchmarks. Furthermore,\nfine-tuning enables MLLMs to retain performance enhancements even when\ndetection models are swapped, indicating improved understanding of formatted\ntextual data. We release our codes to support further exploration of fusion\nstrategies for vision detection models and the enhancement of MLLMs'\nfine-grained multimodal capabilities.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2401.17981v3.pdf","comment":"32 pages, 22 tables, 7 figures"},{"id":"http://arxiv.org/abs/2410.23091v5","updated":"2024-12-19T11:18:58Z","published":"2024-10-30T15:06:44Z","title":"CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for\n  Adversarial Defense","summary":"  Despite ongoing efforts to defend neural classifiers from adversarial\nattacks, they remain vulnerable, especially to unseen attacks. In contrast,\nhumans are difficult to be cheated by subtle manipulations, since we make\njudgments only based on essential factors. Inspired by this observation, we\nattempt to model label generation with essential label-causative factors and\nincorporate label-non-causative factors to assist data generation. For an\nadversarial example, we aim to discriminate the perturbations as non-causative\nfactors and make predictions only based on the label-causative factors.\nConcretely, we propose a casual diffusion model (CausalDiff) that adapts\ndiffusion models for conditional data generation and disentangles the two types\nof casual factors by learning towards a novel casual information bottleneck\nobjective. Empirically, CausalDiff has significantly outperformed\nstate-of-the-art defense methods on various unseen attacks, achieving an\naverage robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on\nCIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition\nBenchmark). The code is available at\nhttps://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff\n","authors":["Mingkun Zhang","Keping Bi","Wei Chen","Quanrun Chen","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.23091v5.pdf","comment":"accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.11383v2","updated":"2024-12-19T11:12:30Z","published":"2024-09-17T17:34:24Z","title":"Training Datasets Generation for Machine Learning: Application to Vision\n  Based Navigation","summary":"  Vision Based Navigation consists in utilizing cameras as precision sensors\nfor GNC after extracting information from images. To enable the adoption of\nmachine learning for space applications, one of obstacles is the demonstration\nthat available training datasets are adequate to validate the algorithms. The\nobjective of the study is to generate datasets of images and metadata suitable\nfor training machine learning algorithms. Two use cases were selected and a\nrobust methodology was developed to validate the datasets including the ground\ntruth. The first use case is in-orbit rendezvous with a man-made object: a\nmockup of satellite ENVISAT. The second use case is a Lunar landing scenario.\nDatasets were produced from archival datasets (Chang'e 3), from the laboratory\nat DLR TRON facility and at Airbus Robotic laboratory, from SurRender software\nhigh fidelity image simulator using Model Capture and from Generative\nAdversarial Networks. The use case definition included the selection of\nalgorithms as benchmark: an AI-based pose estimation algorithm and a dense\noptical flow algorithm were selected. Eventually it is demonstrated that\ndatasets produced with SurRender and selected laboratory facilities are\nadequate to train machine learning algorithms.\n","authors":["Jérémy Lebreton","Ingo Ahrns","Roland Brochard","Christoph Haskamp","Hans Krüger","Matthieu Le Goff","Nicolas Menga","Nicolas Ollagnier","Ralf Regele","Francesco Capolupo","Massimo Casasco"],"pdf_url":"https://arxiv.org/pdf/2409.11383v2.pdf","comment":"6 pages, 4 figures, preprint of the proceedings of ESA SPAICE\n  conference 2024"},{"id":"http://arxiv.org/abs/2408.04594v3","updated":"2024-12-19T11:04:20Z","published":"2024-08-08T17:10:16Z","title":"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models","summary":"  High-performance Multimodal Large Language Models (MLLMs) are heavily\ndependent on data quality. To advance fine-grained image recognition within\nMLLMs, we introduce a novel data synthesis method inspired by contrastive\nlearning and image difference captioning. Our key idea involves challenging the\nmodel to discern both matching and distinct elements by scrutinizing object\ndifferences in detailed regions across similar images. We begin by generating\npairs of similar images that emphasize object variations. Following this, we\nemploy a Difference Area Generator to pinpoint object differences, and\nsubsequently, a Difference Captions Generator to articulate these differences.\nThis process results in a high-quality dataset of \"object replacement\" samples,\ntermed Img-Diff, which can be scaled as needed due to its automated nature. We\nleverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs,\nsuch as InternVL2, achieving substantial improvements across various image\ndifference and Visual Question Answering tasks. Notably, the trained models\nsignificantly outperform existing SOTA models like GPT-4V and Gemini on the\nMMVP benchmark. Additionally, we conduct comprehensive evaluations to validate\nthe dataset's diversity, quality, and robustness, offering several insights\ninto the synthesis of such contrastive datasets. We release our codes and\ndataset to encourage further research on multimodal data synthesis and MLLMs'\nfundamental capabilities for image understanding.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Bolin Ding","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04594v3.pdf","comment":"22 pages, 10 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.10681v2","updated":"2024-12-19T10:59:24Z","published":"2024-12-14T05:01:46Z","title":"One Pixel is All I Need","summary":"  Vision Transformers (ViTs) have achieved record-breaking performance in\nvarious visual tasks. However, concerns about their robustness against backdoor\nattacks have grown. Backdoor attacks involve associating a specific trigger\nwith a target label, causing the model to predict the attacker-specified label\nwhen the trigger is present, while correctly identifying clean images.We found\nthat ViTs exhibit higher attack success rates for quasi-triggers(patterns\ndifferent from but similar to the original training triggers)compared to CNNs.\nMoreover, some backdoor features in clean samples can suppress the original\ntrigger, making quasi-triggers more effective.To better understand and exploit\nthese vulnerabilities, we developed a tool called the Perturbation Sensitivity\nDistribution Map (PSDM). PSDM computes and sums gradients over many inputs to\nshow how sensitive the model is to small changes in the input. In ViTs, PSDM\nreveals a patch-like pattern where central pixels are more sensitive than\nedges. We use PSDM to guide the creation of quasi-triggers.Based on these\nfindings, we designed \"WorstVIT,\" a simple yet effective data poisoning\nbackdoor for ViT models. This attack requires an extremely low poisoning rate,\ntrains for just one epoch, and modifies a single pixel to successfully attack\nall validation images.\n","authors":["Deng Siqin","Zhou Xiaoyi"],"pdf_url":"https://arxiv.org/pdf/2412.10681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16729v3","updated":"2024-12-19T10:53:48Z","published":"2024-08-29T17:20:59Z","title":"Prediction-Feedback DETR for Temporal Action Detection","summary":"  Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.\n","authors":["Jihwan Kim","Miso Lee","Cheol-Ho Cho","Jihyun Lee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2408.16729v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2406.02507v3","updated":"2024-12-19T10:43:11Z","published":"2024-06-04T17:25:59Z","title":"Guiding a Diffusion Model with a Bad Version of Itself","summary":"  The primary axes of interest in image-generating diffusion models are image\nquality, the amount of variation in the results, and how well the results align\nwith a given condition, e.g., a class label or a text prompt. The popular\nclassifier-free guidance approach uses an unconditional model to guide a\nconditional model, leading to simultaneously better prompt alignment and\nhigher-quality images at the cost of reduced variation. These effects seem\ninherently entangled, and thus hard to control. We make the surprising\nobservation that it is possible to obtain disentangled control over image\nquality without compromising the amount of variation by guiding generation\nusing a smaller, less-trained version of the model itself rather than an\nunconditional model. This leads to significant improvements in ImageNet\ngeneration, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using\npublicly available networks. Furthermore, the method is also applicable to\nunconditional diffusion models, drastically improving their quality.\n","authors":["Tero Karras","Miika Aittala","Tuomas Kynkäänniemi","Jaakko Lehtinen","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2406.02507v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14719v1","updated":"2024-12-19T10:41:24Z","published":"2024-12-19T10:41:24Z","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","summary":"  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the\nambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to\nidentify the ambiguous sample, categorizing them into distinct sets of\nambiguous samples of false negatives and false positives, considering both\nbody- and action-level categories. \\textbf{Secondly}, we implement an ambiguous\ncontrastive refinement module to calibrate these ambiguous samples by\nregulating the distance between ambiguous samples and their corresponding\nprototypes. This calibration process aims to pull false negative\n($\\mathbb{FN}$) samples closer to their respective prototypes and push false\npositive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. \\textbf{Finally}, we propose a prototype-guided rectification to\nrectify prediction by incorporating the representability of prototypes.\nExtensive experiments conducted on the benchmark dataset demonstrate the\nsuperior performance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14719v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14706v1","updated":"2024-12-19T10:19:43Z","published":"2024-12-19T10:19:43Z","title":"EnergyMoGen: Compositional Human Motion Generation with Energy-Based\n  Diffusion Model in Latent Space","summary":"  Diffusion models, particularly latent diffusion models, have demonstrated\nremarkable success in text-driven human motion generation. However, it remains\nchallenging for latent diffusion models to effectively compose multiple\nsemantic concepts into a single, coherent motion sequence. To address this\nissue, we propose EnergyMoGen, which includes two spectrums of Energy-Based\nModels: (1) We interpret the diffusion model as a latent-aware energy-based\nmodel that generates motions by composing a set of diffusion models in latent\nspace; (2) We introduce a semantic-aware energy model based on cross-attention,\nwhich enables semantic composition and adaptive gradient descent for text\nembeddings. To overcome the challenges of semantic inconsistency and motion\ndistortion across these two spectrums, we introduce Synergistic Energy Fusion.\nThis design allows the motion latent diffusion model to synthesize\nhigh-quality, complex motions by combining multiple energy terms corresponding\nto textual descriptions. Experiments show that our approach outperforms\nexisting state-of-the-art models on various motion generation tasks, including\ntext-to-motion generation, compositional motion generation, and multi-concept\nmotion generation. Additionally, we demonstrate that our method can be used to\nextend motion datasets and improve the text-to-motion task.\n","authors":["Jianrong Zhang","Hehe Fan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14706v1.pdf","comment":"Project page: https://jiro-zhang.github.io/EnergyMoGen/"},{"id":"http://arxiv.org/abs/2412.14705v1","updated":"2024-12-19T10:17:50Z","published":"2024-12-19T10:17:50Z","title":"Event-assisted 12-stop HDR Imaging of Dynamic Scene","summary":"  High dynamic range (HDR) imaging is a crucial task in computational\nphotography, which captures details across diverse lighting conditions.\nTraditional HDR fusion methods face limitations in dynamic scenes with extreme\nexposure differences, as aligning low dynamic range (LDR) frames becomes\nchallenging due to motion and brightness variation. In this work, we propose a\nnovel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera\nsystem with an event camera and an RGB camera. The event camera provides\ntemporally dense, high dynamic range signals that improve alignment between LDR\nframes with large exposure differences, reducing ghosting artifacts caused by\nmotion. Also, a real-world finetuning strategy is proposed to increase the\ngeneralization of alignment module on real-world events. Additionally, we\nintroduce a diffusion-based fusion module that incorporates image priors from\npre-trained diffusion models to address artifacts in high-contrast regions and\nminimize errors from the alignment process. To support this work, we developed\nthe ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized\nevent signals, and validated our approach on both simulated and real-world\ndata. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, successfully extending HDR imaging to 12 stops in\ndynamic scenes.\n","authors":["Shi Guo","Zixuan Chen","Ziran Zhang","Yutian Chen","Gangwei Xu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2412.14705v1.pdf","comment":"Project page:\n  https://openimaginglab.github.io/Event-Assisted-12stops-HDR/"},{"id":"http://arxiv.org/abs/2410.17098v2","updated":"2024-12-19T10:03:18Z","published":"2024-10-22T15:22:53Z","title":"Activity Recognition on Avatar-Anonymized Datasets with Masked\n  Differential Privacy","summary":"  Privacy-preserving computer vision is an important emerging problem in\nmachine learning and artificial intelligence. Prevalent methods tackling this\nproblem use differential privacy (DP) or obfuscation techniques to protect the\nprivacy of individuals. In both cases, the utility of the trained model is\nsacrificed heavily in this process. In this work, we present an anonymization\npipeline that replaces sensitive human subjects in video datasets with\nsynthetic avatars within context, employing a combined rendering and stable\ndiffusion-based strategy. Additionally we propose masked differential privacy\n({MaskDP}) to protect non-anonymized but privacy sensitive background\ninformation. MaskDP allows for controlling sensitive regions where differential\nprivacy is applied, in contrast to applying DP on the entire input. This\ncombined methodology provides strong privacy protection while minimizing the\nusual performance penalty of privacy preserving methods. Experiments on\nmultiple challenging action recognition datasets demonstrate that our proposed\ntechniques result in better utility-privacy trade-offs compared to standard\ndifferentially private training in the especially demanding $\\epsilon<1$\nregime.\n","authors":["David Schneider","Sina Sajadmanesh","Vikash Sehwag","Saquib Sarfraz","Rainer Stiefelhagen","Lingjuan Lyu","Vivek Sharma"],"pdf_url":"https://arxiv.org/pdf/2410.17098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14692v1","updated":"2024-12-19T09:51:45Z","published":"2024-12-19T09:51:45Z","title":"Explicit Relational Reasoning Network for Scene Text Detection","summary":"  Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed.\n","authors":["Yuchen Su","Zhineng Chen","Yongkun Du","Zhilong Ji","Kai Hu","Jinfeng Bai","Xieping Gao"],"pdf_url":"https://arxiv.org/pdf/2412.14692v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14680v1","updated":"2024-12-19T09:32:53Z","published":"2024-12-19T09:32:53Z","title":"A Light-Weight Framework for Open-Set Object Detection with Decoupled\n  Feature Alignment in Joint Space","summary":"  Open-set object detection (OSOD) is highly desirable for robotic manipulation\nin unstructured environments. However, existing OSOD methods often fail to meet\nthe requirements of robotic applications due to their high computational burden\nand complex deployment. To address this issue, this paper proposes a\nlight-weight framework called Decoupled OSOD (DOSOD), which is a practical and\nhighly efficient solution to support real-time OSOD tasks in robotic systems.\nSpecifically, DOSOD builds upon the YOLO-World pipeline by integrating a\nvision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)\nadaptor is developed to transform text embeddings extracted by the VLM into a\njoint space, within which the detector learns the region representations of\nclass-agnostic proposals. Cross-modality features are directly aligned in the\njoint space, avoiding the complex feature interactions and thereby improving\ncomputational efficiency. DOSOD operates like a traditional closed-set detector\nduring the testing phase, effectively bridging the gap between closed-set and\nopen-set detection. Compared to the baseline YOLO-World, the proposed DOSOD\nsignificantly enhances real-time performance while maintaining comparable\naccuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\\%$, compared to\n$26.2\\%$ for YOLO-World-v1-S and $22.7\\%$ for YOLO-World-v2-S, using similar\nbackbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is\n$57.1\\%$ higher than YOLO-World-v1-S and $29.6\\%$ higher than YOLO-World-v2-S.\nMeanwhile, we demonstrate that the DOSOD model facilitates the deployment of\nedge devices. The codes and models are publicly available at\nhttps://github.com/D-Robotics-AI-Lab/DOSOD.\n","authors":["Yonghao He","Hu Su","Haiyong Yu","Cong Yang","Wei Sui","Cong Wang","Song Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14678v1","updated":"2024-12-19T09:31:53Z","published":"2024-12-19T09:31:53Z","title":"Efficient Few-Shot Neural Architecture Search by Counting the Number of\n  Nonlinear Functions","summary":"  Neural architecture search (NAS) enables finding the best-performing\narchitecture from a search space automatically. Most NAS methods exploit an\nover-parameterized network (i.e., a supernet) containing all possible\narchitectures (i.e., subnets) in the search space. However, the subnets that\nshare the same set of parameters are likely to have different characteristics,\ninterfering with each other during training. To address this, few-shot NAS\nmethods have been proposed that divide the space into a few subspaces and\nemploy a separate supernet for each subspace to limit the extent of weight\nsharing. They achieve state-of-the-art performance, but the computational cost\nincreases accordingly. We introduce in this paper a novel few-shot NAS method\nthat exploits the number of nonlinear functions to split the search space. To\nbe specific, our method divides the space such that each subspace consists of\nsubnets with the same number of nonlinear functions. Our splitting criterion is\nefficient, since it does not require comparing gradients of a supernet to split\nthe space. In addition, we have found that dividing the space allows us to\nreduce the channel dimensions required for each supernet, which enables\ntraining multiple supernets in an efficient manner. We also introduce a\nsupernet-balanced sampling (SBS) technique, sampling several subnets at each\ntraining step, to train different supernets evenly within a limited number of\ntraining steps. Extensive experiments on standard NAS benchmarks demonstrate\nthe effectiveness of our approach. Our code is available at\nhttps://cvlab.yonsei.ac.kr/projects/EFS-NAS.\n","authors":["Youngmin Oh","Hyunju Lee","Bumsub Ham"],"pdf_url":"https://arxiv.org/pdf/2412.14678v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14672v1","updated":"2024-12-19T09:24:10Z","published":"2024-12-19T09:24:10Z","title":"FiVL: A Framework for Improved Vision-Language Alignment","summary":"  Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. This issue extends to\nvision-language benchmarks, where it is difficult to make the image\nindispensable for accurate answer generation, particularly in vision\nquestion-answering tasks. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nto evaluate their effectiveness in achieving it. These datasets can be utilized\nfor both training and assessing an LVLM's ability to use image content as\nsubstantive evidence rather than relying solely on linguistic priors, providing\ninsights into the model's reliance on visual information. To demonstrate the\nutility of our dataset, we introduce an innovative training task that\noutperforms baselines alongside a validation method and application for\nexplainability. The code is available at https://github.com/IntelLabs/fivl.\n","authors":["Estelle Aflalo","Gabriela Ben Melech Stan","Tiep Le","Man Luo","Shachar Rosenman","Sayak Paul","Shao-Yen Tseng","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2412.14672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14671v1","updated":"2024-12-19T09:22:19Z","published":"2024-12-19T09:22:19Z","title":"MUSTER: Longitudinal Deformable Registration by Composition of\n  Consecutive Deformations","summary":"  Longitudinal imaging allows for the study of structural changes over time.\nOne approach to detecting such changes is by non-linear image registration.\nThis study introduces Multi-Session Temporal Registration (MUSTER), a novel\nmethod that facilitates longitudinal analysis of changes in extended series of\nmedical images. MUSTER improves upon conventional pairwise registration by\nincorporating more than two imaging sessions to recover longitudinal\ndeformations. Longitudinal analysis at a voxel-level is challenging due to\neffects of a changing image contrast as well as instrumental and environmental\nsources of bias between sessions. We show that local normalized\ncross-correlation as an image similarity metric leads to biased results and\npropose a robust alternative. We test the performance of MUSTER on a synthetic\nmulti-site, multi-session neuroimaging dataset and show that, in various\nscenarios, using MUSTER significantly enhances the estimated deformations\nrelative to pairwise registration. Additionally, we apply MUSTER on a sample of\nolder adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.\nThe results show that MUSTER can effectively identify patterns of\nneuro-degeneration from T1-weighted images and that these changes correlate\nwith changes in cognition, matching the performance of state of the art\nsegmentation methods. By leveraging GPU acceleration, MUSTER efficiently\nhandles large datasets, making it feasible also in situations with limited\ncomputational resources.\n","authors":["Edvard O. S. Grødem","Donatas Sederevičius","Esten H. Leonardsen","Bradley J. MacIntosh","Atle Bjørnerud","Till Schellhorn","Øystein Sørensen","Inge Amlien","Pablo F. Garrido","Anders M. Fjell"],"pdf_url":"https://arxiv.org/pdf/2412.14671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01220v3","updated":"2024-12-19T09:16:19Z","published":"2024-07-01T12:07:26Z","title":"Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation","summary":"  Understanding 3D scenes is a crucial challenge in computer vision research\nwith applications spanning multiple domains. Recent advancements in distilling\n2D vision-language foundation models into neural fields, like NeRF and 3DGS,\nenable open-vocabulary segmentation of 3D scenes from 2D multi-view images\nwithout the need for precise 3D annotations. However, while effective, these\nmethods typically rely on the per-pixel distillation of high-dimensional CLIP\nfeatures, introducing ambiguity and necessitating complex regularization\nstrategies, which adds inefficiency during training. This paper presents\nMaskField, which enables efficient 3D open-vocabulary segmentation with neural\nfields from a novel perspective. Unlike previous methods, MaskField decomposes\nthe distillation of mask and semantic features from foundation models by\nformulating a mask feature field and queries. MaskField overcomes ambiguous\nobject boundaries by naturally introducing SAM segmented object shapes without\nextra regularization during training. By circumventing the direct handling of\ndense high-dimensional CLIP features during training, MaskField is particularly\ncompatible with explicit scene representations like 3DGS. Our extensive\nexperiments show that MaskField not only surpasses prior state-of-the-art\nmethods but also achieves remarkably fast convergence. We hope that MaskField\nwill inspire further exploration into how neural fields can be trained to\ncomprehend 3D scenes from 2D models.\n","authors":["Zihan Gao","Lingling Li","Licheng Jiao","Fang Liu","Xu Liu","Wenping Ma","Yuwei Guo","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01220v3.pdf","comment":"15 pages, 9 figures, Code:https://github.com/keloee/MaskField"},{"id":"http://arxiv.org/abs/2412.14660v1","updated":"2024-12-19T09:10:07Z","published":"2024-12-19T09:10:07Z","title":"Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.\n","authors":["Zijun Chen","Wenbo Hu","Guande He","Zhijie Deng","Zheng Zhang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.14660v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2403.15031v2","updated":"2024-12-19T09:00:34Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.00143v2","updated":"2024-12-19T08:58:15Z","published":"2024-05-31T19:13:09Z","title":"Diversifying Query: Region-Guided Transformer for Temporal Sentence\n  Grounding","summary":"  Temporal sentence grounding is a challenging task that aims to localize the\nmoment spans relevant to a language description. Although recent DETR-based\nmodels have achieved notable progress by leveraging multiple learnable moment\nqueries, they suffer from overlapped and redundant proposals, leading to\ninaccurate predictions. We attribute this limitation to the lack of\ntask-related guidance for the learnable queries to serve a specific mode.\nFurthermore, the complex solution space generated by variable and\nopen-vocabulary language descriptions complicates optimization, making it\nharder for learnable queries to distinguish each other adaptively. To tackle\nthis limitation, we present a Region-Guided TRansformer (RGTR) for temporal\nsentence grounding, which diversifies moment queries to eliminate overlapped\nand redundant predictions. Instead of using learnable queries, RGTR adopts a\nset of anchor pairs as moment queries to introduce explicit regional guidance.\nEach anchor pair takes charge of moment prediction for a specific temporal\nregion, which reduces the optimization difficulty and ensures the diversity of\nthe final predictions. In addition, we design an IoU-aware scoring head to\nimprove proposal quality. Extensive experiments demonstrate the effectiveness\nof RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA\nand TACoS datasets. Codes are available at https://github.com/TensorsSun/RGTR\n","authors":["Xiaolong Sun","Liushuai Shi","Le Wang","Sanping Zhou","Kun Xia","Yabing Wang","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2406.00143v2.pdf","comment":"Accepted by AAAI-25. Code is available at\n  https://github.com/TensorsSun/RGTR"},{"id":"http://arxiv.org/abs/2412.11953v2","updated":"2024-12-19T08:52:56Z","published":"2024-12-16T16:37:03Z","title":"Reliable Breast Cancer Molecular Subtype Prediction based on\n  uncertainty-aware Bayesian Deep Learning by Mammography","summary":"  Breast cancer is a heterogeneous disease with different molecular subtypes,\nclinical behavior, treatment responses as well as survival outcomes. The\ndevelopment of a reliable, accurate, available and inexpensive method to\npredict the molecular subtypes using medical images plays an important role in\nthe diagnosis and prognosis of breast cancer. Recently, deep learning methods\nhave shown good performance in the breast cancer classification tasks using\nvarious medical images. Despite all that success, classical deep learning\ncannot deliver the predictive uncertainty. The uncertainty represents the\nvalidity of the predictions. Therefore, the high predicted uncertainty might\ncause a negative effect in the accurate diagnosis of breast cancer molecular\nsubtypes. To overcome this, uncertainty quantification methods are used to\ndetermine the predictive uncertainty. Accordingly, in this study, we proposed\nan uncertainty-aware Bayesian deep learning model using the full mammogram\nimages. In addition, to increase the performance of the multi-class molecular\nsubtype classification task, we proposed a novel hierarchical classification\nstrategy, named the two-stage classification strategy. The separate AUC of the\nproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,\nluminal and triple-negative classes, respectively. The proposed model not only\nhas a comparable performance to other studies in the field of breast cancer\nmolecular subtypes prediction, even using full mammography images, but it is\nalso more reliable, due to quantify the predictive uncertainty.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2412.11953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14643v1","updated":"2024-12-19T08:51:57Z","published":"2024-12-19T08:51:57Z","title":"RefHCM: A Unified Model for Referring Perceptions in Human-Centric\n  Scenarios","summary":"  Human-centric perceptions play a crucial role in real-world applications.\nWhile recent human-centric works have achieved impressive progress, these\nefforts are often constrained to the visual domain and lack interaction with\nhuman instructions, limiting their applicability in broader scenarios such as\nchatbots and sports analysis. This paper introduces Referring Human\nPerceptions, where a referring prompt specifies the person of interest in an\nimage. To tackle the new task, we propose RefHCM (Referring Human-Centric\nModel), a unified framework to integrate a wide range of human-centric\nreferring tasks. Specifically, RefHCM employs sequence mergers to convert raw\nmultimodal data -- including images, text, coordinates, and parsing maps --\ninto semantic tokens. This standardized representation enables RefHCM to\nreformulate diverse human-centric referring tasks into a sequence-to-sequence\nparadigm, solved using a plain encoder-decoder transformer architecture.\nBenefiting from a unified learning strategy, RefHCM effectively facilitates\nknowledge transfer across tasks and exhibits unforeseen capabilities in\nhandling complex reasoning. This work represents the first attempt to address\nreferring human perceptions with a general-purpose framework, while\nsimultaneously establishing a corresponding benchmark that sets new standards\nfor the field. Extensive experiments showcase RefHCM's competitive and even\nsuperior performance across multiple human-centric referring tasks. The code\nand data are publicly at https://github.com/JJJYmmm/RefHCM.\n","authors":["Jie Huang","Ruibing Hou","Jiahe Zhao","Hong Chang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2412.14643v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.14640v1","updated":"2024-12-19T08:51:01Z","published":"2024-12-19T08:51:01Z","title":"Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention\n  for Fine-Grained Few-Shot Learning","summary":"  Few-shot, fine-grained classification in computer vision poses significant\nchallenges due to the need to differentiate subtle class distinctions with\nlimited data. This paper presents a novel method that enhances the Contrastive\nLanguage-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided\nby real-time visual inputs. Unlike existing techniques such as Context\nOptimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by\nstatic prompts or visual token reliance, the proposed approach leverages a\ncross-attention mechanism to dynamically refine text prompts for the image at\nhand. This enables an image-specific alignment of textual features with image\npatches extracted from the Vision Transformer, making the model more effective\nfor datasets with high intra-class variance and low inter-class differences.\nThe method is evaluated on several datasets, including CUBirds, Oxford Flowers,\nand FGVC Aircraft, showing significant performance gains over static prompt\ntuning approaches. To ensure these performance gains translate into trustworthy\npredictions, we integrate Monte-Carlo Dropout in our approach to improve the\nreliability of the model predictions and uncertainty estimates. This\nintegration provides valuable insights into the model's predictive confidence,\nhelping to identify when predictions can be trusted and when additional\nverification is necessary. This dynamic approach offers a robust solution,\nadvancing the state-of-the-art for few-shot fine-grained classification.\n","authors":["Eric Brouwer","Jan Erik van Woerden","Gertjan Burghouts","Matias Valedenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2412.14640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18459v3","updated":"2024-12-19T08:47:07Z","published":"2024-04-29T06:35:34Z","title":"Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in\n  the Wild","summary":"  Large language models have evolved data-efficient generalists, benefiting\nfrom the universal language interface and large-scale pre-training. However,\nconstructing a data-efficient generalist for dense visual prediction presents a\ndistinct challenge due to the variation in label structures across different\ntasks. Consequently, generalization to unseen dense prediction tasks in the\nlow-data regime is not straightforward and has received less attention from\nprevious vision generalists. In this study, we explore a universal model that\ncan flexibly adapt to unseen dense label structures with a few examples,\nenabling it to serve as a data-efficient vision generalist in diverse\nreal-world scenarios. To this end, we base our method on a powerful\nmeta-learning framework and explore several axes to improve its performance and\nversatility for real-world problems, such as flexible adaptation mechanisms and\nscalability. We evaluate our model across a spectrum of unseen real-world\nscenarios where low-shot learning is desirable, including video, 3D, medical,\nbiological, and user-interactive tasks. Equipped with a generic architecture\nand an effective adaptation mechanism, our model flexibly adapts to all of\nthese tasks with at most 50 labeled images, showcasing a significant\nadvancement over existing data-efficient generalist approaches. Codes are\navailable at https://github.com/GitGyun/chameleon.\n","authors":["Donggyun Kim","Seongwoong Cho","Semin Kim","Chong Luo","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2404.18459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v3","updated":"2024-12-19T08:41:19Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Xue-Mei Dong","Jingqun Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12974v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14633v1","updated":"2024-12-19T08:38:59Z","published":"2024-12-19T08:38:59Z","title":"Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit\n  Post-Training Quantization in Vision Transformers","summary":"  Due to its efficiency, Post-Training Quantization (PTQ) has been widely\nadopted for compressing Vision Transformers (ViTs). However, when quantized\ninto low-bit representations, there is often a significant performance drop\ncompared to their full-precision counterparts. To address this issue,\nreconstruction methods have been incorporated into the PTQ framework to improve\nperformance in low-bit quantization settings. Nevertheless, existing related\nmethods predefine the reconstruction granularity and seldom explore the\nprogressive relationships between different reconstruction granularities, which\nleads to sub-optimal quantization results in ViTs. To this end, in this paper,\nwe propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for\naccurate PTQ, which significantly improves the performance of low-bit quantized\nvision transformers. Specifically, we define multi-head self-attention and\nmulti-layer perceptron modules along with their shortcuts as the finest\nreconstruction units. After reconstructing these two fine-grained units, we\ncombine them to form coarser blocks and reconstruct them at a coarser\ngranularity level. We iteratively perform this combination and reconstruction\nprocess, achieving progressive fine-to-coarse reconstruction. Additionally, we\nintroduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the\ndifficulty of training, thereby further enhancing model performance.\nExperimental results on the ImageNet dataset demonstrate that our proposed\nmethod achieves the best Top-1 accuracy among state-of-the-art methods,\nparticularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides,\nquantization results on the COCO dataset reveal the effectiveness and\ngeneralization of our proposed method on other computer vision tasks like\nobject detection and instance segmentation.\n","authors":["Rui Ding","Liang Yong","Sihuan Zhao","Jing Nie","Lihui Chen","Haijun Liu","Xichuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14631v1","updated":"2024-12-19T08:36:32Z","published":"2024-12-19T08:36:32Z","title":"Review of Fruit Tree Image Segmentation","summary":"  Fruit tree image segmentation is an essential problem in automating a variety\nof agricultural tasks such as phenotyping, harvesting, spraying, and pruning.\nMany research papers have proposed a diverse spectrum of solutions suitable to\nspecific tasks and environments. The review scope of this paper is confined to\nthe front views of fruit trees and based on 158 relevant papers collected using\na newly designed crawling review method. These papers are systematically\nreviewed based on a taxonomy that sequentially considers the method, image,\ntask, and fruit. This taxonomy will assist readers to intuitively grasp the big\npicture of these research activities. Our review reveals that the most\nnoticeable deficiency of the previous studies was the lack of a versatile\ndataset and segmentation model that could be applied to a variety of tasks and\nenvironments. Six important future research tasks are suggested, with the\nexpectation that these will pave the way to building a versatile tree\nsegmentation module.\n","authors":["Il-Seok Oh"],"pdf_url":"https://arxiv.org/pdf/2412.14631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14630v1","updated":"2024-12-19T08:33:33Z","published":"2024-12-19T08:33:33Z","title":"Unified Image Restoration and Enhancement: Degradation Calibrated Cycle\n  Reconstruction Diffusion Model","summary":"  Image restoration and enhancement are pivotal for numerous computer vision\napplications, yet unifying these tasks efficiently remains a significant\nchallenge. Inspired by the iterative refinement capabilities of diffusion\nmodels, we propose CycleRDM, a novel framework designed to unify restoration\nand enhancement tasks while achieving high-quality mapping. Specifically,\nCycleRDM first learns the mapping relationships among the degraded domain, the\nrough normal domain, and the normal domain through a two-stage diffusion\ninference process. Subsequently, we transfer the final calibration process to\nthe wavelet low-frequency domain using discrete wavelet transform, performing\nfine-grained calibration from a frequency domain perspective by leveraging\ntask-specific frequency spaces. To improve restoration quality, we design a\nfeature gain module for the decomposed wavelet high-frequency domain to\neliminate redundant features. Additionally, we employ multimodal textual\nprompts and Fourier transform to drive stable denoising and reduce randomness\nduring the inference process. After extensive validation, CycleRDM can be\neffectively generalized to a wide range of image restoration and enhancement\ntasks while requiring only a small number of training samples to be\nsignificantly superior on various benchmarks of reconstruction quality and\nperceptual quality. The source code will be available at\nhttps://github.com/hejh8/CycleRDM.\n","authors":["Minglong Xue","Jinhong He","Shivakumara Palaiahnakote","Mingliang Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11216v2","updated":"2024-12-19T08:32:20Z","published":"2024-12-15T15:13:14Z","title":"Distribution-Consistency-Guided Multi-modal Hashing","summary":"  Multi-modal hashing methods have gained popularity due to their fast speed\nand low storage requirements. Among them, the supervised methods demonstrate\nbetter performance by utilizing labels as supervisory signals compared with\nunsupervised methods. Currently, for almost all supervised multi-modal hashing\nmethods, there is a hidden assumption that training sets have no noisy labels.\nHowever, labels are often annotated incorrectly due to manual labeling in\nreal-world scenarios, which will greatly harm the retrieval performance. To\naddress this issue, we first discover a significant distribution consistency\npattern through experiments, i.e., the 1-0 distribution of the presence or\nabsence of each category in the label is consistent with the high-low\ndistribution of similarity scores of the hash codes relative to category\ncenters. Then, inspired by this pattern, we propose a novel\nDistribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to\nfilter and reconstruct noisy labels to enhance retrieval performance.\nSpecifically, the proposed method first randomly initializes several category\ncenters, which are used to compute the high-low distribution of similarity\nscores; Noisy and clean labels are then separately filtered out via the\ndiscovered distribution consistency pattern to mitigate the impact of noisy\nlabels; Subsequently, a correction strategy, which is indirectly designed via\nthe distribution consistency pattern, is applied to the filtered noisy labels,\ncorrecting high-confidence ones while treating low-confidence ones as unlabeled\nfor unsupervised learning, thereby further enhancing the model's performance.\nExtensive experiments on three widely used datasets demonstrate the superiority\nof the proposed method compared to state-of-the-art baselines in multi-modal\nretrieval tasks. The code is available at\nhttps://github.com/LiuJinyu1229/DCGMH.\n","authors":["Jin-Yu Liu","Xian-Ling Mao","Tian-Yi Che","Rong-Cheng Tu"],"pdf_url":"https://arxiv.org/pdf/2412.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14629v1","updated":"2024-12-19T08:31:42Z","published":"2024-12-19T08:31:42Z","title":"Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix\n  Factorization","summary":"  Robust Principal Component Analysis (RPCA) is a fundamental technique for\ndecomposing data into low-rank and sparse components, which plays a critical\nrole for applications such as image processing and anomaly detection.\nTraditional RPCA methods commonly use $\\ell_1$ norm regularization to enforce\nsparsity, but this approach can introduce bias and result in suboptimal\nestimates, particularly in the presence of significant noise or outliers.\nNon-convex regularization methods have been proposed to mitigate these\nchallenges, but they tend to be complex to optimize and sensitive to initial\nconditions, leading to potential instability in solutions. To overcome these\nchallenges, in this paper, we propose a novel RPCA model that integrates\nadaptive weighted least squares (AWLS) and low-rank matrix factorization\n(LRMF). The model employs a {self-attention-inspired} mechanism in its weight\nupdate process, allowing the weight matrix to dynamically adjust and emphasize\nsignificant components during each iteration. By employing a weighted F-norm\nfor the sparse component, our method effectively reduces bias while simplifying\nthe computational process compared to traditional $\\ell_1$-norm-based methods.\nWe use an alternating minimization algorithm, where each subproblem has an\nexplicit solution, thereby improving computational efficiency. Despite its\nsimplicity, numerical experiments demonstrate that our method outperforms\nexisting non-convex regularization approaches, offering superior performance\nand stability, as well as enhanced accuracy and robustness in practical\napplications.\n","authors":["Kexin Li","You-wei Wen","Xu Xiao","Mingchao Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14628v1","updated":"2024-12-19T08:30:54Z","published":"2024-12-19T08:30:54Z","title":"Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models","summary":"  Diffusion Models (DM) have democratized AI image generation through an\niterative denoising process. Quantization is a major technique to alleviate the\ninference cost and reduce the size of DM denoiser networks. However, as\ndenoisers evolve from variants of convolutional U-Nets toward newer Transformer\narchitectures, it is of growing importance to understand the quantization\nsensitivity of different weight layers, operations and architecture types to\nperformance. In this work, we address this challenge with Qua$^2$SeDiMo, a\nmixed-precision Post-Training Quantization framework that generates explainable\ninsights on the cost-effectiveness of various model weight quantization methods\nfor different denoiser operation types and block structures. We leverage these\ninsights to make high-quality mixed-precision quantization decisions for a\nmyriad of diffusion models ranging from foundational U-Nets to state-of-the-art\nTransformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,\n3.65-bit and 3.7-bit weight quantization on PixArt-${\\alpha}$,\nPixArt-${\\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our\nweight-quantization configurations with 6-bit activation quantization and\noutperform existing approaches in terms of quantitative metrics and generative\nimage quality.\n","authors":["Keith G. Mills","Mohammad Salameh","Ruichen Chen","Negar Hassanpour","Wei Lu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2412.14628v1.pdf","comment":"AAAI 2025; version includes supplementary material; 22 Pages, 18\n  Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2412.14623v1","updated":"2024-12-19T08:21:28Z","published":"2024-12-19T08:21:28Z","title":"FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors\n  Guided by Facial Recognizers","summary":"  Previous Deepfake detection methods perform well within their training\ndomains, but their effectiveness diminishes significantly with new synthesis\ntechniques. Recent studies have revealed that detection models often create\ndecision boundaries based on facial identity rather than synthetic artifacts,\nresulting in poor performance on cross-domain datasets. To address this\nlimitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a\nnovel training method that mitigates facial identity influence using a face\nrecognizer. Specifically, we first train a face recognizer using the same\nbackbone as the Deepfake detector. The recognizer is then frozen and employed\nduring the detector's training to reduce facial identity information. This is\nachieved by feeding input images into both the recognizer and the detector, and\nminimizing the similarity of their feature embeddings through our Facial\nIdentity Attenuating loss. This process encourages the detector to generate\nembeddings distinct from the recognizer, effectively reducing the impact of\nfacial identity. Extensive experiments demonstrate that our approach\nsignificantly enhances detection performance on both in-domain and cross-domain\ndatasets.\n","authors":["Younhun Kim","Myung-Joon Kwon","Wonjun Lee","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.14623v1.pdf","comment":"5 pages, 4 figures. In 2024 IEEE International Conference on Visual\n  Communications and Image Processing (VCIP) Oral"},{"id":"http://arxiv.org/abs/2409.17671v3","updated":"2024-12-19T08:19:41Z","published":"2024-09-26T09:30:37Z","title":"Leveraging Anthropometric Measurements to Improve Human Mesh Estimation\n  and Ensure Consistent Body Shapes","summary":"  The basic body shape (i.e., the body shape in T-pose) of a person does not\nchange within a single video. However, most SOTA human mesh estimation (HME)\nmodels output a slightly different, thus inconsistent basic body shape for each\nvideo frame. Furthermore, we find that SOTA 3D human pose estimation (HPE)\nmodels outperform HME models regarding the precision of the estimated 3D\nkeypoint positions. We solve the problem of inconsistent body shapes by\nleveraging anthropometric measurements like taken by tailors from humans. We\ncreate a model called A2B that converts given anthropometric measurements to\nbasic body shape parameters of human mesh models. We obtain superior and\nconsistent human meshes by combining the A2B model results with the keypoints\nof 3D HPE models using inverse kinematics. We evaluate our approach on\nchallenging datasets like ASPset or fit3D, where we can lower the MPJPE by over\n30 mm compared to SOTA HME models. Further, replacing estimates of the body\nshape parameters from existing HME models with A2B results not only increases\nthe performance of these HME models, but also guarantees consistent body\nshapes.\n","authors":["Katja Ludwig","Julian Lorenz","Daniel Kienzle","Tuan Bui","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2409.17671v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14619v1","updated":"2024-12-19T08:11:42Z","published":"2024-12-19T08:11:42Z","title":"Pitfalls of topology-aware image segmentation","summary":"  Topological correctness, i.e., the preservation of structural integrity and\nspecific characteristics of shape, is a fundamental requirement for medical\nimaging tasks, such as neuron or vessel segmentation. Despite the recent surge\nin topology-aware methods addressing this challenge, their real-world\napplicability is hindered by flawed benchmarking practices. In this paper, we\nidentify critical pitfalls in model evaluation that include inadequate\nconnectivity choices, overlooked topological artifacts in ground truth\nannotations, and inappropriate use of evaluation metrics. Through detailed\nempirical analysis, we uncover these issues' profound impact on the evaluation\nand ranking of segmentation methods. Drawing from our findings, we propose a\nset of actionable recommendations to establish fair and robust evaluation\nstandards for topology-aware medical image segmentation methods.\n","authors":["Alexander H. Berger","Laurin Lux","Alexander Weers","Martin Menten","Daniel Rueckert","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2412.14619v1.pdf","comment":"Code is available at\n  https://github.com/AlexanderHBerger/topo-pitfalls"},{"id":"http://arxiv.org/abs/2412.14613v1","updated":"2024-12-19T08:03:16Z","published":"2024-12-19T08:03:16Z","title":"HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic\n  Evaluation Using a Vision Language Model","summary":"  Vision-language models (VLMs) have shown impressive abilities in text and\nimage understanding. However, existing metrics for evaluating the text\ngenerated by VLMs focus exclusively on overall quality, leading to two\nlimitations: 1) it is challenging to identify which aspects of the text need\nimprovement from the overall score; 2) metrics may overlook specific evaluation\ncriteria when predicting an overall score. To address these limitations, we\npropose HarmonicEval, a reference-free evaluation metric that aggregates\ncriterion-wise scores to produce the overall score in a bottom-up manner.\nFurthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)\ndataset, which comprises 18,000 expert human judgments across four\nvision-language tasks. Our experiments demonstrate that HarmonicEval achieves\nhigher correlations with human judgments than conventional metrics while\nproviding numerical scores for each criterion.\n","authors":["Masanari Ohi","Masahiro Kaneko","Naoaki Okazaki","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2412.14613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19101v4","updated":"2024-12-19T08:00:44Z","published":"2024-06-27T11:28:36Z","title":"DocKylin: A Large Multimodal Model for Visual Document Understanding\n  with Efficient Visual Slimming","summary":"  Current multimodal large language models (MLLMs) face significant challenges\nin visual document understanding (VDU) tasks due to the high resolution, dense\ntext, and complex layouts typical of document images. These characteristics\ndemand a high level of detail perception ability from MLLMs. While increasing\ninput resolution improves detail perception capability, it also leads to longer\nsequences of visual tokens, increasing computational costs and straining the\nmodels' ability to handle long contexts. To address these challenges, we\nintroduce DocKylin, a document-centric MLLM that performs visual content\nslimming at both the pixel and token levels, thereby reducing token sequence\nlength in VDU scenarios. We introduce an Adaptive Pixel Slimming (APS)\npreprocessing module to perform pixel-level slimming, increasing the proportion\nof informative pixels. Moreover, we propose a novel Dynamic Token Slimming\n(DTS) module to conduct token-level slimming, filtering essential tokens and\nremoving others to adaptively create a more compact visual sequence.\nExperiments demonstrate DocKylin's promising performance across various VDU\nbenchmarks and the effectiveness of each component.\n","authors":["Jiaxin Zhang","Wentao Yang","Songxuan Lai","Zecheng Xie","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2406.19101v4.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14603v1","updated":"2024-12-19T07:49:40Z","published":"2024-12-19T07:49:40Z","title":"Successive optimization of optics and post-processing with\n  differentiable coherent PSF operator and field information","summary":"  Recently, the joint design of optical systems and downstream algorithms is\nshowing significant potential. However, existing rays-described methods are\nlimited to optimizing geometric degradation, making it difficult to fully\nrepresent the optical characteristics of complex, miniaturized lenses\nconstrained by wavefront aberration or diffraction effects. In this work, we\nintroduce a precise optical simulation model, and every operation in pipeline\nis differentiable. This model employs a novel initial value strategy to enhance\nthe reliability of intersection calculation on high aspherics. Moreover, it\nutilizes a differential operator to reduce memory consumption during coherent\npoint spread function calculations. To efficiently address various degradation,\nwe design a joint optimization procedure that leverages field information.\nGuided by a general restoration network, the proposed method not only enhances\nthe image quality, but also successively improves the optical performance\nacross multiple lenses that are already in professional level. This joint\noptimization pipeline offers innovative insights into the practical design of\nsophisticated optical systems and post-processing algorithms. The source code\nwill be made publicly available at\nhttps://github.com/Zrr-ZJU/Successive-optimization\n","authors":["Zheng Ren","Jingwen Zhou","Wenguan Zhang","Jiapu Yan","Bingkun Chen","Huajun Feng","Shiqi Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14598v1","updated":"2024-12-19T07:39:06Z","published":"2024-12-19T07:39:06Z","title":"Can We Get Rid of Handcrafted Feature Extractors? SparseViT:\n  Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization\n  Through Spare-Coding Transformer","summary":"  Non-semantic features or semantic-agnostic features, which are irrelevant to\nimage context but sensitive to image manipulations, are recognized as\nevidential to Image Manipulation Localization (IML). Since manual labels are\nimpossible, existing works rely on handcrafted methods to extract non-semantic\nfeatures. Handcrafted non-semantic features jeopardize IML model's\ngeneralization ability in unseen or complex scenarios. Therefore, for IML, the\nelephant in the room is: How to adaptively extract non-semantic features?\nNon-semantic features are context-irrelevant and manipulation-sensitive. That\nis, within an image, they are consistent across patches unless manipulation\noccurs. Then, spare and discrete interactions among image patches are\nsufficient for extracting non-semantic features. However, image semantics vary\ndrastically on different patches, requiring dense and continuous interactions\namong image patches for learning semantic representations. Hence, in this\npaper, we propose a Sparse Vision Transformer (SparseViT), which reformulates\nthe dense, global self-attention in ViT into a sparse, discrete manner. Such\nsparse self-attention breaks image semantics and forces SparseViT to adaptively\nextract non-semantic features for images. Besides, compared with existing IML\nmodels, the sparse self-attention mechanism largely reduced the model size (max\n80% in FLOPs), achieving stunning parameter efficiency and computation\nreduction. Extensive experiments demonstrate that, without any handcrafted\nfeature extractors, SparseViT is superior in both generalization and efficiency\nacross benchmark datasets.\n","authors":["Lei Su","Xiaochen Ma","Xuekang Zhu","Chaoqun Niu","Zeyu Lei","Ji-Zhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14598v1.pdf","comment":"12 page, 8 figures, published to AAAI"},{"id":"http://arxiv.org/abs/2412.14596v1","updated":"2024-12-19T07:31:40Z","published":"2024-12-19T07:31:40Z","title":"LDP: Generalizing to Multilingual Visual Information Extraction by\n  Language Decoupled Pretraining","summary":"  Visual Information Extraction (VIE) plays a crucial role in the comprehension\nof semi-structured documents, and several pre-trained models have been\ndeveloped to enhance performance. However, most of these works are monolingual\n(usually English). Due to the extremely unbalanced quantity and quality of\npre-training corpora between English and other languages, few works can extend\nto non-English scenarios. In this paper, we conduct systematic experiments to\nshow that vision and layout modality hold invariance among images with\ndifferent languages. If decoupling language bias from document images, a\nvision-layout-based model can achieve impressive cross-lingual generalization.\nAccordingly, we present a simple but effective multilingual training paradigm\nLDP (Language Decoupled Pre-training) for better utilization of monolingual\npre-training data. Our proposed model LDM (Language Decoupled Model) is first\npre-trained on the language-independent data, where the language knowledge is\ndecoupled by a diffusion model, and then the LDM is fine-tuned on the\ndownstream languages. Extensive experiments show that the LDM outperformed all\nSOTA multilingual pre-trained models, and also maintains competitiveness on\ndownstream monolingual/English benchmarks.\n","authors":["Huawen Shen","Gengluo Li","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14596v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14592v1","updated":"2024-12-19T07:23:17Z","published":"2024-12-19T07:23:17Z","title":"Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,\n  and Internal Properties","summary":"  Object anomaly detection is essential for industrial quality inspection, yet\ntraditional single-sensor methods face critical limitations. They fail to\ncapture the wide range of anomaly types, as single sensors are often\nconstrained to either external appearance, geometric structure, or internal\nproperties. To overcome these challenges, we introduce MulSen-AD, the first\nhigh-resolution, multi-sensor anomaly detection dataset tailored for industrial\napplications. MulSen-AD unifies data from RGB cameras, laser scanners, and\nlock-in infrared thermography, effectively capturing external appearance,\ngeometric deformations, and internal defects. The dataset spans 15 industrial\nproducts with diverse, real-world anomalies. We also present MulSen-AD Bench, a\nbenchmark designed to evaluate multi-sensor methods, and propose\nMulSen-TripleAD, a decision-level fusion algorithm that integrates these three\nmodalities for robust, unsupervised object anomaly detection. Our experiments\ndemonstrate that multi-sensor fusion substantially outperforms single-sensor\napproaches, achieving 96.1% AUROC in object-level detection accuracy. These\nresults highlight the importance of integrating multi-sensor data for\ncomprehensive industrial anomaly detection.\n","authors":["Wenqiao Li","Bozhong Zheng","Xiaohao Xu","Jinye Gan","Fading Lu","Xiang Li","Na Ni","Zheng Tian","Xiaonan Huang","Shenghua Gao","Yingna Wu"],"pdf_url":"https://arxiv.org/pdf/2412.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09583v4","updated":"2024-12-19T07:21:43Z","published":"2024-10-12T16:28:40Z","title":"POPoS: Improving Efficient and Robust Facial Landmark Detection with\n  Parallel Optimal Position Search","summary":"  Achieving a balance between accuracy and efficiency is a critical challenge\nin facial landmark detection (FLD). This paper introduces Parallel Optimal\nPosition Search (POPoS), a high-precision encoding-decoding framework designed\nto address the limitations of traditional FLD methods. POPoS employs three key\ncontributions: (1) Pseudo-range multilateration is utilized to correct heatmap\nerrors, improving landmark localization accuracy. By integrating multiple\nanchor points, it reduces the impact of individual heatmap inaccuracies,\nleading to robust overall positioning. (2) To enhance the pseudo-range accuracy\nof selected anchor points, a new loss function, named multilateration anchor\nloss, is proposed. This loss function enhances the accuracy of the distance\nmap, mitigates the risk of local optima, and ensures optimal solutions. (3) A\nsingle-step parallel computation algorithm is introduced, boosting\ncomputational efficiency and reducing processing time. Extensive evaluations\nacross five benchmark datasets demonstrate that POPoS consistently outperforms\nexisting methods, particularly excelling in low-resolution heatmaps scenarios\nwith minimal computational overhead. These advantages make POPoS a highly\nefficient and accurate tool for FLD, with broad applicability in real-world\nscenarios.\n","authors":["Chong-Yang Xiang","Jun-Yan He","Zhi-Qi Cheng","Xiao Wu","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2410.09583v4.pdf","comment":"Accepted to AAAI 2025, 9 pages, 6 figures. Code:\n  https://github.com/teslatasy/POPoS"},{"id":"http://arxiv.org/abs/2410.20815v2","updated":"2024-12-19T07:19:52Z","published":"2024-10-28T08:02:34Z","title":"Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian\n  Splatting","summary":"  Recently, Gaussian splatting has received more and more attention in the\nfield of static scene rendering. Due to the low computational overhead and\ninherent flexibility of explicit representations, plane-based explicit methods\nare popular ways to predict deformations for Gaussian-based dynamic scene\nrendering models. However, plane-based methods rely on the inappropriate\nlow-rank assumption and excessively decompose the space-time 4D encoding,\nresulting in overmuch feature overlap and unsatisfactory rendering quality. To\ntackle these problems, we propose Grid4D, a dynamic scene rendering model based\non Gaussian splatting and employing a novel explicit encoding method for the 4D\ninput through the hash encoding. Different from plane-based explicit\nrepresentations, we decompose the 4D encoding into one spatial and three\ntemporal 3D hash encodings without the low-rank assumption. Additionally, we\ndesign a novel attention module that generates the attention scores in a\ndirectional range to aggregate the spatial and temporal features. The\ndirectional attention enables Grid4D to more accurately fit the diverse\ndeformations across distinct scene components based on the spatial encoded\nfeatures. Moreover, to mitigate the inherent lack of smoothness in explicit\nrepresentation methods, we introduce a smooth regularization term that keeps\nour model from the chaos of deformation prediction. Our experiments demonstrate\nthat Grid4D significantly outperforms the state-of-the-art models in visual\nquality and rendering speed.\n","authors":["Jiawei Xu","Zexin Fan","Jian Yang","Jin Xie"],"pdf_url":"https://arxiv.org/pdf/2410.20815v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14587v1","updated":"2024-12-19T07:13:15Z","published":"2024-12-19T07:13:15Z","title":"Spike2Former: Efficient Spiking Transformer for High-performance Image\n  Segmentation","summary":"  Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly\nin image segmentation tasks. The reason is that directly converting neural\nnetworks with complex architectural designs for segmentation tasks into spiking\nversions leads to performance degradation and non-convergence. To address this\nchallenge, we first identify the modules in the architecture design that lead\nto the severe reduction in spike firing, make targeted improvements, and\npropose Spike2Former architecture. Second, we propose normalized integer\nspiking neurons to solve the training stability problem of SNNs with complex\narchitectures. We set a new state-of-the-art for SNNs in various semantic\nsegmentation datasets, with a significant improvement of +12.7% mIoU and 5.0\nefficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU\nand 6.6 efficiency on CityScapes.\n","authors":["Zhenxin Lei","Man Yao","Jiakui Hu","Xinhao Luo","Yanye Lu","Bo Xu","Guoqi Li"],"pdf_url":"https://arxiv.org/pdf/2412.14587v1.pdf","comment":"This work has been accepted on Association for the Advancement of\n  Artificial Intelligence 2025"},{"id":"http://arxiv.org/abs/2412.14585v1","updated":"2024-12-19T07:06:25Z","published":"2024-12-19T07:06:25Z","title":"HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video\n  Captioning","summary":"  With the growing demand for solutions to real-world video challenges,\ninterest in dense video captioning (DVC) has been on the rise. DVC involves the\nautomatic captioning and localization of untrimmed videos. Several studies\nhighlight the challenges of DVC and introduce improved methods utilizing prior\nknowledge, such as pre-training and external memory. In this research, we\npropose a model that leverages the prior knowledge of human-oriented\nhierarchical compact memory inspired by human memory hierarchy and cognition.\nTo mimic human-like memory recall, we construct a hierarchical memory and a\nhierarchical memory reading module. We build an efficient hierarchical compact\nmemory by employing clustering of memory events and summarization using large\nlanguage models. Comparative experiments demonstrate that this hierarchical\nmemory recall process improves the performance of DVC by achieving\nstate-of-the-art performance on YouCook2 and ViTT datasets.\n","authors":["Minkuk Kim","Hyeon Bae Kim","Jinyoung Moon","Jinwoo Choi","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2412.14585v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.14580v1","updated":"2024-12-19T07:00:03Z","published":"2024-12-19T07:00:03Z","title":"DiffSim: Taming Diffusion Models for Evaluating Visual Similarity","summary":"  Diffusion models have fundamentally transformed the field of generative\nmodels, making the assessment of similarity between customized model outputs\nand reference inputs critically important. However, traditional perceptual\nsimilarity metrics operate primarily at the pixel and patch levels, comparing\nlow-level colors and textures but failing to capture mid-level similarities and\ndifferences in image layout, object pose, and semantic content. Contrastive\nlearning-based CLIP and self-supervised learning-based DINO are often used to\nmeasure semantic similarity, but they highly compress image features,\ninadequately assessing appearance details. This paper is the first to discover\nthat pretrained diffusion models can be utilized for measuring visual\nsimilarity and introduces the DiffSim method, addressing the limitations of\ntraditional metrics in capturing perceptual consistency in custom generation\ntasks. By aligning features in the attention layers of the denoising U-Net,\nDiffSim evaluates both appearance and style similarity, showing superior\nalignment with human visual preferences. Additionally, we introduce the Sref\nand IP benchmarks to evaluate visual similarity at the level of style and\ninstance, respectively. Comprehensive evaluations across multiple benchmarks\ndemonstrate that DiffSim achieves state-of-the-art performance, providing a\nrobust tool for measuring visual coherence in generative models.\n","authors":["Yiren Song","Xiaokang Liu","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2412.14580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14579v1","updated":"2024-12-19T06:57:37Z","published":"2024-12-19T06:57:37Z","title":"GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D\n  Gaussian Splatting","summary":"  3D occupancy perception is gaining increasing attention due to its capability\nto offer detailed and precise environment representations. Previous\nweakly-supervised NeRF methods balance efficiency and accuracy, with mIoU\nvarying by 5-10 points due to sampling count along camera rays. Recently,\nreal-time Gaussian splatting has gained widespread popularity in 3D\nreconstruction, and the occupancy prediction task can also be viewed as a\nreconstruction task. Consequently, we propose GSRender, which naturally employs\n3D Gaussian Splatting for occupancy prediction, simplifying the sampling\nprocess. In addition, the limitations of 2D supervision result in duplicate\npredictions along the same camera ray. We implemented the Ray Compensation (RC)\nmodule, which mitigates this issue by compensating for features from adjacent\nframes. Finally, we redesigned the loss to eliminate the impact of dynamic\nobjects from adjacent frames. Extensive experiments demonstrate that our\napproach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while\nnarrowing the gap with 3D supervision methods. Our code will be released soon.\n","authors":["Qianpu Sun","Changyong Shu","Sifan Zhou","Zichen Yu","Yan Chen","Dawei Yang","Yuan Chun"],"pdf_url":"https://arxiv.org/pdf/2412.14579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14576v1","updated":"2024-12-19T06:52:12Z","published":"2024-12-19T06:52:12Z","title":"Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and\n  Progressive Correlation Network","summary":"  Alignment-free RGB-Thermal (RGB-T) salient object detection (SOD) aims to\nachieve robust performance in complex scenes by directly leveraging the\ncomplementary information from unaligned visible-thermal image pairs, without\nrequiring manual alignment. However, the labor-intensive process of collecting\nand annotating image pairs limits the scale of existing benchmarks, hindering\nthe advancement of alignment-free RGB-T SOD. In this paper, we construct a\nlarge-scale and high-diversity unaligned RGB-T SOD dataset named UVT20K,\ncomprising 20,000 image pairs, 407 scenes, and 1256 object categories. All\nsamples are collected from real-world scenarios with various challenges, such\nas low illumination, image clutter, complex salient objects, and so on. To\nsupport the exploration for further research, each sample in UVT20K is\nannotated with a comprehensive set of ground truths, including saliency masks,\nscribbles, boundaries, and challenge attributes. In addition, we propose a\nProgressive Correlation Network (PCNet), which models inter- and intra-modal\ncorrelations on the basis of explicit alignment to achieve accurate predictions\nin unaligned image pairs. Extensive experiments conducted on unaligned and\naligned datasets demonstrate the effectiveness of our method.Code and dataset\nare available at https://github.com/Angknpng/PCNet.\n","authors":["Kunpeng Wang","Keke Chen","Chenglong Li","Zhengzheng Tu","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2412.14576v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14571v1","updated":"2024-12-19T06:42:25Z","published":"2024-12-19T06:42:25Z","title":"SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar\n  Object Detection","summary":"  3D object detection is one of the fundamental perception tasks for autonomous\nvehicles. Fulfilling such a task with a 4D millimeter-wave radar is very\nattractive since the sensor is able to acquire 3D point clouds similar to Lidar\nwhile maintaining robust measurements under adverse weather. However, due to\nthe high sparsity and noise associated with the radar point clouds, the\nperformance of the existing methods is still much lower than expected. In this\npaper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation\n(SCKD) method for 4D radar-based 3D object detection. It characterizes the\ncapability of learning the feature from a Lidar-radar-fused teacher network\nwith semi-supervised distillation. We first propose an adaptive fusion module\nin the teacher network to boost its performance. Then, two feature distillation\nmodules are designed to facilitate the cross-modality knowledge transfer.\nFinally, a semi-supervised output distillation is proposed to increase the\neffectiveness and flexibility of the distillation framework. With the same\nnetwork structure, our radar-only student trained by SCKD boosts the mAP by\n10.38% over the baseline and outperforms the state-of-the-art works on the VoD\ndataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the\nmoderate difficulty level over the baseline when extra unlabeled data are\navailable. Code is available at https://github.com/Ruoyu-Xu/SCKD.\n","authors":["Ruoyu Xu","Zhiyu Xiang","Chenwei Zhang","Hanzhi Zhong","Xijun Zhao","Ruina Dang","Peng Xu","Tianyu Pu","Eryun Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14571v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.04749v2","updated":"2024-12-19T06:41:40Z","published":"2024-10-07T04:59:08Z","title":"LLaVA Needs More Knowledge: Retrieval Augmented Natural Language\n  Generation with Knowledge Graph for Explaining Thoracic Pathologies","summary":"  Generating Natural Language Explanations (NLEs) for model predictions on\nmedical images, particularly those depicting thoracic pathologies, remains a\ncritical and challenging task. Existing methodologies often struggle due to\ngeneral models' insufficient domain-specific medical knowledge and privacy\nconcerns associated with retrieval-based augmentation techniques. To address\nthese issues, we propose a novel Vision-Language framework augmented with a\nKnowledge Graph (KG)-based datastore, which enhances the model's understanding\nby incorporating additional domain-specific medical knowledge essential for\ngenerating accurate and informative NLEs. Our framework employs a KG-based\nretrieval mechanism that not only improves the precision of the generated\nexplanations but also preserves data privacy by avoiding direct data retrieval.\nThe KG datastore is designed as a plug-and-play module, allowing for seamless\nintegration with various model architectures. We introduce and evaluate three\ndistinct frameworks within this paradigm: KG-LLaVA, which integrates the\npre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining\nMedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts\nLLaVA by incorporating the Bio-ViT-L vision model. These frameworks are\nvalidated on the MIMIC-NLE dataset, where they achieve state-of-the-art\nresults, underscoring the effectiveness of KG augmentation in generating\nhigh-quality NLEs for thoracic pathologies.\n","authors":["Ameer Hamza"," Abdullah","Yong Hyun Ahn","Sungyoung Lee","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04749v2.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.14568v1","updated":"2024-12-19T06:39:28Z","published":"2024-12-19T06:39:28Z","title":"Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF\n  Separation","summary":"  Recent learning-based Multi-View Stereo models have demonstrated\nstate-of-the-art performance in sparse-view 3D reconstruction. However,\ndirectly applying 3D Gaussian Splatting (3DGS) as a refinement step following\nthese models presents challenges. We hypothesize that the excessive positional\ndegrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting\ncolor patterns at the cost of structural fidelity. To address this, we propose\nreprojection-based DoF separation, a method distinguishing positional DoFs in\nterms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To\nindependently manage each DoF, we introduce a reprojection process along with\ntailored constraints for each DoF. Through experiments across various datasets,\nwe confirm that separating the positional DoFs of Gaussians and applying\ntargeted constraints effectively suppresses geometric artifacts, producing\nreconstruction results that are both visually and geometrically plausible.\n","authors":["Yongsung Kim","Minjun Park","Jooyoung Choi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.14568v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.11530v2","updated":"2024-12-19T06:32:22Z","published":"2024-12-16T08:08:35Z","title":"RoMeO: Robust Metric Visual Odometry","summary":"  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a\nfundamental building block for many applications such as VR/AR and robotics.\nThis work focuses on monocular RGB VO where the input is a monocular RGB video\nwithout IMU or 3D sensors. Existing approaches lack robustness under this\nchallenging scenario and fail to generalize to unseen data (especially\noutdoors); they also cannot recover metric-scale poses. We propose Robust\nMetric Visual Odometry (RoMeO), a novel method that resolves these issues\nleveraging priors from pre-trained depth models. RoMeO incorporates both\nmonocular metric depth and multi-view stereo (MVS) models to recover\nmetric-scale, simplify correspondence search, provide better initialization and\nregularize optimization. Effective strategies are proposed to inject noise\nduring training and adaptively filter noisy depth priors, which ensure the\nrobustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the\nstate-of-the-art (SOTA) by a large margin across 6 diverse datasets covering\nboth indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO\nreduces the relative (align the trajectory scale with GT) and absolute\ntrajectory errors both by >50%. The performance gain also transfers to the full\nSLAM pipeline (with global BA & loop closure). Code will be released upon\nacceptance.\n","authors":["Junda Cheng","Zhipeng Cai","Zhaoxing Zhang","Wei Yin","Matthias Muller","Michael Paulitsch","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11973v6","updated":"2024-12-19T06:29:38Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) and Task-agnostic\nIncremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning\n(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is\ndesigned to prevent overfitting when the data samples are scarce. Furthermore,\nthe sparse reuse of WSN weights is considered for Video Incremental Learning\n(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It\nenables compact encoding of videos and identifies reusable subnetworks across\nvarying bandwidths. We have integrated FSO into different architectural\nframeworks for continual learning, including VIL, TIL, and FSCIL. Our\ncomprehensive experiments demonstrate FSO's effectiveness, significantly\nimproving task performance at various convolutional representational levels.\nSpecifically, FSO enhances higher-layer performance in TIL and FSCIL and\nlower-layer performance in VIL.\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v6.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (T-PAMI)"},{"id":"http://arxiv.org/abs/2412.14561v1","updated":"2024-12-19T06:26:16Z","published":"2024-12-19T06:26:16Z","title":"GBRIP: Granular Ball Representation for Imbalanced Partial Label\n  Learning","summary":"  Partial label learning (PLL) is a complicated weakly supervised\nmulti-classification task compounded by class imbalance. Currently, existing\nmethods only rely on inter-class pseudo-labeling from inter-class features,\noften overlooking the significant impact of the intra-class imbalanced features\ncombined with the inter-class. To address these limitations, we introduce\nGranular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for\nimbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and\nmulti-center loss to construct a granular ball-based nfeature space through\nunsupervised learning, effectively capturing the feature distribution within\neach class. GBRIP mitigates the impact of confusing features by systematically\nrefining label disambiguation and estimating imbalance distributions. The novel\nmulti-center loss function enhances learning by emphasizing the relationships\nbetween samples and their respective centers within the granular balls.\nExtensive experiments on standard benchmarks demonstrate that GBRIP outperforms\nexisting state-of-the-art methods, offering a robust solution to the challenges\nof imbalanced PLL.\n","authors":["Jintao Huang","Yiu-ming Cheung","Chi-man Vong","Wenbin Qian"],"pdf_url":"https://arxiv.org/pdf/2412.14561v1.pdf","comment":"AAAI25"},{"id":"http://arxiv.org/abs/2409.01179v3","updated":"2024-12-19T06:26:04Z","published":"2024-09-02T11:19:54Z","title":"Recoverable Compression: A Multimodal Vision Token Recovery Mechanism\n  Guided by Text Information","summary":"  With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance.\n","authors":["Yi Chen","Jian Xu","Xu-Yao Zhang","Wen-Zhuo Liu","Yang-Yang Liu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01179v3.pdf","comment":"AAAI2025 Accepted"},{"id":"http://arxiv.org/abs/2403.10650v3","updated":"2024-12-19T06:25:45Z","published":"2024-03-15T19:35:10Z","title":"PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time\n  Adaptation","summary":"  Real-world vision models in dynamic environments face rapid shifts in domain\ndistributions, leading to decreased recognition performance. Using unlabeled\ntest data, continuous test-time adaptation (CTTA) directly adjusts a\npre-trained source discriminative model to these changing domains. A highly\neffective CTTA method involves applying layer-wise adaptive learning rates for\nselectively adapting pre-trained layers. However, it suffers from the poor\nestimation of domain shift and the inaccuracies arising from the pseudo-labels.\nThis work aims to overcome these limitations by identifying layers for\nadaptation via quantifying model prediction uncertainty without relying on\npseudo-labels. We utilize the magnitude of gradients as a metric, calculated by\nbackpropagating the KL divergence between the softmax output and a uniform\ndistribution, to select layers for further adaptation. Subsequently, for the\nparameters exclusively belonging to these selected layers, with the remaining\nones frozen, we evaluate their sensitivity to approximate the domain shift and\nadjust their learning rates accordingly. We conduct extensive image\nclassification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,\ndemonstrating the superior efficacy of our method compared to prior approaches.\n","authors":["Sarthak Kumar Maharana","Baoming Zhang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10650v3.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14559v1","updated":"2024-12-19T06:22:19Z","published":"2024-12-19T06:22:19Z","title":"ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation\n  Model","summary":"  The scaling law has been validated in various domains, such as natural\nlanguage processing (NLP) and massive computer vision tasks; however, its\napplication to motion generation remains largely unexplored. In this paper, we\nintroduce a scalable motion generation framework that includes the motion\ntokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through\ncomprehensive experiments, we observe the scaling behavior of this system. For\nthe first time, we confirm the existence of scaling laws within the context of\nmotion generation. Specifically, our results demonstrate that the normalized\ntest loss of our prefix autoregressive models adheres to a logarithmic law in\nrelation to compute budgets. Furthermore, we also confirm the power law between\nNon-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect\nto compute budgets respectively. Leveraging the scaling law, we predict the\noptimal transformer size, vocabulary size, and data requirements for a compute\nbudget of $1e18$. The test loss of the system, when trained with the optimal\nmodel size, vocabulary size, and required data, aligns precisely with the\npredicted test loss, thereby validating the scaling law.\n","authors":["Shunlin Lu","Jingbo Wang","Zeyu Lu","Ling-Hao Chen","Wenxun Dai","Junting Dong","Zhiyang Dou","Bo Dai","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.14559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20633v3","updated":"2024-12-19T06:22:05Z","published":"2024-05-31T05:49:37Z","title":"Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust\n  Out-of-Distribution Human Action Detection","summary":"  Human action recognition is crucial in computer vision systems. However, in\nreal-world scenarios, human actions often fall outside the distribution of\ntraining data, requiring a model to both recognize in-distribution (ID) actions\nand reject out-of-distribution (OOD) ones. Despite its importance, there has\nbeen limited research on OOD detection in human actions. Existing works on OOD\ndetection mainly focus on image data with RGB structure, and many methods are\npost-hoc in nature. While these methods are convenient and computationally\nefficient, they often lack sufficient accuracy, fail to consider the exposure\nof OOD samples, and ignore the application in skeleton structure data. To\naddress these challenges, we propose a novel end-to-end skeleton-based model\ncalled Skeleton-OOD, which is committed to improving the effectiveness of OOD\ntasks while ensuring the accuracy of ID recognition. Through extensive\nexperiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400\ndatasets, Skeleton-OOD demonstrates the superior performance of our proposed\napproach compared to state-of-the-art methods. Our findings underscore the\neffectiveness of classic OOD detection techniques in the context of\nskeleton-based action recognition tasks, offering promising avenues for future\nresearch in this field. Code is available at\nhttps://github.com/YilliaJing/Skeleton-OOD.git.\n","authors":["Jing Xu","Anqi Zhu","Jingyu Lin","Qiuhong Ke","Cunjian Chen"],"pdf_url":"https://arxiv.org/pdf/2405.20633v3.pdf","comment":"Accepted by Neurocomputing"},{"id":"http://arxiv.org/abs/2412.14547v1","updated":"2024-12-19T05:55:18Z","published":"2024-12-19T05:55:18Z","title":"Bright-NeRF:Brightening Neural Radiance Field with Color Restoration\n  from Low-light Raw Images","summary":"  Neural Radiance Fields (NeRFs) have demonstrated prominent performance in\nnovel view synthesis. However, their input heavily relies on image acquisition\nunder normal light conditions, making it challenging to learn accurate scene\nrepresentation in low-light environments where images typically exhibit\nsignificant noise and severe color distortion. To address these challenges, we\npropose a novel approach, Bright-NeRF, which learns enhanced and high-quality\nradiance fields from multi-view low-light raw images in an unsupervised manner.\nOur method simultaneously achieves color restoration, denoising, and enhanced\nnovel view synthesis. Specifically, we leverage a physically-inspired model of\nthe sensor's response to illumination and introduce a chromatic adaptation loss\nto constrain the learning of response, enabling consistent color perception of\nobjects regardless of lighting conditions. We further utilize the raw data's\nproperties to expose the scene's intensity automatically. Additionally, we have\ncollected a multi-view low-light raw image dataset to advance research in this\nfield. Experimental results demonstrate that our proposed method significantly\noutperforms existing 2D and 3D approaches. Our code and dataset will be made\npublicly available.\n","authors":["Min Wang","Xin Huang","Guoqing Zhou","Qifeng Guo","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14547v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14546v1","updated":"2024-12-19T05:52:16Z","published":"2024-12-19T05:52:16Z","title":"{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation","summary":"  Small lesions play a critical role in early disease diagnosis and\nintervention of severe infections. Popular models often face challenges in\nsegmenting small lesions, as it occupies only a minor portion of an image,\nwhile down\\_sampling operations may inevitably lose focus on local features of\nsmall lesions. To tackle the challenges, we propose a {\\bf S}mall-{\\bf\nS}ize-{\\bf S}ensitive {\\bf Mamba} ({\\bf S$^3$-Mamba}), which promotes the\nsensitivity to small lesions across three dimensions: channel, spatial, and\ntraining strategy. Specifically, an Enhanced Visual State Space block is\ndesigned to focus on small lesions through multiple residual connections to\npreserve local features, and selectively amplify important details while\nsuppressing irrelevant ones through channel-wise attention. A Tensor-based\nCross-feature Multi-scale Attention is designed to integrate input image\nfeatures and intermediate-layer features with edge features and exploit the\nattentive support of features across multiple scales, thereby retaining spatial\ndetails of small lesions at various granularities. Finally, we introduce a\nnovel regularized curriculum learning to automatically assess lesion size and\nsample difficulty, and gradually focus from easy samples to hard ones like\nsmall lesions. Extensive experiments on three medical image segmentation\ndatasets show the superiority of our S$^3$-Mamba, especially in segmenting\nsmall lesions. Our code is available at\nhttps://github.com/ErinWang2023/S3-Mamba.\n","authors":["Gui Wang","Yuexiang Li","Wenting Chen","Meidan Ding","Wooi Ping Cheah","Rong Qu","Jianfeng Ren","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14546v1.pdf","comment":"Accept by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14545v1","updated":"2024-12-19T05:51:46Z","published":"2024-12-19T05:51:46Z","title":"Summary of Point Transformer with Federated Learning for Predicting\n  Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide\n  Images","summary":"  This study introduces a federated learning-based approach to predict HER2\nstatus from hematoxylin and eosin (HE)-stained whole slide images (WSIs),\nreducing costs and speeding up treatment decisions. To address label imbalance\nand feature representation challenges in multisite datasets, a point\ntransformer is proposed, incorporating dynamic label distribution, an auxiliary\nclassifier, and farthest cosine sampling. Extensive experiments demonstrate\nstate-of-the-art performance across four sites (2687 WSIs) and strong\ngeneralization to two unseen sites (229 WSIs).\n","authors":["Kamorudeen A. Amuda","Almustapha A. Wakili"],"pdf_url":"https://arxiv.org/pdf/2412.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16214v2","updated":"2024-12-19T05:50:06Z","published":"2024-07-23T06:42:55Z","title":"Diff-Shadow: Global-guided Diffusion Model for Shadow Removal","summary":"  We propose Diff-Shadow, a global-guided diffusion model for shadow removal.\nPrevious transformer-based approaches can utilize global information to relate\nshadow and non-shadow regions but are limited in their synthesis ability and\nrecover images with obvious boundaries. In contrast, diffusion-based methods\ncan generate better content but they are not exempt from issues related to\ninconsistent illumination. In this work, we combine the advantages of diffusion\nmodels and global guidance to achieve shadow-free restoration. Specifically, we\npropose a parallel UNets architecture: 1) the local branch performs the\npatch-based noise estimation in the diffusion process, and 2) the global branch\nrecovers the low-resolution shadow-free images. A Reweight Cross Attention\n(RCA) module is designed to integrate global contextual information of\nnon-shadow regions into the local branch. We further design a Global-guided\nSampling Strategy (GSS) that mitigates patch boundary issues and ensures\nconsistent illumination across shaded and unshaded regions in the recovered\nimage. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have\ndemonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art\nmethods, our method achieves a significant improvement in terms of PSNR,\nincreasing from 32.33dB to 33.69dB on the ISTD dataset.\n","authors":["Jinting Luo","Ru Li","Chengzhi Jiang","Xiaoming Zhang","Mingyan Han","Ting Jiang","Haoqiang Fan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.16214v2.pdf","comment":"Proceedings of the 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.08125v2","updated":"2024-12-19T05:46:29Z","published":"2024-12-11T06:21:33Z","title":"Progressive Multi-granular Alignments for Grounded Reasoning in Large\n  Vision-Language Models","summary":"  Existing Large Vision-Language Models (LVLMs) excel at matching concepts\nacross multi-modal inputs but struggle with compositional concepts and\nhigh-level relationships between entities. This paper introduces Progressive\nmulti-granular Vision-Language alignments (PromViL), a novel framework to\nenhance LVLMs' ability in performing grounded compositional visual reasoning\ntasks. Our approach constructs a hierarchical structure of multi-modal\nalignments, ranging from simple to complex concepts. By progressively aligning\ntextual descriptions with corresponding visual regions, our model learns to\nleverage contextual information from lower levels to inform higher-level\nreasoning. To facilitate this learning process, we introduce a data generation\nprocess that creates a novel dataset derived from Visual Genome, providing a\nwide range of nested compositional vision-language pairs. Experimental results\ndemonstrate that our PromViL framework significantly outperforms baselines on\nvarious visual grounding and compositional question answering tasks. The code\nis available at: https://github.com/lqh52/PromViL.\n","authors":["Quang-Hung Le","Long Hoang Dang","Ngan Le","Truyen Tran","Thao Minh Le"],"pdf_url":"https://arxiv.org/pdf/2412.08125v2.pdf","comment":null}],"Graphics":[{"id":"http://arxiv.org/abs/2412.15216v1","updated":"2024-12-19T18:59:58Z","published":"2024-12-19T18:59:58Z","title":"UIP2P: Unsupervised Instruction-based Image Editing via Cycle Edit\n  Consistency","summary":"  We propose an unsupervised model for instruction-based image editing that\neliminates the need for ground-truth edited images during training. Existing\nsupervised methods depend on datasets containing triplets of input image,\nedited image, and edit instruction. These are generated by either existing\nediting methods or human-annotations, which introduce biases and limit their\ngeneralization ability. Our method addresses these challenges by introducing a\nnovel editing mechanism called Cycle Edit Consistency (CEC), which applies\nforward and backward edits in one training step and enforces consistency in\nimage and attention spaces. This allows us to bypass the need for ground-truth\nedited images and unlock training for the first time on datasets comprising\neither real image-caption pairs or image-caption-edit triplets. We empirically\nshow that our unsupervised technique performs better across a broader range of\nedits with high fidelity and precision. By eliminating the need for\npre-existing datasets of triplets, reducing biases associated with supervised\nmethods, and proposing CEC, our work represents a significant advancement in\nunblocking scaling of instruction-based image editing.\n","authors":["Enis Simsar","Alessio Tonioni","Yongqin Xian","Thomas Hofmann","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2412.15216v1.pdf","comment":"Project page: https://enis.dev/uip2p/"},{"id":"http://arxiv.org/abs/2412.15215v1","updated":"2024-12-19T18:59:57Z","published":"2024-12-19T18:59:57Z","title":"EnvGS: Modeling View-Dependent Appearance with Environment Gaussian","summary":"  Reconstructing complex reflections in real-world scenes from 2D images is\nessential for achieving photorealistic novel view synthesis. Existing methods\nthat utilize environment maps to model reflections from distant lighting often\nstruggle with high-frequency reflection details and fail to account for\nnear-field reflections. In this work, we introduce EnvGS, a novel approach that\nemploys a set of Gaussian primitives as an explicit 3D representation for\ncapturing reflections of environments. These environment Gaussian primitives\nare incorporated with base Gaussian primitives to model the appearance of the\nwhole scene. To efficiently render these environment Gaussian primitives, we\ndeveloped a ray-tracing-based renderer that leverages the GPU's RT core for\nfast rendering. This allows us to jointly optimize our model for high-quality\nreconstruction while maintaining real-time rendering speeds. Results from\nmultiple real-world and synthetic datasets demonstrate that our method produces\nsignificantly more detailed reflections, achieving the best rendering quality\nin real-time novel view synthesis.\n","authors":["Tao Xie","Xi Chen","Zhen Xu","Yiman Xie","Yudong Jin","Yujun Shen","Sida Peng","Hujun Bao","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15215v1.pdf","comment":"Project page: https://zju3dv.github.io/envgs/"},{"id":"http://arxiv.org/abs/2412.15213v1","updated":"2024-12-19T18:59:56Z","published":"2024-12-19T18:59:56Z","title":"Flowing from Words to Pixels: A Framework for Cross-Modality Evolution","summary":"  Diffusion models, and their generalization, flow matching, have had a\nremarkable impact on the field of media generation. Here, the conventional\napproach is to learn the complex mapping from a simple source distribution of\nGaussian noise to the target media distribution. For cross-modal tasks such as\ntext-to-image generation, this same mapping from noise to image is learnt\nwhilst including a conditioning mechanism in the model. One key and thus far\nrelatively unexplored feature of flow matching is that, unlike Diffusion\nmodels, they are not constrained for the source distribution to be noise.\nHence, in this paper, we propose a paradigm shift, and ask the question of\nwhether we can instead train flow matching models to learn a direct mapping\nfrom the distribution of one modality to the distribution of another, thus\nobviating the need for both the noise distribution and conditioning mechanism.\nWe present a general and simple framework, CrossFlow, for cross-modal flow\nmatching. We show the importance of applying Variational Encoders to the input\ndata, and introduce a method to enable Classifier-free guidance. Surprisingly,\nfor text-to-image, CrossFlow with a vanilla transformer without cross attention\nslightly outperforms standard flow matching, and we show that it scales better\nwith training steps and model size, while also allowing for interesting latent\narithmetic which results in semantically meaningful edits in the output space.\nTo demonstrate the generalizability of our approach, we also show that\nCrossFlow is on par with or outperforms the state-of-the-art for various\ncross-modal / intra-modal mapping tasks, viz. image captioning, depth\nestimation, and image super-resolution. We hope this paper contributes to\naccelerating progress in cross-modal media generation.\n","authors":["Qihao Liu","Xi Yin","Alan Yuille","Andrew Brown","Mannat Singh"],"pdf_url":"https://arxiv.org/pdf/2412.15213v1.pdf","comment":"Project page: https://cross-flow.github.io/"},{"id":"http://arxiv.org/abs/2412.15214v1","updated":"2024-12-19T18:59:56Z","published":"2024-12-19T18:59:56Z","title":"LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis","summary":"  The intuitive nature of drag-based interaction has led to its growing\nadoption for controlling object trajectories in image-to-video synthesis.\nStill, existing methods that perform dragging in the 2D space usually face\nambiguity when handling out-of-plane movements. In this work, we augment the\ninteraction with a new dimension, i.e., the depth dimension, such that users\nare allowed to assign a relative depth for each point on the trajectory. That\nway, our new interaction paradigm not only inherits the convenience from 2D\ndragging, but facilitates trajectory control in the 3D space, broadening the\nscope of creativity. We propose a pioneering method for 3D trajectory control\nin image-to-video synthesis by abstracting object masks into a few cluster\npoints. These points, accompanied by the depth information and the instance\ninformation, are finally fed into a video diffusion model as the control\nsignal. Extensive experiments validate the effectiveness of our approach,\ndubbed LeviTor, in precisely manipulating the object movements when producing\nphoto-realistic videos from static images. Project page:\nhttps://ppetrichor.github.io/levitor.github.io/\n","authors":["Hanlin Wang","Hao Ouyang","Qiuyu Wang","Wen Wang","Ka Leong Cheng","Qifeng Chen","Yujun Shen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2412.15214v1.pdf","comment":"Project page available at\n  https://ppetrichor.github.io/levitor.github.io/"},{"id":"http://arxiv.org/abs/2412.15211v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Generative Multiview Relighting for 3D Reconstruction under Extreme\n  Illumination Variation","summary":"  Reconstructing the geometry and appearance of objects from photographs taken\nin different environments is difficult as the illumination and therefore the\nobject appearance vary across captured images. This is particularly challenging\nfor more specular objects whose appearance strongly depends on the viewing\ndirection. Some prior approaches model appearance variation across images using\na per-image embedding vector, while others use physically-based rendering to\nrecover the materials and per-image illumination. Such approaches fail at\nfaithfully recovering view-dependent appearance given significant variation in\ninput illumination and tend to produce mostly diffuse results. We present an\napproach that reconstructs objects from images taken under different\nilluminations by first relighting the images under a single reference\nillumination with a multiview relighting diffusion model and then\nreconstructing the object's geometry and appearance with a radiance field\narchitecture that is robust to the small remaining inconsistencies among the\nrelit images. We validate our proposed approach on both synthetic and real\ndatasets and demonstrate that it greatly outperforms existing techniques at\nreconstructing high-fidelity appearance from images taken under extreme\nillumination variation. Moreover, our approach is particularly effective at\nrecovering view-dependent \"shiny\" appearance which cannot be reconstructed by\nprior methods.\n","authors":["Hadi Alzayer","Philipp Henzler","Jonathan T. Barron","Jia-Bin Huang","Pratul P. Srinivasan","Dor Verbin"],"pdf_url":"https://arxiv.org/pdf/2412.15211v1.pdf","comment":"Project page: https://relight-to-reconstruct.github.io/"},{"id":"http://arxiv.org/abs/2412.15212v1","updated":"2024-12-19T18:59:51Z","published":"2024-12-19T18:59:51Z","title":"Scaling 4D Representations","summary":"  Scaling has not yet been convincingly demonstrated for pure self-supervised\nlearning from video. However, prior work has focused evaluations on\nsemantic-related tasks $\\unicode{x2013}$ action classification, ImageNet\nclassification, etc. In this paper we focus on evaluating self-supervised\nlearning on non-semantic vision tasks that are more spatial (3D) and temporal\n(+1D = 4D), such as camera pose estimation, point and object tracking, and\ndepth estimation. We show that by learning from very large video datasets,\nmasked auto-encoding (MAE) with transformer video models actually scales,\nconsistently improving performance on these 4D tasks, as model size increases\nfrom 20M all the way to the largest by far reported self-supervised video model\n$\\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with\nmany recent image and video models demonstrates the benefits of scaling 4D\nrepresentations.\n","authors":["João Carreira","Dilara Gokay","Michael King","Chuhan Zhang","Ignacio Rocco","Aravindh Mahendran","Thomas Albert Keck","Joseph Heyward","Skanda Koppula","Etienne Pot","Goker Erdogan","Yana Hasson","Yi Yang","Klaus Greff","Guillaume Le Moing","Sjoerd van Steenkiste","Daniel Zoran","Drew A. Hudson","Pedro Vélez","Luisa Polanía","Luke Friedman","Chris Duvarney","Ross Goroshin","Kelsey Allen","Jacob Walker","Rishabh Kabra","Eric Aboussouan","Jennifer Sun","Thomas Kipf","Carl Doersch","Viorica Pătrăucean","Dima Damen","Pauline Luc","Mehdi S. M. Sajjadi","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2412.15212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15209v1","updated":"2024-12-19T18:59:44Z","published":"2024-12-19T18:59:44Z","title":"PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation","summary":"  Despite significant advancements in Large Vision-Language Models (LVLMs),\nexisting pixel-grounding models operate on single-image settings, limiting\ntheir ability to perform detailed, fine-grained comparisons across multiple\nimages. Conversely, current multi-image understanding models lack pixel-level\ngrounding. Our work addresses this gap by introducing the task of multi-image\npixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates\npixel-level grounding with robust multi-image reasoning capabilities to produce\ncontextually rich, pixel-grounded explanations. Central to PRIMA is an\nefficient vision module that queries fine-grained visual representations across\nmultiple images, reducing TFLOPs by $25.3\\%$. To support training and\nevaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark\nconsisting of $\\sim$224K question-answer pairs that require fine-grained visual\nunderstanding across multiple images. Experimental results demonstrate PRIMA\noutperforms state-of-the-art baselines.\n","authors":["Muntasir Wahed","Kiet A. Nguyen","Adheesh Sunil Juvekar","Xinzhuo Li","Xiaona Zhou","Vedant Shah","Tianjiao Yu","Pinar Yanardag","Ismini Lourentzou"],"pdf_url":"https://arxiv.org/pdf/2412.15209v1.pdf","comment":"Project page: https://plan-lab.github.io/prima"},{"id":"http://arxiv.org/abs/2412.15208v1","updated":"2024-12-19T18:59:40Z","published":"2024-12-19T18:59:40Z","title":"OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving","summary":"  Since the advent of Multimodal Large Language Models (MLLMs), they have made\na significant impact across a wide range of real-world applications,\nparticularly in Autonomous Driving (AD). Their ability to process complex\nvisual data and reason about intricate driving scenarios has paved the way for\na new paradigm in end-to-end AD systems. However, the progress of developing\nend-to-end models for AD has been slow, as existing fine-tuning methods demand\nsubstantial resources, including extensive computational power, large-scale\ndatasets, and significant funding. Drawing inspiration from recent advancements\nin inference computing, we propose OpenEMMA, an open-source end-to-end\nframework based on MLLMs. By incorporating the Chain-of-Thought reasoning\nprocess, OpenEMMA achieves significant improvements compared to the baseline\nwhen leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates\neffectiveness, generalizability, and robustness across a variety of challenging\ndriving scenarios, offering a more efficient and effective approach to\nautonomous driving. We release all the codes in\nhttps://github.com/taco-group/OpenEMMA.\n","authors":["Shuo Xing","Chengyuan Qian","Yuping Wang","Hongyuan Hua","Kexin Tian","Yang Zhou","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15206v1","updated":"2024-12-19T18:59:33Z","published":"2024-12-19T18:59:33Z","title":"AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models\n  for Autonomous Driving","summary":"  Recent advancements in large vision language models (VLMs) tailored for\nautonomous driving (AD) have shown strong scene understanding and reasoning\ncapabilities, making them undeniable candidates for end-to-end driving systems.\nHowever, limited work exists on studying the trustworthiness of DriveVLMs -- a\ncritical factor that directly impacts public transportation safety. In this\npaper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for\nlarge vision-language models in autonomous driving (DriveVLMs), considering\ndiverse perspectives -- including trustfulness, safety, robustness, privacy,\nand fairness. We constructed the largest visual question-answering dataset for\ninvestigating trustworthiness issues in driving scenarios, comprising over 10k\nunique scenes and 18k queries. We evaluated six publicly available VLMs,\nspanning from generalist to specialist, from open-source to commercial models.\nOur exhaustive evaluations have unveiled previously undiscovered\nvulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found\nthat the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform\nspecialized models fine-tuned for driving in terms of overall trustworthiness.\nDriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing\nsensitive information. Additionally, both generalist and specialist VLMs remain\nsusceptible to adversarial attacks and struggle to ensure unbiased\ndecision-making across diverse environments and populations. Our findings call\nfor immediate and decisive action to address the trustworthiness of DriveVLMs\n-- an issue of critical importance to public safety and the welfare of all\ncitizens relying on autonomous transportation systems. Our benchmark is\npublicly available at \\url{https://github.com/taco-group/AutoTrust}, and the\nleaderboard is released at \\url{https://taco-group.github.io/AutoTrust/}.\n","authors":["Shuo Xing","Hongyuan Hua","Xiangbo Gao","Shenzhe Zhu","Renjie Li","Kexin Tian","Xiaopeng Li","Heng Huang","Tianbao Yang","Zhangyang Wang","Yang Zhou","Huaxiu Yao","Zhengzhong Tu"],"pdf_url":"https://arxiv.org/pdf/2412.15206v1.pdf","comment":"55 pages, 14 figures"},{"id":"http://arxiv.org/abs/2412.15205v1","updated":"2024-12-19T18:59:31Z","published":"2024-12-19T18:59:31Z","title":"FlowAR: Scale-wise Autoregressive Image Generation Meets Flow Matching","summary":"  Autoregressive (AR) modeling has achieved remarkable success in natural\nlanguage processing by enabling models to generate text with coherence and\ncontextual understanding through next token prediction. Recently, in image\ngeneration, VAR proposes scale-wise autoregressive modeling, which extends the\nnext token prediction to the next scale prediction, preserving the 2D structure\nof images. However, VAR encounters two primary challenges: (1) its complex and\nrigid scale design limits generalization in next scale prediction, and (2) the\ngenerator's dependence on a discrete tokenizer with the same complex scale\nstructure restricts modularity and flexibility in updating the tokenizer. To\naddress these limitations, we introduce FlowAR, a general next scale prediction\nmethod featuring a streamlined scale design, where each subsequent scale is\nsimply double the previous one. This eliminates the need for VAR's intricate\nmulti-scale residual tokenizer and enables the use of any off-the-shelf\nVariational AutoEncoder (VAE). Our simplified design enhances generalization in\nnext scale prediction and facilitates the integration of Flow Matching for\nhigh-quality image synthesis. We validate the effectiveness of FlowAR on the\nchallenging ImageNet-256 benchmark, demonstrating superior generation\nperformance compared to previous methods. Codes will be available at\n\\url{https://github.com/OliverRensu/FlowAR}.\n","authors":["Sucheng Ren","Qihang Yu","Ju He","Xiaohui Shen","Alan Yuille","Liang-Chieh Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15200v1","updated":"2024-12-19T18:58:46Z","published":"2024-12-19T18:58:46Z","title":"DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation\n  for High-quality 3D Asset Creation","summary":"  Procedural Content Generation (PCG) is powerful in creating high-quality 3D\ncontents, yet controlling it to produce desired shapes is difficult and often\nrequires extensive parameter tuning. Inverse Procedural Content Generation aims\nto automatically find the best parameters under the input condition. However,\nexisting sampling-based and neural network-based methods still suffer from\nnumerous sample iterations or limited controllability. In this work, we present\nDI-PCG, a novel and efficient method for Inverse PCG from general image\nconditions. At its core is a lightweight diffusion transformer model, where PCG\nparameters are directly treated as the denoising target and the observed images\nas conditions to control parameter generation. DI-PCG is efficient and\neffective. With only 7.6M network parameters and 30 GPU hours to train, it\ndemonstrates superior performance in recovering parameters accurately, and\ngeneralizing well to in-the-wild images. Quantitative and qualitative\nexperiment results validate the effectiveness of DI-PCG in inverse PCG and\nimage-to-3D generation tasks. DI-PCG offers a promising approach for efficient\ninverse PCG and represents a valuable exploration step towards a 3D generation\npath that models how to construct a 3D asset using parametric models.\n","authors":["Wang Zhao","Yan-Pei Cao","Jiale Xu","Yuejiang Dong","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2412.15200v1.pdf","comment":"Project page: https://thuzhaowang.github.io/projects/DI-PCG/"},{"id":"http://arxiv.org/abs/2412.15199v1","updated":"2024-12-19T18:58:36Z","published":"2024-12-19T18:58:36Z","title":"LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation","summary":"  This paper targets the challenge of real-time LiDAR re-simulation in dynamic\ndriving scenarios. Recent approaches utilize neural radiance fields combined\nwith the physical modeling of LiDAR sensors to achieve high-fidelity\nre-simulation results. Unfortunately, these methods face limitations due to\nhigh computational demands in large-scale scenes and cannot perform real-time\nLiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel\nframework that supports real-time, physically accurate LiDAR re-simulation for\ndriving scenes. Our primary contribution is the development of an efficient and\neffective rendering pipeline, which integrates Gaussian primitives and\nhardware-accelerated ray tracing technology. Specifically, we model the\nphysical properties of LiDAR sensors using Gaussian primitives with learnable\nparameters and incorporate scene graphs to handle scene dynamics. Building upon\nthis scene representation, our framework first constructs a bounding volume\nhierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views\nthrough a differentiable rendering algorithm. Importantly, our framework\nsupports realistic rendering with flexible scene editing operations and various\nsensor configurations. Extensive experiments across multiple public benchmarks\ndemonstrate that our method outperforms state-of-the-art methods in terms of\nrendering quality and efficiency. Our project page is at\nhttps://zju3dv.github.io/lidar-rt.\n","authors":["Chenxu Zhou","Lvchang Fu","Sida Peng","Yunzhi Yan","Zhanhua Zhang","Yong Chen","Jiazhi Xia","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.15199v1.pdf","comment":"Project page: https://zju3dv.github.io/lidar-rt"},{"id":"http://arxiv.org/abs/2412.15195v1","updated":"2024-12-19T18:58:14Z","published":"2024-12-19T18:58:14Z","title":"Preventing Local Pitfalls in Vector Quantization via Optimal Transport","summary":"  Vector-quantized networks (VQNs) have exhibited remarkable performance across\nvarious tasks, yet they are prone to training instability, which complicates\nthe training process due to the necessity for techniques such as subtle\ninitialization and model distillation. In this study, we identify the local\nminima issue as the primary cause of this instability. To address this, we\nintegrate an optimal transport method in place of the nearest neighbor search\nto achieve a more globally informed assignment. We introduce OptVQ, a novel\nvector quantization method that employs the Sinkhorn algorithm to optimize the\noptimal transport problem, thereby enhancing the stability and efficiency of\nthe training process. To mitigate the influence of diverse data distributions\non the Sinkhorn algorithm, we implement a straightforward yet effective\nnormalization strategy. Our comprehensive experiments on image reconstruction\ntasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses\ncurrent state-of-the-art VQNs in reconstruction quality.\n","authors":["Borui Zhang","Wenzhao Zheng","Jie Zhou","Jiwen Lu"],"pdf_url":"https://arxiv.org/pdf/2412.15195v1.pdf","comment":"Code is available at https://github.com/zbr17/OptVQ"},{"id":"http://arxiv.org/abs/2412.15191v1","updated":"2024-12-19T18:57:21Z","published":"2024-12-19T18:57:21Z","title":"AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal\n  Audio-Video Generation","summary":"  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video\ngeneration that leverages the activations of frozen video and audio diffusion\nmodels for temporally-aligned cross-modal conditioning. The key to our\nframework is a Fusion Block that enables bidirectional information exchange\nbetween our backbone video and audio diffusion models through a\ntemporally-aligned self attention operation. Unlike prior work that uses\nfeature extractors pretrained for other tasks for the conditioning signal,\nAV-Link can directly leverage features obtained by the complementary modality\nin a single framework i.e. video features to generate audio, or audio features\nto generate video. We extensively evaluate our design choices and demonstrate\nthe ability of our method to achieve synchronized and high-quality audiovisual\ncontent, showcasing its potential for applications in immersive media\ngeneration. Project Page: snap-research.github.io/AVLink/\n","authors":["Moayed Haji-Ali","Willi Menapace","Aliaksandr Siarohin","Ivan Skorokhodov","Alper Canberk","Kwot Sin Lee","Vicente Ordonez","Sergey Tulyakov"],"pdf_url":"https://arxiv.org/pdf/2412.15191v1.pdf","comment":"Project Page: snap-research.github.io/AVLink/"},{"id":"http://arxiv.org/abs/2412.15190v1","updated":"2024-12-19T18:57:13Z","published":"2024-12-19T18:57:13Z","title":"EarthDial: Turning Multi-sensory Earth Observations to Interactive\n  Dialogues","summary":"  Automated analysis of vast Earth observation data via interactive\nVision-Language Models (VLMs) can unlock new opportunities for environmental\nmonitoring, disaster response, and resource management. Existing generic VLMs\ndo not perform well on Remote Sensing data, while the recent Geo-spatial VLMs\nremain restricted to a fixed resolution and few sensor modalities. In this\npaper, we introduce EarthDial, a conversational assistant specifically designed\nfor Earth Observation (EO) data, transforming complex, multi-sensory Earth\nobservations into interactive, natural language dialogues. EarthDial supports\nmulti-spectral, multi-temporal, and multi-resolution imagery, enabling a wide\nrange of remote sensing tasks, including classification, detection, captioning,\nquestion answering, visual reasoning, and visual grounding. To achieve this, we\nintroduce an extensive instruction tuning dataset comprising over 11.11M\ninstruction pairs covering RGB, Synthetic Aperture Radar (SAR), and\nmultispectral modalities such as Near-Infrared (NIR) and infrared. Furthermore,\nEarthDial handles bi-temporal and multi-temporal sequence analysis for\napplications like change detection. Our extensive experimental results on 37\ndownstream applications demonstrate that EarthDial outperforms existing generic\nand domain-specific models, achieving better generalization across various EO\ntasks.\n","authors":["Sagar Soni","Akshay Dudhane","Hiyam Debary","Mustansar Fiaz","Muhammad Akhtar Munir","Muhammad Sohail Danish","Paolo Fraccaro","Campbell D Watson","Levente J Klein","Fahad Shahbaz Khan","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2412.15190v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15188v1","updated":"2024-12-19T18:56:24Z","published":"2024-12-19T18:56:24Z","title":"LlamaFusion: Adapting Pretrained Language Models for Multimodal\n  Generation","summary":"  We present LlamaFusion, a framework for empowering pretrained text-only large\nlanguage models (LLMs) with multimodal generative capabilities, enabling them\nto understand and generate both text and images in arbitrary sequences.\nLlamaFusion leverages existing Llama-3's weights for processing texts\nautoregressively while introducing additional and parallel transformer modules\nfor processing images with diffusion. During training, the data from each\nmodality is routed to its dedicated modules: modality-specific feedforward\nlayers, query-key-value projections, and normalization layers process each\nmodality independently, while the shared self-attention layers allow\ninteractions across text and image features. By freezing the text-specific\nmodules and only training the image-specific modules, LlamaFusion preserves the\nlanguage capabilities of text-only LLMs while developing strong visual\nunderstanding and generation abilities. Compared to methods that pretrain\nmultimodal generative models from scratch, our experiments demonstrate that,\nLlamaFusion improves image understanding by 20% and image generation by 3.6%\nusing only 50% of the FLOPs while maintaining Llama-3's language capabilities.\nWe also demonstrate that this framework can adapt existing vision-language\nmodels with multimodal generation ability. Overall, this framework not only\nleverages existing computational investments in text-only LLMs but also enables\nthe parallel development of language and vision capabilities, presenting a\npromising direction for efficient multimodal model development.\n","authors":["Weijia Shi","Xiaochuang Han","Chunting Zhou","Weixin Liang","Xi Victoria Lin","Luke Zettlemoyer","Lili Yu"],"pdf_url":"https://arxiv.org/pdf/2412.15188v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15185v1","updated":"2024-12-19T18:55:25Z","published":"2024-12-19T18:55:25Z","title":"Tiled Diffusion","summary":"  Image tiling -- the seamless connection of disparate images to create a\ncoherent visual field -- is crucial for applications such as texture creation,\nvideo game asset development, and digital art. Traditionally, tiles have been\nconstructed manually, a method that poses significant limitations in\nscalability and flexibility. Recent research has attempted to automate this\nprocess using generative models. However, current approaches primarily focus on\ntiling textures and manipulating models for single-image generation, without\ninherently supporting the creation of multiple interconnected tiles across\ndiverse domains. This paper presents Tiled Diffusion, a novel approach that\nextends the capabilities of diffusion models to accommodate the generation of\ncohesive tiling patterns across various domains of image synthesis that require\ntiling. Our method supports a wide range of tiling scenarios, from self-tiling\nto complex many-to-many connections, enabling seamless integration of multiple\nimages. Tiled Diffusion automates the tiling process, eliminating the need for\nmanual intervention and enhancing creative possibilities in various\napplications, such as seamlessly tiling of existing images, tiled texture\ncreation, and 360{\\deg} synthesis.\n","authors":["Or Madar","Ohad Fried"],"pdf_url":"https://arxiv.org/pdf/2412.15185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07449v2","updated":"2024-12-19T18:51:28Z","published":"2024-11-12T00:20:11Z","title":"Tracing the Roots: Leveraging Temporal Dynamics in Diffusion\n  Trajectories for Origin Attribution","summary":"  Diffusion models have revolutionized image synthesis, garnering significant\nresearch interest in recent years. Diffusion is an iterative algorithm in which\nsamples are generated step-by-step, starting from pure noise. This process\nintroduces the notion of diffusion trajectories, i.e., paths from the standard\nGaussian distribution to the target image distribution. In this context, we\nstudy discriminative algorithms operating on these trajectories. Specifically,\ngiven a pre-trained diffusion model, we consider the problem of classifying\nimages as part of the training dataset, generated by the model or originating\nfrom an external source. Our approach demonstrates the presence of patterns\nacross steps that can be leveraged for classification. We also conduct ablation\nstudies, which reveal that using higher-order gradient features to characterize\nthe trajectories leads to significant performance gains and more robust\nalgorithms.\n","authors":["Andreas Floros","Seyed-Mohsen Moosavi-Dezfooli","Pier Luigi Dragotti"],"pdf_url":"https://arxiv.org/pdf/2411.07449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15171v1","updated":"2024-12-19T18:46:55Z","published":"2024-12-19T18:46:55Z","title":"SqueezeMe: Efficient Gaussian Avatars for VR","summary":"  Gaussian Splatting has enabled real-time 3D human avatars with unprecedented\nlevels of visual quality. While previous methods require a desktop GPU for\nreal-time inference of a single avatar, we aim to squeeze multiple Gaussian\navatars onto a portable virtual reality headset with real-time drivable\ninference. We begin by training a previous work, Animatable Gaussians, on a\nhigh quality dataset captured with 512 cameras. The Gaussians are animated by\ncontrolling base set of Gaussians with linear blend skinning (LBS) motion and\nthen further adjusting the Gaussians with a neural network decoder to correct\ntheir appearance. When deploying the model on a Meta Quest 3 VR headset, we\nfind two major computational bottlenecks: the decoder and the rendering. To\naccelerate the decoder, we train the Gaussians in UV-space instead of\npixel-space, and we distill the decoder to a single neural network layer.\nFurther, we discover that neighborhoods of Gaussians can share a single\ncorrective from the decoder, which provides an additional speedup. To\naccelerate the rendering, we develop a custom pipeline in Vulkan that runs on\nthe mobile GPU. Putting it all together, we run 3 Gaussian avatars concurrently\nat 72 FPS on a VR headset. Demo videos are at\nhttps://forresti.github.io/squeezeme.\n","authors":["Shunsuke Saito","Stanislav Pidhorskyi","Igor Santesteban","Forrest Iandola","Divam Gupta","Anuj Pahuja","Nemanja Bartolovic","Frank Yu","Emanuel Garbin","Tomas Simon"],"pdf_url":"https://arxiv.org/pdf/2412.15171v1.pdf","comment":"Initial version"},{"id":"http://arxiv.org/abs/2412.15159v1","updated":"2024-12-19T18:34:50Z","published":"2024-12-19T18:34:50Z","title":"OnlineVPO: Align Video Diffusion Model with Online Video-Centric\n  Preference Optimization","summary":"  In recent years, the field of text-to-video (T2V) generation has made\nsignificant strides. Despite this progress, there is still a gap between\ntheoretical advancements and practical application, amplified by issues like\ndegraded image quality and flickering artifacts. Recent advancements in\nenhancing the video diffusion model (VDM) through feedback learning have shown\npromising results. However, these methods still exhibit notable limitations,\nsuch as misaligned feedback and inferior scalability. To tackle these issues,\nwe introduce OnlineVPO, a more efficient preference learning approach tailored\nspecifically for video diffusion models. Our method features two novel designs,\nfirstly, instead of directly using image-based reward feedback, we leverage the\nvideo quality assessment (VQA) model trained on synthetic data as the reward\nmodel to provide distribution and modality-aligned feedback on the video\ndiffusion model. Additionally, we introduce an online DPO algorithm to address\nthe off-policy optimization and scalability issue in existing video preference\nlearning frameworks. By employing the video reward model to offer concise video\nfeedback on the fly, OnlineVPO offers effective and efficient preference\nguidance. Extensive experiments on the open-source video-diffusion model\ndemonstrate OnlineVPO as a simple yet effective and more importantly scalable\npreference learning algorithm for video diffusion models, offering valuable\ninsights for future advancements in this domain.\n","authors":["Jiacheng Zhang","Jie Wu","Weifeng Chen","Yatai Ji","Xuefeng Xiao","Weilin Huang","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2412.15159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15156v1","updated":"2024-12-19T18:32:21Z","published":"2024-12-19T18:32:21Z","title":"Prompt-A-Video: Prompt Your Video Diffusion Model via Preference-Aligned\n  LLM","summary":"  Text-to-video models have made remarkable advancements through optimization\non high-quality text-video pairs, where the textual prompts play a pivotal role\nin determining quality of output videos. However, achieving the desired output\noften entails multiple revisions and iterative inference to refine\nuser-provided prompts. Current automatic methods for refining prompts encounter\nchallenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware\nwhen applied to text-to-video diffusion models. To address these problem, we\nintroduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,\nwhich excels in crafting Video-Centric, Labor-Free and Preference-Aligned\nprompts tailored to specific video diffusion model. Our approach involves a\nmeticulously crafted two-stage optimization and alignment system. Initially, we\nconduct a reward-guided prompt evolution pipeline to automatically create\noptimal prompts pool and leverage them for supervised fine-tuning (SFT) of the\nLLM. Then multi-dimensional rewards are employed to generate pairwise data for\nthe SFT model, followed by the direct preference optimization (DPO) algorithm\nto further facilitate preference alignment. Through extensive experimentation\nand comparative analyses, we validate the effectiveness of Prompt-A-Video\nacross diverse generation models, highlighting its potential to push the\nboundaries of video generation.\n","authors":["Yatai Ji","Jiacheng Zhang","Jie Wu","Shilong Zhang","Shoufa Chen","Chongjian GE","Peize Sun","Weifeng Chen","Wenqi Shao","Xuefeng Xiao","Weilin Huang","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2412.15156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15150v1","updated":"2024-12-19T18:28:37Z","published":"2024-12-19T18:28:37Z","title":"Leveraging Color Channel Independence for Improved Unsupervised Object\n  Detection","summary":"  Object-centric architectures can learn to extract distinct object\nrepresentations from visual scenes, enabling downstream applications on the\nobject level. Similarly to autoencoder-based image models, object-centric\napproaches have been trained on the unsupervised reconstruction loss of images\nencoded by RGB color spaces. In our work, we challenge the common assumption\nthat RGB images are the optimal color space for unsupervised learning in\ncomputer vision. We discuss conceptually and empirically that other color\nspaces, such as HSV, bear essential characteristics for object-centric\nrepresentation learning, like robustness to lighting conditions. We further\nshow that models improve when requiring them to predict additional color\nchannels. Specifically, we propose to transform the predicted targets to the\nRGB-S space, which extends RGB with HSV's saturation component and leads to\nmarkedly better reconstruction and disentanglement for five common evaluation\ndatasets. The use of composite color spaces can be implemented with basically\nno computational overhead, is agnostic of the models' architecture, and is\nuniversally applicable across a wide range of visual computing tasks and\ntraining types. The findings of our approach encourage additional\ninvestigations in computer vision tasks beyond object-centric learning.\n","authors":["Bastian Jäckl","Yannick Metz","Udo Schlegel","Daniel A. Keim","Maximilian T. Fischer"],"pdf_url":"https://arxiv.org/pdf/2412.15150v1.pdf","comment":"38 pages incl. references, 16 figures"},{"id":"http://arxiv.org/abs/2412.15129v1","updated":"2024-12-19T18:09:42Z","published":"2024-12-19T18:09:42Z","title":"Jet: A Modern Transformer-Based Normalizing Flow","summary":"  In the past, normalizing generative flows have emerged as a promising class\nof generative models for natural images. This type of model has many modeling\nadvantages: the ability to efficiently compute log-likelihood of the input\ndata, fast generation and simple overall structure. Normalizing flows remained\na topic of active research but later fell out of favor, as visual quality of\nthe samples was not competitive with other model classes, such as GANs,\nVQ-VAE-based approaches or diffusion models. In this paper we revisit the\ndesign of the coupling-based normalizing flow models by carefully ablating\nprior design choices and using computational blocks based on the Vision\nTransformer architecture, not convolutional neural networks. As a result, we\nachieve state-of-the-art quantitative and qualitative performance with a much\nsimpler architecture. While the overall visual quality is still behind the\ncurrent state-of-the-art models, we argue that strong normalizing flow models\ncan help advancing research frontier by serving as building components of more\npowerful generative models.\n","authors":["Alexander Kolesnikov","André Susano Pinto","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2412.15129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15119v1","updated":"2024-12-19T17:59:54Z","published":"2024-12-19T17:59:54Z","title":"Parallelized Autoregressive Visual Generation","summary":"  Autoregressive models have emerged as a powerful approach for visual\ngeneration but suffer from slow inference speed due to their sequential\ntoken-by-token prediction process. In this paper, we propose a simple yet\neffective approach for parallelized autoregressive visual generation that\nimproves generation efficiency while preserving the advantages of\nautoregressive modeling. Our key insight is that parallel generation depends on\nvisual token dependencies-tokens with weak dependencies can be generated in\nparallel, while strongly dependent adjacent tokens are difficult to generate\ntogether, as their independent sampling may lead to inconsistencies. Based on\nthis observation, we develop a parallel generation strategy that generates\ndistant tokens with weak dependencies in parallel while maintaining sequential\ngeneration for strongly dependent local tokens. Our approach can be seamlessly\nintegrated into standard autoregressive models without modifying the\narchitecture or tokenizer. Experiments on ImageNet and UCF-101 demonstrate that\nour method achieves a 3.6x speedup with comparable quality and up to 9.5x\nspeedup with minimal quality degradation across both image and video generation\ntasks. We hope this work will inspire future research in efficient visual\ngeneration and unified autoregressive modeling. Project page:\nhttps://epiphqny.github.io/PAR-project.\n","authors":["Yuqing Wang","Shuhuai Ren","Zhijie Lin","Yujin Han","Haoyuan Guo","Zhenheng Yang","Difan Zou","Jiashi Feng","Xihui Liu"],"pdf_url":"https://arxiv.org/pdf/2412.15119v1.pdf","comment":"Project page: https://epiphqny.github.io/PAR-project"},{"id":"http://arxiv.org/abs/2412.11917v3","updated":"2024-12-19T17:57:59Z","published":"2024-12-16T16:01:18Z","title":"Does VLM Classification Benefit from LLM Description Semantics?","summary":"  Accurately describing images with text is a foundation of explainable AI.\nVision-Language Models (VLMs) like CLIP have recently addressed this by\naligning images and texts in a shared embedding space, expressing semantic\nsimilarities between vision and language embeddings. VLM classification can be\nimproved with descriptions generated by Large Language Models (LLMs). However,\nit is difficult to determine the contribution of actual description semantics,\nas the performance gain may also stem from a semantic-agnostic ensembling\neffect, where multiple modified text prompts act as a noisy test-time\naugmentation for the original one. We propose an alternative evaluation\nscenario to decide if a performance boost of LLM-generated descriptions is\ncaused by such a noise augmentation effect or rather by genuine description\nsemantics. The proposed scenario avoids noisy test-time augmentation and\nensures that genuine, distinctive descriptions cause the performance boost.\nFurthermore, we propose a training-free method for selecting discriminative\ndescriptions that work independently of classname-ensembling effects. Our\napproach identifies descriptions that effectively differentiate classes within\na local CLIP label neighborhood, improving classification accuracy across seven\ndatasets. Additionally, we provide insights into the explainability of\ndescription-based image classification with VLMs.\n","authors":["Pingchuan Ma","Lennart Rietdorf","Dmytro Kotovenko","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2412.11917v3.pdf","comment":"AAAI-25 (extended version), Code: https://github.com/CompVis/DisCLIP"},{"id":"http://arxiv.org/abs/2412.15106v1","updated":"2024-12-19T17:51:49Z","published":"2024-12-19T17:51:49Z","title":"Knowing Where to Focus: Attention-Guided Alignment for Text-based Person\n  Search","summary":"  In the realm of Text-Based Person Search (TBPS), mainstream methods aim to\nexplore more efficient interaction frameworks between text descriptions and\nvisual data. However, recent approaches encounter two principal challenges.\nFirstly, the widely used random-based Masked Language Modeling (MLM) considers\nall the words in the text equally during training. However, massive\nsemantically vacuous words ('with', 'the', etc.) be masked fail to contribute\nefficient interaction in the cross-modal MLM and hampers the representation\nalignment. Secondly, manual descriptions in TBPS datasets are tedious and\ninevitably contain several inaccuracies. To address these issues, we introduce\nan Attention-Guided Alignment (AGA) framework featuring two innovative\ncomponents: Attention-Guided Mask (AGM) Modeling and Text Enrichment Module\n(TEM). AGM dynamically masks semantically meaningful words by aggregating the\nattention weight derived from the text encoding process, thereby cross-modal\nMLM can capture information related to the masked word from text context and\nimages and align their representations. Meanwhile, TEM alleviates low-quality\nrepresentations caused by repetitive and erroneous text descriptions by\nreplacing those semantically meaningful words with MLM's prediction. It not\nonly enriches text descriptions but also prevents overfitting. Extensive\nexperiments across three challenging benchmarks demonstrate the effectiveness\nof our AGA, achieving new state-of-the-art results with Rank-1 accuracy\nreaching 78.36%, 67.31%, and 67.4% on CUHK-PEDES, ICFG-PEDES, and RSTPReid,\nrespectively.\n","authors":["Lei Tan","Weihao Li","Pingyang Dai","Jie Chen","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2412.15106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13788v2","updated":"2024-12-19T17:51:42Z","published":"2024-03-20T17:51:53Z","title":"DepthFM: Fast Monocular Depth Estimation with Flow Matching","summary":"  Current discriminative depth estimation methods often produce blurry\nartifacts, while generative approaches suffer from slow sampling due to\ncurvatures in the noise-to-depth transport. Our method addresses these\nchallenges by framing depth estimation as a direct transport between image and\ndepth distributions. We are the first to explore flow matching in this field,\nand we demonstrate that its interpolation trajectories enhance both training\nand sampling efficiency while preserving high performance. While generative\nmodels typically require extensive training data, we mitigate this dependency\nby integrating external knowledge from a pre-trained image diffusion model,\nenabling effective transfer even across differing objectives. To further boost\nour model performance, we employ synthetic data and utilize image-depth pairs\ngenerated by a discriminative model on an in-the-wild image dataset. As a\ngenerative model, our model can reliably estimate depth confidence, which\nprovides an additional advantage. Our approach achieves competitive zero-shot\nperformance on standard benchmarks of complex natural scenes while improving\nsampling efficiency and only requiring minimal synthetic data for training.\n","authors":["Ming Gui","Johannes Schusterbauer","Ulrich Prestel","Pingchuan Ma","Dmytro Kotovenko","Olga Grebenkova","Stefan Andreas Baumann","Vincent Tao Hu","Björn Ommer"],"pdf_url":"https://arxiv.org/pdf/2403.13788v2.pdf","comment":"AAAI 2025, Project Page: https://github.com/CompVis/depth-fm"},{"id":"http://arxiv.org/abs/2412.15095v1","updated":"2024-12-19T17:45:08Z","published":"2024-12-19T17:45:08Z","title":"A Full Transformer-based Framework for Automatic Pain Estimation using\n  Videos","summary":"  The automatic estimation of pain is essential in designing an optimal pain\nmanagement system offering reliable assessment and reducing the suffering of\npatients. In this study, we present a novel full transformer-based framework\nconsisting of a Transformer in Transformer (TNT) model and a Transformer\nleveraging cross-attention and self-attention blocks. Elaborating on videos\nfrom the BioVid database, we demonstrate state-of-the-art performances, showing\nthe efficacy, efficiency, and generalization capability across all the primary\npain estimation tasks.\n","authors":["Stefanos Gkikas","Manolis Tsiknakis"],"pdf_url":"https://arxiv.org/pdf/2412.15095v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15077v1","updated":"2024-12-19T17:26:07Z","published":"2024-12-19T17:26:07Z","title":"Till the Layers Collapse: Compressing a Deep Neural Network through the\n  Lenses of Batch Normalization Layers","summary":"  Today, deep neural networks are widely used since they can handle a variety\nof complex tasks. Their generality makes them very powerful tools in modern\ntechnology. However, deep neural networks are often overparameterized. The\nusage of these large models consumes a lot of computation resources. In this\npaper, we introduce a method called \\textbf{T}ill the \\textbf{L}ayers\n\\textbf{C}ollapse (TLC), which compresses deep neural networks through the\nlenses of batch normalization layers. By reducing the depth of these networks,\nour method decreases deep neural networks' computational requirements and\noverall latency. We validate our method on popular models such as Swin-T,\nMobileNet-V2, and RoBERTa, across both image classification and natural\nlanguage processing (NLP) tasks.\n","authors":["Zhu Liao","Nour Hezbri","Victor Quétu","Van-Tam Nguyen","Enzo Tartaglione"],"pdf_url":"https://arxiv.org/pdf/2412.15077v1.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2412.15058v1","updated":"2024-12-19T17:06:53Z","published":"2024-12-19T17:06:53Z","title":"MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging\n  Datasets with In-Context Guidance","summary":"  Medical researchers and clinicians often need to perform novel segmentation\ntasks on a set of related images. Existing methods for segmenting a new dataset\nare either interactive, requiring substantial human effort for each image, or\nrequire an existing set of manually labeled images. We introduce a system,\nMultiverSeg, that enables practitioners to rapidly segment an entire new\ndataset without requiring access to any existing labeled data from that task or\ndomain. Along with the image to segment, the model takes user interactions such\nas clicks, bounding boxes or scribbles as input, and predicts a segmentation.\nAs the user segments more images, those images and segmentations become\nadditional inputs to the model, providing context. As the context set of\nlabeled images grows, the number of interactions required to segment each new\nimage decreases. We demonstrate that MultiverSeg enables users to interactively\nsegment new datasets efficiently, by amortizing the number of interactions per\nimage to achieve an accurate segmentation. Compared to using a state-of-the-art\ninteractive segmentation method, using MultiverSeg reduced the total number of\nscribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images\nfrom unseen tasks. We release code and model weights at\nhttps://multiverseg.csail.mit.edu\n","authors":["Hallee E. Wong","Jose Javier Gonzalez Ortiz","John Guttag","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2412.15058v1.pdf","comment":"Project Website: https://multiverseg.csail.mit.edu Keywords:\n  interactive segmentation, in-context learning, medical image analysis,\n  biomedical imaging, image annotation, visual prompting"},{"id":"http://arxiv.org/abs/2412.15054v1","updated":"2024-12-19T17:02:03Z","published":"2024-12-19T17:02:03Z","title":"GIRAFE: Glottal Imaging Dataset for Advanced Segmentation, Analysis, and\n  Facilitative Playbacks Evaluation","summary":"  The advances in the development of Facilitative Playbacks extracted from\nHigh-Speed videoendoscopic sequences of the vocal folds are hindered by a\nnotable lack of publicly available datasets annotated with the semantic\nsegmentations corresponding to the area of the glottal gap. This fact also\nlimits the reproducibility and further exploration of existing research in this\nfield.\n  To address this gap, GIRAFE is a data repository designed to facilitate the\ndevelopment of advanced techniques for the semantic segmentation, analysis, and\nfast evaluation of High-Speed videoendoscopic sequences of the vocal folds. The\nrepository includes 65 high-speed videoendoscopic recordings from a cohort of\n50 patients (30 female, 20 male). The dataset comprises 15 recordings from\nhealthy controls, 26 from patients with diagnosed voice disorders, and 24 with\nan unknown health condition. All of them were manually annotated by an expert,\nincluding the masks corresponding to the semantic segmentation of the glottal\ngap. The repository is also complemented with the automatic segmentation of the\nglottal area using different state-of-the-art approaches.\n  This data set has already supported several studies, which demonstrates its\nusefulness for the development of new glottal gap segmentation algorithms from\nHigh-Speed-Videoendoscopic sequences to improve or create new Facilitative\nPlaybacks. Despite these advances and others in the field, the broader\nchallenge of performing an accurate and completely automatic semantic\nsegmentation method of the glottal area remains open.\n","authors":["G. Andrade-Miranda","K. Chatzipapas","J. D. Arias-Londoño","J. I. Godino-Llorente"],"pdf_url":"https://arxiv.org/pdf/2412.15054v1.pdf","comment":"18 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.15050v1","updated":"2024-12-19T16:57:45Z","published":"2024-12-19T16:57:45Z","title":"Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream\n  Diffusion","summary":"  Rendering and inverse rendering are pivotal tasks in both computer vision and\ngraphics. The rendering equation is the core of the two tasks, as an ideal\nconditional distribution transfer function from intrinsic properties to RGB\nimages. Despite achieving promising results of existing rendering methods, they\nmerely approximate the ideal estimation for a specific scene and come with a\nhigh computational cost. Additionally, the inverse conditional distribution\ntransfer is intractable due to the inherent ambiguity. To address these\nchallenges, we propose a data-driven method that jointly models rendering and\ninverse rendering as two conditional generation tasks within a single diffusion\nframework. Inspired by UniDiffuser, we utilize two distinct time schedules to\nmodel both tasks, and with a tailored dual streaming module, we achieve\ncross-conditioning of two pre-trained diffusion models. This unified approach,\nnamed Uni-Renderer, allows the two processes to facilitate each other through a\ncycle-consistent constrain, mitigating ambiguity by enforcing consistency\nbetween intrinsic properties and rendered images. Combined with a meticulously\nprepared dataset, our method effectively decomposition of intrinsic properties\nand demonstrates a strong capability to recognize changes during rendering. We\nwill open-source our training and inference code to the public, fostering\nfurther research and development in this area.\n","authors":["Zhifei Chen","Tianshuo Xu","Wenhang Ge","Leyi Wu","Dongyu Yan","Jing He","Luozhou Wang","Lu Zeng","Shunsi Zhang","Yingcong Chen"],"pdf_url":"https://arxiv.org/pdf/2412.15050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.03767v2","updated":"2024-12-19T16:45:52Z","published":"2023-01-10T03:10:32Z","title":"Metric Compatible Training for Online Backfilling in Large-Scale\n  Retrieval","summary":"  Backfilling is the process of re-extracting all gallery embeddings from\nupgraded models in image retrieval systems. It inevitably requires a\nprohibitively large amount of computational cost and even entails the downtime\nof the service. Although backward-compatible learning sidesteps this challenge\nby tackling query-side representations, this leads to suboptimal solutions in\nprinciple because gallery embeddings cannot benefit from model upgrades. We\naddress this dilemma by introducing an online backfilling algorithm, which\nenables us to achieve a progressive performance improvement during the\nbackfilling process while not sacrificing the final performance of new model\nafter the completion of backfilling. To this end, we first propose a simple\ndistance rank merge technique for online backfilling. Then, we incorporate a\nreverse transformation module for more effective and efficient merging, which\nis further enhanced by adopting a metric-compatible contrastive learning\napproach. These two components help to make the distances of old and new models\ncompatible, resulting in desirable merge results during backfilling with no\nextra computational overhead. Extensive experiments show the effectiveness of\nour framework on four standard benchmarks in various settings.\n","authors":["Seonguk Seo","Mustafa Gokhan Uzunbas","Bohyung Han","Sara Cao","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2301.03767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15032v1","updated":"2024-12-19T16:44:01Z","published":"2024-12-19T16:44:01Z","title":"DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT\n  Space","summary":"  This paper explores image modeling from the frequency space and introduces\nDCTdiff, an end-to-end diffusion generative paradigm that efficiently models\nimages in the discrete cosine transform (DCT) space. We investigate the design\nspace of DCTdiff and reveal the key design factors. Experiments on different\nframeworks (UViT, DiT), generation tasks, and various diffusion samplers\ndemonstrate that DCTdiff outperforms pixel-based diffusion models regarding\ngenerative quality and training efficiency. Remarkably, DCTdiff can seamlessly\nscale up to high-resolution generation without using the latent diffusion\nparadigm. Finally, we illustrate several intriguing properties of DCT image\nmodeling. For example, we provide a theoretical proof of why `image diffusion\ncan be seen as spectral autoregression', bridging the gap between diffusion and\nautoregressive models. The effectiveness of DCTdiff and the introduced\nproperties suggest a promising direction for image modeling in the frequency\nspace. The code is at \\url{https://github.com/forever208/DCTdiff}.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Haozhe Jia","Lanmiao Liu","Martin Beneš","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2412.15032v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2302.10634v2","updated":"2024-12-19T16:41:57Z","published":"2023-02-21T12:48:44Z","title":"A Deep Learning-Based and Fully Automated Pipeline for Regurgitant\n  Mitral Valve Anatomy Analysis from 3D Echocardiography","summary":"  3D transesophageal echocardiography (3DTEE), is the recommended method for\ndiagnosing mitral regurgitation (MR). 3DTEE provides a high-quality 3D image of\nthe mitral valve (MV), allowing for precise segmentation and measurement of the\nregurgitant valve anatomy. However, manual TEE segmentations are time-consuming\nand prone to intra-operator variability, affecting the reliability of the\nmeasurements. To address this, we developed a fully automated pipeline using a\n3D convolutional neural network (CNN) to segment MV substructures (annulus,\nanterior leaflet, and posterior leaflet) and quantify MV anatomy. The 3D CNN,\nbased on a multi-decoder residual U-Net architecture, was trained and tested on\na dataset comprising 100 3DTEE images with corresponding segmentations. Within\nthe pipeline, a custom algorithm refines the CNN-based segmentations and\nextracts MV models, from which anatomical landmarks and features are\nquantified. The accuracy of the proposed method was assessed using Dice score\nand mean surface distance (MSD) against ground truth segmentations, and the\nextracted anatomical parameters were compared against a semiautomated\ncommercial software TomTec Image Arena. The trained 3D CNN achieved an average\nDice score of 0.79 and MSD of 0.47 mm for the combined segmentation of the\nannulus, anterior and posterior leaflet. The proposed CNN architecture\noutperformed a baseline residual U-Net architecture in MV substructure\nsegmentation, and the refinement of the predicted annulus segmentation improved\nMSD by 8.36%. The annular and leaflet linear measurements differed by less than\n7.94 mm and 3.67 mm, respectively, compared to the 3D measurements obtained\nwith TomTec Image Arena. The proposed pipeline was faster than the commercial\nsoftware, with a modeling time of 12.54 s and a quantification time of 54.42 s.\n","authors":["Riccardo Munafò","Simone Saitta","Giacomo Ingallina","Paolo Denti","Francesco Maisano","Eustachio Agricola","Alberto Redaelli","Emiliano Votta"],"pdf_url":"https://arxiv.org/pdf/2302.10634v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15023v1","updated":"2024-12-19T16:37:19Z","published":"2024-12-19T16:37:19Z","title":"Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and\n  Semantic Controls","summary":"  Sound designers and Foley artists usually sonorize a scene, such as from a\nmovie or video game, by manually annotating and sonorizing each action of\ninterest in the video. In our case, the intent is to leave full creative\ncontrol to sound designers with a tool that allows them to bypass the more\nrepetitive parts of their work, thus being able to focus on the creative\naspects of sound production. We achieve this presenting Stable-V2A, a two-stage\nmodel consisting of: an RMS-Mapper that estimates an envelope representative of\nthe audio characteristics associated with the input video; and Stable-Foley, a\ndiffusion model based on Stable Audio Open that generates audio semantically\nand temporally aligned with the target video. Temporal alignment is guaranteed\nby the use of the envelope as a ControlNet input, while semantic alignment is\nachieved through the use of sound representations chosen by the designer as\ncross-attention conditioning of the diffusion process. We train and test our\nmodel on Greatest Hits, a dataset commonly used to evaluate V2A models. In\naddition, to test our model on a case study of interest, we introduce Walking\nThe Maps, a dataset of videos extracted from video games depicting animated\ncharacters walking in different locations. Samples and code available on our\ndemo page at https://ispamm.github.io/Stable-V2A.\n","authors":["Riccardo Fosco Gramaccioni","Christian Marinoni","Emilian Postolache","Marco Comunità","Luca Cosmo","Joshua D. Reiss","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2412.15023v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.15010v1","updated":"2024-12-19T16:22:37Z","published":"2024-12-19T16:22:37Z","title":"Robust Federated Learning in the Face of Covariate Shift: A Magnitude\n  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation","summary":"  The development of highly sophisticated neural networks has allowed for fast\nprogress in every field of computer vision, however, applications where\nannotated data is prohibited due to privacy or security concerns remain\nchallenging. Federated Learning (FL) offers a promising framework for\nindividuals aiming to collaboratively develop a shared model while preserving\ndata privacy. Nevertheless, our findings reveal that variations in data\ndistribution among clients can profoundly affect FL methodologies, primarily\ndue to instabilities in the aggregation process. We also propose a novel FL\nframework to mitigate the adverse effects of covariate shifts among federated\nclients by combining individual parameter pruning and regularization techniques\nto improve the robustness of individual clients' models to aggregate. Each\nclient's model is optimized through magnitude-based pruning and the addition of\ndropout and noise injection layers to build more resilient decision pathways in\nthe networks and improve the robustness of the model's parameter aggregation\nstep. The proposed framework is capable of extracting robust representations\neven in the presence of very large covariate shifts among client data\ndistributions and in the federation of a small number of clients. Empirical\nfindings substantiate the effectiveness of our proposed methodology across\ncommon benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.\nFurthermore, we introduce the CelebA-Gender dataset, specifically designed to\nevaluate performance on a more realistic domain. The proposed method is capable\nof extracting robust representations even in the presence of both high and low\ncovariate shifts among client data distributions.\n","authors":["Ozgu Goksu","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2412.15010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14988v1","updated":"2024-12-19T16:00:10Z","published":"2024-12-19T16:00:10Z","title":"Stitch Contrast and Segment_Learning a Human Action Segmentation Model\n  Using Trimmed Skeleton Videos","summary":"  Existing skeleton-based human action classification models rely on\nwell-trimmed action-specific skeleton videos for both training and testing,\nprecluding their scalability to real-world applications where untrimmed videos\nexhibiting concatenated actions are predominant. To overcome this limitation,\nrecently introduced skeleton action segmentation models involve un-trimmed\nskeleton videos into end-to-end training. The model is optimized to provide\nframe-wise predictions for any length of testing videos, simultaneously\nrealizing action localization and classification. Yet, achieving such an\nimprovement im-poses frame-wise annotated skeleton videos, which remains\ntime-consuming in practice. This paper features a novel framework for\nskeleton-based action segmentation trained on short trimmed skeleton videos,\nbut that can run on longer un-trimmed videos. The approach is implemented in\nthree steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral\nskeleton stitching scheme that treats trimmed skeleton videos as elementary\nhuman motions that compose a semantic space and can be sampled to generate\nmulti-action stitched se-quences. Contrast learns contrastive representations\nfrom stitched sequences with a novel discrimination pretext task that enables a\nskeleton encoder to learn meaningful action-temporal contexts to improve action\nsegmentation. Finally, Segment relates the proposed method to action\nsegmentation by learning a segmentation layer while handling particular da-ta\navailability. Experiments involve a trimmed source dataset and an untrimmed\ntarget dataset in an adaptation formulation for real-world skeleton-based human\naction segmentation to evaluate the effectiveness of the proposed method.\n","authors":["Haitao Tian","Pierre Payeur"],"pdf_url":"https://arxiv.org/pdf/2412.14988v1.pdf","comment":"Accepted as AAAI 2025"},{"id":"http://arxiv.org/abs/2412.08941v3","updated":"2024-12-19T15:59:19Z","published":"2024-12-12T05:08:05Z","title":"Optimized Gradient Clipping for Noisy Label Learning","summary":"  Previous research has shown that constraining the gradient of loss function\nwith respect to model-predicted probabilities can enhance the model robustness\nagainst noisy labels. These methods typically specify a fixed optimal threshold\nfor gradient clipping through validation data to obtain the desired robustness\nagainst noise. However, this common practice overlooks the dynamic distribution\nof gradients from both clean and noisy-labeled samples at different stages of\ntraining, significantly limiting the model capability to adapt to the variable\nnature of gradients throughout the training process. To address this issue, we\npropose a simple yet effective approach called Optimized Gradient Clipping\n(OGC), which dynamically adjusts the clipping threshold based on the ratio of\nnoise gradients to clean gradients after clipping, estimated by modeling the\ndistributions of clean and noisy samples. This approach allows us to modify the\nclipping threshold at each training step, effectively controlling the influence\nof noise gradients. Additionally, we provide statistical analysis to certify\nthe noise-tolerance ability of OGC. Our extensive experiments across various\ntypes of label noise, including symmetric, asymmetric, instance-dependent, and\nreal-world noise, demonstrate the effectiveness of our approach.\n","authors":["Xichen Ye","Yifan Wu","Weizhong Zhang","Xiaoqiang Li","Yifan Chen","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2412.08941v3.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14974v1","updated":"2024-12-19T15:48:51Z","published":"2024-12-19T15:48:51Z","title":"Arti-PG: A Toolbox for Procedurally Synthesizing Large-Scale and Diverse\n  Articulated Objects with Rich Annotations","summary":"  The acquisition of substantial volumes of 3D articulated object data is\nexpensive and time-consuming, and consequently the scarcity of 3D articulated\nobject data becomes an obstacle for deep learning methods to achieve remarkable\nperformance in various articulated object understanding tasks. Meanwhile,\npairing these object data with detailed annotations to enable training for\nvarious tasks is also difficult and labor-intensive to achieve. In order to\nexpeditiously gather a significant number of 3D articulated objects with\ncomprehensive and detailed annotations for training, we propose Articulated\nObject Procedural Generation toolbox, a.k.a. Arti-PG toolbox. Arti-PG toolbox\nconsists of i) descriptions of articulated objects by means of a generalized\nstructure program along with their analytic correspondence to the objects'\npoint cloud, ii) procedural rules about manipulations on the structure program\nto synthesize large-scale and diverse new articulated objects, and iii)\nmathematical descriptions of knowledge (e.g. affordance, semantics, etc.) to\nprovide annotations to the synthesized object. Arti-PG has two appealing\nproperties for providing training data for articulated object understanding\ntasks: i) objects are created with unlimited variations in shape through\nprogram-oriented structure manipulation, ii) Arti-PG is widely applicable to\ndiverse tasks by easily providing comprehensive and detailed annotations.\nArti-PG now supports the procedural generation of 26 categories of articulate\nobjects and provides annotations across a wide range of both vision and\nmanipulation tasks, and we provide exhaustive experiments which fully\ndemonstrate its advantages. We will make Arti-PG toolbox publicly available for\nthe community to use.\n","authors":["Jianhua Sun","Yuxuan Li","Jiude Wei","Longfei Xu","Nange Wang","Yining Zhang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2412.14974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14969v1","updated":"2024-12-19T15:47:31Z","published":"2024-12-19T15:47:31Z","title":"PhotoHolmes: a Python library for forgery detection in digital images","summary":"  In this paper, we introduce PhotoHolmes, an open-source Python library\ndesigned to easily run and benchmark forgery detection methods on digital\nimages. The library includes implementations of popular and state-of-the-art\nmethods, dataset integration tools, and evaluation metrics. Utilizing the\nBenchmark tool in PhotoHolmes, users can effortlessly compare various methods.\nThis facilitates an accurate and reproducible comparison between their own\nmethods and those in the existing literature. Furthermore, PhotoHolmes includes\na command-line interface (CLI) to easily run the methods implemented in the\nlibrary on any suspicious image. As such, image forgery methods become more\naccessible to the community. The library has been built with extensibility and\nmodularity in mind, which makes adding new methods, datasets and metrics to the\nlibrary a straightforward process. The source code is available at\nhttps://github.com/photoholmes/photoholmes.\n","authors":["Julián O'Flaherty","Rodrigo Paganini","Juan Pablo Sotelo","Julieta Umpiérrez","Marina Gardella","Matías Tailanian","Pablo Musé"],"pdf_url":"https://arxiv.org/pdf/2412.14969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14965v1","updated":"2024-12-19T15:44:04Z","published":"2024-12-19T15:44:04Z","title":"Movie2Story: A framework for understanding videos and telling stories in\n  the form of novel text","summary":"  Multimodal video-to-text models have made considerable progress, primarily in\ngenerating brief descriptions of video content. However, there is still a\ndeficiency in generating rich long-form text descriptions that integrate both\nvideo and audio. In this paper, we introduce a framework called M2S, designed\nto generate novel-length text by combining audio, video, and character\nrecognition. M2S includes modules for video long-form text description and\ncomprehension, audio-based analysis of emotion, speech rate, and character\nalignment, and visual-based character recognition alignment. By integrating\nmultimodal information using the large language model GPT4o, M2S stands out in\nthe field of multimodal text generation. We demonstrate the effectiveness and\naccuracy of M2S through comparative experiments and human evaluation.\nAdditionally, the model framework has good scalability and significant\npotential for future research.\n","authors":["Kangning Li","Zheyang Jia","Anyu Ying"],"pdf_url":"https://arxiv.org/pdf/2412.14965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14963v1","updated":"2024-12-19T15:43:05Z","published":"2024-12-19T15:43:05Z","title":"IDOL: Instant Photorealistic 3D Human Creation from a Single Image","summary":"  Creating a high-fidelity, animatable 3D full-body avatar from a single image\nis a challenging task due to the diverse appearance and poses of humans and the\nlimited availability of high-quality training data. To achieve fast and\nhigh-quality human reconstruction, this work rethinks the task from the\nperspectives of dataset, model, and representation. First, we introduce a\nlarge-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K\ndiverse, photorealistic sets of human images. Each set contains 24-view frames\nin specific human poses, generated using a pose-controllable\nimage-to-multi-view model. Next, leveraging the diversity in views, poses, and\nappearances within HuGe100K, we develop a scalable feed-forward transformer\nmodel to predict a 3D human Gaussian representation in a uniform space from a\ngiven human image. This model is trained to disentangle human pose, body shape,\nclothing geometry, and texture. The estimated Gaussians can be animated without\npost-processing. We conduct comprehensive experiments to validate the\neffectiveness of the proposed dataset and method. Our model demonstrates the\nability to efficiently reconstruct photorealistic humans at 1K resolution from\na single input image using a single GPU instantly. Additionally, it seamlessly\nsupports various applications, as well as shape and texture editing tasks.\n","authors":["Yiyu Zhuang","Jiaxi Lv","Hao Wen","Qing Shuai","Ailing Zeng","Hao Zhu","Shifeng Chen","Yujiu Yang","Xun Cao","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14963v1.pdf","comment":"21 pages, 15 figures, includes main content, supplementary materials,\n  and references"},{"id":"http://arxiv.org/abs/2412.14961v1","updated":"2024-12-19T15:42:21Z","published":"2024-12-19T15:42:21Z","title":"TDCNet: Transparent Objects Depth Completion with CNN-Transformer\n  Dual-Branch Parallel Network","summary":"  The sensing and manipulation of transparent objects present a critical\nchallenge in industrial and laboratory robotics. Conventional sensors face\nchallenges in obtaining the full depth of transparent objects due to the\nrefraction and reflection of light on their surfaces and their lack of visible\ntexture. Previous research has attempted to obtain complete depth maps of\ntransparent objects from RGB and damaged depth maps (collected by depth sensor)\nusing deep learning models. However, existing methods fail to fully utilize the\noriginal depth map, resulting in limited accuracy for deep completion. To solve\nthis problem, we propose TDCNet, a novel dual-branch CNN-Transformer parallel\nnetwork for transparent object depth completion. The proposed framework\nconsists of two different branches: one extracts features from partial depth\nmaps, while the other processes RGB-D images. Experimental results demonstrate\nthat our model achieves state-of-the-art performance across multiple public\ndatasets. Our code and the pre-trained model are publicly available at\nhttps://github.com/XianghuiFan/TDCNet.\n","authors":["Xianghui Fan","Chao Ye","Anping Deng","Xiaotian Wu","Mengyang Pan","Hang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14957v1","updated":"2024-12-19T15:38:15Z","published":"2024-12-19T15:38:15Z","title":"Dream to Manipulate: Compositional World Models Empowering Robot\n  Imitation Learning with Imagination","summary":"  A world model provides an agent with a representation of its environment,\nenabling it to predict the causal consequences of its actions. Current world\nmodels typically cannot directly and explicitly imitate the actual environment\nin front of a robot, often resulting in unrealistic behaviors and\nhallucinations that make them unsuitable for real-world applications. In this\npaper, we introduce a new paradigm for constructing world models that are\nexplicit representations of the real world and its dynamics. By integrating\ncutting-edge advances in real-time photorealism with Gaussian Splatting and\nphysics simulators, we propose the first compositional manipulation world\nmodel, which we call DreMa. DreMa replicates the observed world and its\ndynamics, allowing it to imagine novel configurations of objects and predict\nthe future consequences of robot actions. We leverage this capability to\ngenerate new data for imitation learning by applying equivariant\ntransformations to a small set of demonstrations. Our evaluations across\nvarious settings demonstrate significant improvements in both accuracy and\nrobustness by incrementing actions and object distributions, reducing the data\nneeded to learn a policy and improving the generalization of the agents. As a\nhighlight, we show that a real Franka Emika Panda robot, powered by DreMa's\nimagination, can successfully learn novel physical tasks from just a single\nexample per task variation (one-shot policy learning). Our project page and\nsource code can be found in https://leobarcellona.github.io/DreamToManipulate/\n","authors":["Leonardo Barcellona","Andrii Zadaianchuk","Davide Allegro","Samuele Papa","Stefano Ghidoni","Efstratios Gavves"],"pdf_url":"https://arxiv.org/pdf/2412.14957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13647v2","updated":"2024-12-19T15:37:55Z","published":"2024-12-18T09:23:12Z","title":"G-VEval: A Versatile Metric for Evaluating Image and Video Captions\n  Using GPT-4o","summary":"  Evaluation metric of visual captioning is important yet not thoroughly\nexplored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss\nsemantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are\nlimited in zero-shot scenarios. Advanced Language Model-based metrics also\nstruggle with aligning to nuanced human preferences. To address these issues,\nwe introduce G-VEval, a novel metric inspired by G-Eval and powered by the new\nGPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and\nsupports three modes: reference-free, reference-only, and combined,\naccommodating both video and image inputs. We also propose MSVD-Eval, a new\ndataset for video captioning evaluation, to establish a more transparent and\nconsistent framework for both human experts and evaluation metrics. It is\ndesigned to address the lack of clear criteria in existing datasets by\nintroducing distinct dimensions of Accuracy, Completeness, Conciseness, and\nRelevance (ACCR). Extensive results show that G-VEval outperforms existing\nmethods in correlation with human annotations, as measured by Kendall tau-b and\nKendall tau-c. This provides a flexible solution for diverse captioning tasks\nand suggests a straightforward yet effective approach for large language models\nto understand video content, paving the way for advancements in automated\ncaptioning. Codes are available at https://github.com/ztangaj/gveval\n","authors":["Tony Cheng Tong","Sirui He","Zhiwen Shao","Dit-Yan Yeung"],"pdf_url":"https://arxiv.org/pdf/2412.13647v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14954v1","updated":"2024-12-19T15:36:30Z","published":"2024-12-19T15:36:30Z","title":"Corn Ear Detection and Orientation Estimation Using Deep Learning","summary":"  Monitoring growth behavior of maize plants such as the development of ears\ncan give key insights into the plant's health and development. Traditionally,\nthe measurement of the angle of ears is performed manually, which can be\ntime-consuming and prone to human error. To address these challenges, this\npaper presents a computer vision-based system for detecting and tracking ears\nof corn in an image sequence. The proposed system could accurately detect,\ntrack, and predict the ear's orientation, which can be useful in monitoring\ntheir growth behavior. This can significantly save time compared to manual\nmeasurement and enables additional areas of ear orientation research and\npotential increase in efficiencies for maize production. Using an object\ndetector with keypoint detection, the algorithm proposed could detect 90\npercent of all ears. The cardinal estimation had a mean absolute error (MAE) of\n18 degrees, compared to a mean 15 degree difference between two people\nmeasuring by hand. These results demonstrate the feasibility of using computer\nvision techniques for monitoring maize growth and can lead to further research\nin this area.\n","authors":["Nathan Sprague","John Evans","Michael Mardikes"],"pdf_url":"https://arxiv.org/pdf/2412.14954v1.pdf","comment":"22 pages;15 figures"},{"id":"http://arxiv.org/abs/2406.16710v2","updated":"2024-12-19T15:28:26Z","published":"2024-06-24T15:11:35Z","title":"ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait\n  Image","summary":"  While recent works have achieved great success on image-to-3D object\ngeneration, high quality and fidelity 3D head generation from a single image\nremains a great challenge. Previous text-based methods for generating 3D heads\nwere limited by text descriptions and image-based methods struggled to produce\nhigh-quality head geometry. To handle this challenging problem, we propose a\nnovel framework, ID-Sculpt, to generate high-quality 3D heads while preserving\ntheir identities. Our work incorporates the identity information of the\nportrait image into three parts: 1) geometry initialization, 2) geometry\nsculpting, and 3) texture generation stages. Given a reference portrait image,\nwe first align the identity features with text features to realize ID-aware\nguidance enhancement, which contains the control signals representing the face\ninformation. We then use the canny map, ID features of the portrait image, and\na pre-trained text-to-normal/depth diffusion model to generate ID-aware\ngeometry supervision, and 3D-GAN inversion is employed to generate ID-aware\ngeometry initialization. Furthermore, with the ability to inject identity\ninformation into 3D head generation, we use ID-aware guidance to calculate\nID-aware Score Distillation (ISD) for geometry sculpting. For texture\ngeneration, we adopt the ID Consistent Texture Inpainting and Refinement which\nprogressively expands the view for texture inpainting to obtain an\ninitialization UV texture map. We then use the ID-aware guidance to provide\nimage-level supervision for noisy multi-view images to obtain a refined texture\nmap. Extensive experiments demonstrate that we can generate high-quality 3D\nheads with accurate geometry and texture from a single in-the-wild portrait\nimage.\n","authors":["Jinkun Hao","Junshu Tang","Jiangning Zhang","Ran Yi","Yijia Hong","Moran Li","Weijian Cao","Yating Wang","Chengjie Wang","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2406.16710v2.pdf","comment":"Accepted by AAAI 2025; Project page:\n  https://jinkun-hao.github.io/ID-Sculpt/"},{"id":"http://arxiv.org/abs/2411.10958v2","updated":"2024-12-19T15:26:20Z","published":"2024-11-17T04:35:49Z","title":"SageAttention2: Efficient Attention with Thorough Outlier Smoothing and\n  Per-thread INT4 Quantization","summary":"  Although quantization for linear layers has been widely used, its application\nto accelerate the attention process remains limited. To further enhance the\nefficiency of attention computation compared to SageAttention while maintaining\nprecision, we propose SageAttention2, which utilizes significantly faster 4-bit\nmatrix multiplication (Matmul) alongside additional precision-enhancing\ntechniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a\nhardware-friendly thread-level granularity and quantize matrixes $(\\widetilde\nP, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the\naccuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$\nto enhance the accuracy of FP8 $\\widetilde PV$. The operations per second (OPS)\nof SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on\nRTX4090, respectively. Comprehensive experiments confirm that our approach\nincurs negligible end-to-end metrics loss across diverse models, including\nthose for large language processing, image generation, and video generation.\nThe codes are available at https://github.com/thu-ml/SageAttention.\n","authors":["Jintao Zhang","Haofeng Huang","Pengle Zhang","Jia Wei","Jun Zhu","Jianfei Chen"],"pdf_url":"https://arxiv.org/pdf/2411.10958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14939v1","updated":"2024-12-19T15:15:03Z","published":"2024-12-19T15:15:03Z","title":"GURecon: Learning Detailed 3D Geometric Uncertainties for Neural Surface\n  Reconstruction","summary":"  Neural surface representation has demonstrated remarkable success in the\nareas of novel view synthesis and 3D reconstruction. However, assessing the\ngeometric quality of 3D reconstructions in the absence of ground truth mesh\nremains a significant challenge, due to its rendering-based optimization\nprocess and entangled learning of appearance and geometry with photometric\nlosses. In this paper, we present a novel framework, i.e, GURecon, which\nestablishes a geometric uncertainty field for the neural surface based on\ngeometric consistency. Different from existing methods that rely on\nrendering-based measurement, GURecon models a continuous 3D uncertainty field\nfor the reconstructed surface, and is learned by an online distillation\napproach without introducing real geometric information for supervision.\nMoreover, in order to mitigate the interference of illumination on geometric\nconsistency, a decoupled field is learned and exploited to finetune the\nuncertainty field. Experiments on various datasets demonstrate the superiority\nof GURecon in modeling 3D geometric uncertainty, as well as its plug-and-play\nextension to various neural surface representations and improvement on\ndownstream tasks such as incremental reconstruction. The code and supplementary\nmaterial are available on the project website:\nhttps://zju3dv.github.io/GURecon/.\n","authors":["Zesong Yang","Ru Zhang","Jiale Shi","Zixiang Ai","Boming Zhao","Hujun Bao","Luwei Yang","Zhaopeng Cui"],"pdf_url":"https://arxiv.org/pdf/2412.14939v1.pdf","comment":"Accepted by AAAI 2025. Project page:\n  https://zju3dv.github.io/gurecon/"},{"id":"http://arxiv.org/abs/2302.11947v2","updated":"2024-12-19T15:13:46Z","published":"2023-02-23T11:44:43Z","title":"Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight\n  Convolutional Neural Networks","summary":"  The health and safety hazards posed by worn crane lifting ropes mandate\nperiodic inspection for damage. This task is time-consuming, prone to human\nerror, halts operation, and may result in the premature disposal of ropes.\nTherefore, we propose using efficient deep learning and computer vision methods\nto automate the process of detecting damaged ropes. Specifically, we present a\nvision-based system for detecting damage in synthetic fiber rope images using\nlightweight convolutional neural networks. We develop a camera-based apparatus\nto photograph the lifting rope's surface, while in operation, and capture the\nprogressive wear-and-tear as well as the more significant degradation in the\nrope's health state. Experts from Konecranes annotate the collected images in\naccordance with the rope's condition; normal or damaged. Then, we pre-process\nthe images, systematically design a deep learning model, evaluate its detection\nand prediction performance, analyze its computational complexity, and compare\nit with various other models. Experimental results show the proposed model\noutperforms other similar techniques with 96.5% accuracy, 94.8% precision,\n98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the\nmodel's real-time operation, low memory footprint, robustness to various\nenvironmental and operational conditions, and adequacy for deployment in\nindustrial applications such as lifting, mooring, towing, climbing, and\nsailing.\n","authors":["Tuomas Jalonen","Mohammad Al-Sa'd","Roope Mellanen","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2302.11947v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14925v1","updated":"2024-12-19T15:02:50Z","published":"2024-12-19T15:02:50Z","title":"Automatic Spectral Calibration of Hyperspectral Images:Method, Dataset\n  and Benchmark","summary":"  Hyperspectral image (HSI) densely samples the world in both the space and\nfrequency domain and therefore is more distinctive than RGB images. Usually,\nHSI needs to be calibrated to minimize the impact of various illumination\nconditions. The traditional way to calibrate HSI utilizes a physical reference,\nwhich involves manual operations, occlusions, and/or limits camera mobility.\nThese limitations inspire this paper to automatically calibrate HSIs using a\nlearning-based method. Towards this goal, a large-scale HSI calibration dataset\nis created, which has 765 high-quality HSI pairs covering diversified natural\nscenes and illuminations. The dataset is further expanded to 7650 pairs by\ncombining with 10 different physically measured illuminations. A spectral\nillumination transformer (SIT) together with an illumination attention module\nis proposed. Extensive benchmarks demonstrate the SoTA performance of the\nproposed SIT. The benchmarks also indicate that low-light conditions are more\nchallenging than normal conditions. The dataset and codes are available\nonline:https://github.com/duranze/Automatic-spectral-calibration-of-HSI\n","authors":["Zhuoran Du","Shaodi You","Cheng Cheng","Shikui Wei"],"pdf_url":"https://arxiv.org/pdf/2412.14925v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.04272v2","updated":"2024-12-19T15:02:37Z","published":"2024-09-06T13:28:05Z","title":"Cycle Pixel Difference Network for Crisp Edge Detection","summary":"  Edge detection, as a fundamental task in computer vision, has garnered\nincreasing attention. The advent of deep learning has significantly advanced\nthis field. However, recent deep learning-based methods generally face two\nsignificant issues: 1) reliance on large-scale pre-trained weights, and 2)\ngeneration of thick edges. We construct a U-shape encoder-decoder model named\nCPD-Net that successfully addresses these two issues simultaneously. In\nresponse to issue 1), we propose a novel cycle pixel difference convolution\n(CPDC), which effectively integrates edge prior knowledge with modern\nconvolution operations, consequently successfully eliminating the dependence on\nlarge-scale pre-trained weights. As for issue 2), we construct a multi-scale\ninformation enhancement module (MSEM) and a dual residual connection-based\n(DRC) decoder to enhance the edge location ability of the model, thereby\ngenerating crisp and clean contour maps. Comprehensive experiments conducted on\nfour standard benchmarks demonstrate that our method achieves competitive\nperformance on the BSDS500 dataset (ODS=0.813 and AC=0.352), NYUD-V2 (ODS=0.760\nand AC=0.223), BIPED dataset (ODS=0.898 and AC=0.426), and CID (ODS=0.59). Our\napproach provides a novel perspective for addressing these challenges in edge\ndetection.\n","authors":["Changsong Liu","Wei Zhang","Yanyan Liu","Mingyang Li","Wenlin Li","Yimeng Fan","Xiangnan Bai","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.04272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16571v4","updated":"2024-12-19T14:47:05Z","published":"2024-04-25T12:34:23Z","title":"MonoPCC: Photometric-invariant Cycle Constraint for Monocular Depth\n  Estimation of Endoscopic Images","summary":"  Photometric constraint is indispensable for self-supervised monocular depth\nestimation. It involves warping a source image onto a target view using\nestimated depth&pose, and then minimizing the difference between the warped and\ntarget images. However, the endoscopic built-in light causes significant\nbrightness fluctuations, and thus makes the photometric constraint unreliable.\nPrevious efforts only mitigate this relying on extra models to calibrate image\nbrightness. In this paper, we propose MonoPCC to address the brightness\ninconsistency radically by reshaping the photometric constraint into a cycle\nform. Instead of only warping the source image, MonoPCC constructs a closed\nloop consisting of two opposite forward-backward warping paths: from target to\nsource and then back to target. Thus, the target image finally receives an\nimage cycle-warped from itself, which naturally makes the constraint invariant\nto brightness changes. Moreover, MonoPCC transplants the source image's\nphase-frequency into the intermediate warped image to avoid structure lost, and\nalso stabilizes the training via an exponential moving average (EMA) strategy\nto avoid frequent changes in the forward warping. The comprehensive and\nextensive experimental results on four endoscopic datasets demonstrate that our\nproposed MonoPCC shows a great robustness to the brightness inconsistency, and\nexceeds other state-of-the-arts by reducing the absolute relative error by at\nleast 7.27%, 9.38%, 9.90% and 3.17%, respectively.\n","authors":["Zhiwei Wang","Ying Zhou","Shiquan He","Ting Li","Fan Huang","Qiang Ding","Xinxia Feng","Mei Liu","Qiang Li"],"pdf_url":"https://arxiv.org/pdf/2404.16571v4.pdf","comment":"14 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.18512v2","updated":"2024-12-19T14:46:05Z","published":"2023-11-30T12:40:23Z","title":"Union-over-Intersections: Object Detection beyond Winner-Takes-All","summary":"  This paper revisits the problem of predicting box locations in object\ndetection architectures. Typically, each box proposal or box query aims to\ndirectly maximize the intersection-over-union score with the ground truth,\nfollowed by a winner-takes-all non-maximum suppression where only the highest\nscoring box in each region is retained. We observe that both steps are\nsub-optimal: the first involves regressing proposals to the entire ground\ntruth, which is a difficult task even with large receptive fields, and the\nsecond neglects valuable information from boxes other than the top candidate.\nInstead of regressing proposals to the whole ground truth, we propose a simpler\napproach: regress only to the area of intersection between the proposal and the\nground truth. This avoids the need for proposals to extrapolate beyond their\nvisual scope, improving localization accuracy. Rather than adopting a\nwinner-takes-all strategy, we take the union over the regressed intersections\nof all boxes in a region to generate the final box outputs. Our plug-and-play\nmethod integrates seamlessly into proposal-based, grid-based, and query-based\ndetection architectures with minimal modifications, consistently improving\nobject localization and instance segmentation. We demonstrate its broad\napplicability and versatility across various detection and segmentation tasks.\n","authors":["Aritra Bhowmik","Pascal Mettes","Martin R. Oswald","Cees G. M. Snoek"],"pdf_url":"https://arxiv.org/pdf/2311.18512v2.pdf","comment":"17 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2312.06259v2","updated":"2024-12-19T14:38:47Z","published":"2023-12-11T09:57:09Z","title":"Point Cloud Semantic Segmentation with Sparse and Inhomogeneous\n  Annotations","summary":"  Utilizing uniformly distributed sparse annotations, weakly supervised\nlearning alleviates the heavy reliance on fine-grained annotations in point\ncloud semantic segmentation tasks. However, few works discuss the inhomogeneity\nof sparse annotations, albeit it is common in real-world scenarios. Therefore,\nthis work introduces the probability density function into the gradient\nsampling approximation method to qualitatively analyze the impact of annotation\nsparsity and inhomogeneity under weakly supervised learning. Based on our\nanalysis, we propose an Adaptive Annotation Distribution Network (AADNet)\ncapable of robust learning on arbitrarily distributed sparse annotations.\nSpecifically, we propose a label-aware point cloud downsampling strategy to\nincrease the proportion of annotations involved in the training stage.\nFurthermore, we design the multiplicative dynamic entropy as the gradient\ncalibration function to mitigate the gradient bias caused by non-uniformly\ndistributed sparse annotations and explicitly reduce the epistemic uncertainty.\nWithout any prior restrictions and additional information, our proposed method\nachieves comprehensive performance improvements at multiple label rates and\ndifferent annotation distributions.\n","authors":["Zhiyi Pan","Nan Zhang","Wei Gao","Shan Liu","Ge Li"],"pdf_url":"https://arxiv.org/pdf/2312.06259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14902v1","updated":"2024-12-19T14:32:11Z","published":"2024-12-19T14:32:11Z","title":"MagicNaming: Consistent Identity Generation by Finding a \"Name Space\" in\n  T2I Diffusion Models","summary":"  Large-scale text-to-image diffusion models, (e.g., DALL-E, SDXL) are capable\nof generating famous persons by simply referring to their names. Is it possible\nto make such models generate generic identities as simple as the famous ones,\ne.g., just use a name? In this paper, we explore the existence of a \"Name\nSpace\", where any point in the space corresponds to a specific identity.\nFortunately, we find some clues in the feature space spanned by text embedding\nof celebrities' names. Specifically, we first extract the embeddings of\ncelebrities' names in the Laion5B dataset with the text encoder of diffusion\nmodels. Such embeddings are used as supervision to learn an encoder that can\npredict the name (actually an embedding) of a given face image. We\nexperimentally find that such name embeddings work well in promising the\ngenerated image with good identity consistency. Note that like the names of\ncelebrities, our predicted name embeddings are disentangled from the semantics\nof text inputs, making the original generation capability of text-to-image\nmodels well-preserved. Moreover, by simply plugging such name embeddings, all\nvariants (e.g., from Civitai) derived from the same base model (i.e., SDXL)\nreadily become identity-aware text-to-image models. Project homepage:\n\\url{https://magicfusion.github.io/MagicNaming/}.\n","authors":["Jing Zhao","Heliang Zheng","Chaoyue Wang","Long Lan","Wanrong Hunag","Yuhua Tang"],"pdf_url":"https://arxiv.org/pdf/2412.14902v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.13099v2","updated":"2024-12-19T14:23:45Z","published":"2024-12-17T17:10:02Z","title":"Accuracy Limits as a Barrier to Biometric System Security","summary":"  Biometric systems are widely used for identity verification and\nidentification, including authentication (i.e., one-to-one matching to verify a\nclaimed identity) and identification (i.e., one-to-many matching to find a\nsubject in a database). The matching process relies on measuring similarities\nor dissimilarities between a fresh biometric template and enrolled templates.\nThe False Match Rate FMR is a key metric for assessing the accuracy and\nreliability of such systems. This paper analyzes biometric systems based on\ntheir FMR, with two main contributions. First, we explore untargeted attacks,\nwhere an adversary aims to impersonate any user within a database. We determine\nthe number of trials required for an attacker to successfully impersonate a\nuser and derive the critical population size (i.e., the maximum number of users\nin the database) required to maintain a given level of security. Furthermore,\nwe compute the critical FMR value needed to ensure resistance against\nuntargeted attacks as the database size increases. Second, we revisit the\nbiometric birthday problem to evaluate the approximate and exact probabilities\nthat two users in a database collide (i.e., can impersonate each other). Based\non this analysis, we derive both the approximate critical population size and\nthe critical FMR value needed to bound the likelihood of such collisions\noccurring with a given probability. These thresholds offer insights for\ndesigning systems that mitigate the risk of impersonation and collisions,\nparticularly in large-scale biometric databases. Our findings indicate that\ncurrent biometric systems fail to deliver sufficient accuracy to achieve an\nadequate security level against untargeted attacks, even in small-scale\ndatabases. Moreover, state-of-the-art systems face significant challenges in\naddressing the biometric birthday problem, especially as database sizes grow.\n","authors":["Axel Durbet","Paul-Marie Grollemund","Pascal Lafourcade","Kevin Thiry-Atighehchi"],"pdf_url":"https://arxiv.org/pdf/2412.13099v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.14118v2","updated":"2024-12-19T14:18:57Z","published":"2024-12-18T18:04:12Z","title":"GaraMoSt: Parallel Multi-Granularity Motion and Structural Modeling for\n  Efficient Multi-Frame Interpolation in DSA Images","summary":"  The rapid and accurate direct multi-frame interpolation method for Digital\nSubtraction Angiography (DSA) images is crucial for reducing radiation and\nproviding real-time assistance to physicians for precise diagnostics and\ntreatment. DSA images contain complex vascular structures and various motions.\nApplying natural scene Video Frame Interpolation (VFI) methods results in\nmotion artifacts, structural dissipation, and blurriness. Recently, MoSt-DSA\nhas specifically addressed these issues for the first time and achieved SOTA\nresults. However, MoSt-DSA's focus on real-time performance leads to\ninsufficient suppression of high-frequency noise and incomplete filtering of\nlow-frequency noise in the generated images. To address these issues within the\nsame computational time scale, we propose GaraMoSt. Specifically, we optimize\nthe network pipeline with a parallel design and propose a module named MG-MSFE.\nMG-MSFE extracts frame-relative motion and structural features at various\ngranularities in a fully convolutional parallel manner and supports\nindependent, flexible adjustment of context-aware granularity at different\nscales, thus enhancing computational efficiency and accuracy. Extensive\nexperiments demonstrate that GaraMoSt achieves the SOTA performance in\naccuracy, robustness, visual effects, and noise suppression, comprehensively\nsurpassing MoSt-DSA and other natural scene VFI methods. The code and models\nare available at https://github.com/ZyoungXu/GaraMoSt.\n","authors":["Ziyang Xu","Huangxuan Zhao","Wenyu Liu","Xinggang Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14118v2.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2411.13093v2","updated":"2024-12-19T14:17:13Z","published":"2024-11-20T07:44:34Z","title":"Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension","summary":"  Existing large video-language models (LVLMs) struggle to comprehend long\nvideos correctly due to limited context. To address this problem, fine-tuning\nlong-context LVLMs and employing GPT-based agents have emerged as promising\nsolutions. However, fine-tuning LVLMs would require extensive high-quality data\nand substantial GPU resources, while GPT-based agents would rely on proprietary\nmodels (e.g., GPT-4o). In this paper, we propose Video Retrieval-Augmented\nGeneration (Video-RAG), a training-free and cost-effective pipeline that\nemploys visually-aligned auxiliary texts to help facilitate cross-modality\nalignment while providing additional information beyond the visual content.\nSpecifically, we leverage open-source external tools to extract\nvisually-aligned information from pure video data (e.g., audio, optical\ncharacter, and object detection), and incorporate the extracted information\ninto an existing LVLM as auxiliary texts, alongside video frames and queries,\nin a plug-and-play manner. Our Video-RAG offers several key advantages: (i)\nlightweight with low computing overhead due to single-turn retrieval; (ii) easy\nimplementation and compatibility with any LVLM; and (iii) significant,\nconsistent performance gains across long video understanding benchmarks,\nincluding Video-MME, MLVU, and LongVideoBench. Notably, our model demonstrates\nsuperior performance over proprietary models like Gemini-1.5-Pro and GPT-4o\nwhen utilized with a 72B model.\n","authors":["Yongdong Luo","Xiawu Zheng","Xiao Yang","Guilin Li","Haojia Lin","Jinfa Huang","Jiayi Ji","Fei Chao","Jiebo Luo","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2411.13093v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.14880v1","updated":"2024-12-19T14:17:09Z","published":"2024-12-19T14:17:09Z","title":"Multimodal Hypothetical Summary for Retrieval-based Multi-image Question\n  Answering","summary":"  Retrieval-based multi-image question answering (QA) task involves retrieving\nmultiple question-related images and synthesizing these images to generate an\nanswer. Conventional \"retrieve-then-answer\" pipelines often suffer from\ncascading errors because the training objective of QA fails to optimize the\nretrieval stage. To address this issue, we propose a novel method to\neffectively introduce and reference retrieved information into the QA. Given\nthe image set to be retrieved, we employ a multimodal large language model\n(visual perspective) and a large language model (textual perspective) to obtain\nmultimodal hypothetical summary in question-form and description-form. By\ncombining visual and textual perspectives, MHyS captures image content more\nspecifically and replaces real images in retrieval, which eliminates the\nmodality gap by transforming into text-to-text retrieval and helps improve\nretrieval. To more advantageously introduce retrieval with QA, we employ\ncontrastive learning to align queries (questions) with MHyS. Moreover, we\npropose a coarse-to-fine strategy for calculating both sentence-level and\nword-level similarity scores, to further enhance retrieval and filter out\nirrelevant details. Our approach achieves a 3.7% absolute improvement over\nstate-of-the-art methods on RETVQA and a 14.5% improvement over CLIP.\nComprehensive experiments and detailed ablation studies demonstrate the\nsuperiority of our method.\n","authors":["Peize Li","Qingyi Si","Peng Fu","Zheng Lin","Yan Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14880v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14873v1","updated":"2024-12-19T14:11:49Z","published":"2024-12-19T14:11:49Z","title":"Zero-Shot Artifact2Artifact: Self-incentive artifact removal for\n  photoacoustic imaging without any data","summary":"  Photoacoustic imaging (PAI) uniquely combines optical contrast with the\npenetration depth of ultrasound, making it critical for clinical applications.\nHowever, the quality of 3D PAI is often degraded due to reconstruction\nartifacts caused by the sparse and angle-limited configuration of detector\narrays. Existing iterative or deep learning-based methods are either\ntime-consuming or require large training datasets, significantly limiting their\npractical application. Here, we propose Zero-Shot Artifact2Artifact (ZS-A2A), a\nzero-shot self-supervised artifact removal method based on a super-lightweight\nnetwork, which leverages the fact that reconstruction artifacts are sensitive\nto irregularities caused by data loss. By introducing random perturbations to\nthe acquired PA data, it spontaneously generates subset data, which in turn\nstimulates the network to learn the artifact patterns in the reconstruction\nresults, thus enabling zero-shot artifact removal. This approach requires\nneither training data nor prior knowledge of the artifacts, and is capable of\nartifact removal for 3D PAI. For maximum amplitude projection (MAP) images or\nslice images in 3D PAI acquired with arbitrarily sparse or angle-limited\ndetector arrays, ZS-A2A employs a self-incentive strategy to complete artifact\nremoval and improves the Contrast-to-Noise Ratio (CNR). We validated ZS-A2A in\nboth simulation study and $ in\\ vivo $ animal experiments. Results demonstrate\nthat ZS-A2A achieves state-of-the-art (SOTA) performance compared to existing\nzero-shot methods, and for the $ in\\ vivo $ rat liver, ZS-A2A improves CNR from\n17.48 to 43.46 in just 8 seconds. The project for ZS-A2A will be available in\nthe following GitHub repository: https://github.com/JaegerCQ/ZS-A2A.\n","authors":["Shuang Li","Qian Chen","Chulhong Kim","Seongwook Choi","Yibing Wang","Yu Zhang","Changhui Li"],"pdf_url":"https://arxiv.org/pdf/2412.14873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.13620v3","updated":"2024-12-19T14:08:42Z","published":"2022-04-28T16:35:04Z","title":"Generative Adversarial Networks for Image Super-Resolution: A Survey","summary":"  Single image super-resolution (SISR) has played an important role in the\nfield of image processing. Recent generative adversarial networks (GANs) can\nachieve excellent results on low-resolution images with small samples. However,\nthere are little literatures summarizing different GANs in SISR. In this paper,\nwe conduct a comparative study of GANs from different perspectives. We first\ntake a look at developments of GANs. Second, we present popular architectures\nfor GANs in big and small samples for image applications. Then, we analyze\nmotivations, implementations and differences of GANs based optimization methods\nand discriminative learning for image super-resolution in terms of supervised,\nsemi-supervised and unsupervised manners, where these GANs are analyzed via\nintegrating different network architectures, prior knowledge, loss functions\nand multiple tasks. Next, we compare performance of these popular GANs on\npublic datasets via quantitative and qualitative analysis in SISR. Finally, we\nhighlight challenges of GANs and potential research points for SISR.\n","authors":["Chunwei Tian","Xuanyu Zhang","Qi Zhu","Bob Zhang","Jerry Chun-Wei Lin"],"pdf_url":"https://arxiv.org/pdf/2204.13620v3.pdf","comment":"31pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.07024v3","updated":"2024-12-19T14:07:44Z","published":"2024-07-09T16:44:04Z","title":"Exploring Scalability of Self-Training for Open-Vocabulary Temporal\n  Action Localization","summary":"  The vocabulary size in temporal action localization (TAL) is limited by the\nscarcity of large-scale annotated datasets. To overcome this, recent works\nintegrate vision-language models (VLMs), such as CLIP, for open-vocabulary TAL\n(OV-TAL). However, despite the success of VLMs trained on extensive datasets,\nexisting OV-TAL methods still rely on human-labeled TAL datasets of limited\nsize to train action localizers, limiting their generalizability. In this\npaper, we explore the scalability of self-training with unlabeled YouTube\nvideos for OV-TAL. Our approach consists of two stages: (1) a class-agnostic\naction localizer is trained on a human-labeled TAL dataset to generate\npseudo-labels for unlabeled videos, and (2) the large-scale pseudo-labeled\ndataset is then used to train the localizer. Extensive experiments demonstrate\nthat leveraging web-scale videos in self-training significantly enhances the\ngeneralizability of an action localizer. Additionally, we identify limitations\nin existing OV-TAL evaluation schemes and propose a new benchmark for thorough\nassessment. Finally, we showcase the TAL performance of the large multimodal\nmodel Gemini-1.5 on our new benchmark. Code is released at\nhttps://github.com/HYUNJS/STOV-TAL.\n","authors":["Jeongseok Hyun","Su Ho Han","Hyolim Kang","Joon-Young Lee","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2407.07024v3.pdf","comment":"Accepted to WACV 2025"},{"id":"http://arxiv.org/abs/2412.14870v1","updated":"2024-12-19T14:06:56Z","published":"2024-12-19T14:06:56Z","title":"Large-scale School Mapping using Weakly Supervised Deep Learning for\n  Universal School Connectivity","summary":"  Improving global school connectivity is critical for ensuring inclusive and\nequitable quality education. To reliably estimate the cost of connecting\nschools, governments and connectivity providers require complete and accurate\nschool location data - a resource that is often scarce in many low- and\nmiddle-income countries. To address this challenge, we propose a\ncost-effective, scalable approach to locating schools in high-resolution\nsatellite images using weakly supervised deep learning techniques. Our best\nmodels, which combine vision transformers and convolutional neural networks,\nachieve AUPRC values above 0.96 across 10 pilot African countries. Leveraging\nexplainable AI techniques, our approach can approximate the precise\ngeographical coordinates of the school locations using only low-cost,\nclassification-level annotations. To demonstrate the scalability of our method,\nwe generate nationwide maps of school location predictions in African countries\nand present a detailed analysis of our results, using Senegal as our case\nstudy. Finally, we demonstrate the immediate usability of our work by\nintroducing an interactive web mapping tool to streamline human-in-the-loop\nmodel validation efforts by government partners. This work successfully\nshowcases the real-world utility of deep learning and satellite images for\nplanning regional infrastructure and accelerating universal school\nconnectivity.\n","authors":["Isabelle Tingzon","Utku Can Ozturk","Ivan Dotu"],"pdf_url":"https://arxiv.org/pdf/2412.14870v1.pdf","comment":"Accepted at AAAI-25 Special Track on AI for Social Impact (AISI)"},{"id":"http://arxiv.org/abs/2412.14869v1","updated":"2024-12-19T14:06:44Z","published":"2024-12-19T14:06:44Z","title":"AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional\n  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature\n  Screening","summary":"  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood\nwithin the skull, which occurs due to the rupture of blood vessels in or around\nthe brain. If this condition is not diagnosed in a timely manner and\nappropriately treated, it can lead to serious complications such as decreased\nconsciousness, permanent neurological disabilities, or even death.The primary\naim of this study is to detect the occurrence or non-occurrence of ICH,\nfollowed by determining the type of subdural hemorrhage (SDH). These tasks are\nframed as two separate binary classification problems. By adding two layers to\nthe co-scale convolutional attention (CCA) classifier architecture, we\nintroduce a novel approach for ICH detection. In the first layer, after\nextracting features from different slices of computed tomography (CT) scan\nimages, we combine these features and select the 50 components that capture the\nhighest variance in the data, considering them as informative features. We then\nassess the discriminative power of these features using the bootstrap forest\nalgorithm, discarding those that lack sufficient discriminative ability between\ndifferent classes. This algorithm explicitly determines the contribution of\neach feature to the final prediction, assisting us in developing an explainable\nAI model. The features feed into a boosting neural network as a latent feature\nspace. In the second layer, we introduce a novel uncertainty-based fuzzy\nintegral operator to fuse information from different CT scan slices. This\noperator, by accounting for the dependencies between consecutive slices,\nsignificantly improves detection accuracy.\n","authors":["Mehdi Hosseini Chagahi","Md. Jalil Piran","Niloufar Delfan","Behzad Moshiri","Jaber Hatam Parikhan"],"pdf_url":"https://arxiv.org/pdf/2412.14869v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20213v4","updated":"2024-12-19T13:46:40Z","published":"2024-03-29T14:50:43Z","title":"VHM: Versatile and Honest Vision Language Model for Remote Sensing Image\n  Analysis","summary":"  This paper develops a Versatile and Honest vision language Model (VHM) for\nremote sensing image analysis. VHM is built on a large-scale remote sensing\nimage-text dataset with rich-content captions (VersaD), and an honest\ninstruction dataset comprising both factual and deceptive questions (HnstD).\nUnlike prevailing remote sensing image-text datasets, in which image captions\nfocus on a few prominent objects and their relationships, VersaD captions\nprovide detailed information about image properties, object attributes, and the\noverall scene. This comprehensive captioning enables VHM to thoroughly\nunderstand remote sensing images and perform diverse remote sensing tasks.\nMoreover, different from existing remote sensing instruction datasets that only\ninclude factual questions, HnstD contains additional deceptive questions\nstemming from the non-existence of objects. This feature prevents VHM from\nproducing affirmative answers to nonsense queries, thereby ensuring its\nhonesty. In our experiments, VHM significantly outperforms various vision\nlanguage models on common tasks of scene classification, visual question\nanswering, and visual grounding. Additionally, VHM achieves competent\nperformance on several unexplored tasks, such as building vectorizing,\nmulti-label classification and honest question answering. We will release the\ncode, data and model weights at https://github.com/opendatalab/VHM .\n","authors":["Chao Pang","Xingxing Weng","Jiang Wu","Jiayu Li","Yi Liu","Jiaxing Sun","Weijia Li","Shuai Wang","Litong Feng","Gui-Song Xia","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2403.20213v4.pdf","comment":"Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding\n  author: Gui-Song Xia, Conghui He"},{"id":"http://arxiv.org/abs/2411.04865v4","updated":"2024-12-19T13:45:39Z","published":"2024-11-07T16:58:18Z","title":"ZAHA: Introducing the Level of Facade Generalization and the Large-Scale\n  Point Cloud Facade Semantic Segmentation Benchmark Dataset","summary":"  Facade semantic segmentation is a long-standing challenge in photogrammetry\nand computer vision. Although the last decades have witnessed the influx of\nfacade segmentation methods, there is a lack of comprehensive facade classes\nand data covering the architectural variability. In ZAHA, we introduce Level of\nFacade Generalization (LoFG), novel hierarchical facade classes designed based\non international urban modeling standards, ensuring compatibility with\nreal-world challenging classes and uniform methods' comparison. Realizing the\nLoFG, we present to date the largest semantic 3D facade segmentation dataset,\nproviding 601 million annotated points at five and 15 classes of LoFG2 and\nLoFG3, respectively. Moreover, we analyze the performance of baseline semantic\nsegmentation methods on our introduced LoFG classes and data, complementing it\nwith a discussion on the unresolved challenges for facade segmentation. We\nfirmly believe that ZAHA shall facilitate further development of 3D facade\nsemantic segmentation methods, enabling robust segmentation indispensable in\ncreating urban digital twins.\n","authors":["Olaf Wysocki","Yue Tan","Thomas Froech","Yan Xia","Magdalena Wysocki","Ludwig Hoegner","Daniel Cremers","Christoph Holst"],"pdf_url":"https://arxiv.org/pdf/2411.04865v4.pdf","comment":"Accepted to WACV 2025 (IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV))"},{"id":"http://arxiv.org/abs/2412.13913v2","updated":"2024-12-19T13:41:08Z","published":"2024-12-18T14:53:38Z","title":"A Black-Box Evaluation Framework for Semantic Robustness in Bird's Eye\n  View Detection","summary":"  Camera-based Bird's Eye View (BEV) perception models receive increasing\nattention for their crucial role in autonomous driving, a domain where concerns\nabout the robustness and reliability of deep learning have been raised. While\nonly a few works have investigated the effects of randomly generated semantic\nperturbations, aka natural corruptions, on the multi-view BEV detection task,\nwe develop a black-box robustness evaluation framework that adversarially\noptimises three common semantic perturbations: geometric transformation, colour\nshifting, and motion blur, to deceive BEV models, serving as the first approach\nin this emerging field. To address the challenge posed by optimising the\nsemantic perturbation, we design a smoothed, distance-based surrogate function\nto replace the mAP metric and introduce SimpleDIRECT, a deterministic\noptimisation algorithm that utilises observed slopes to guide the optimisation\nprocess. By comparing with randomised perturbation and two optimisation\nbaselines, we demonstrate the effectiveness of the proposed framework.\nAdditionally, we provide a benchmark on the semantic robustness of ten recent\nBEV models. The results reveal that PolarFormer, which emphasises geometric\ninformation from multi-view images, exhibits the highest robustness, whereas\nBEVDet is fully compromised, with its precision reduced to zero.\n","authors":["Fu Wang","Yanghao Zhang","Xiangyu Yin","Guangliang Cheng","Zeyu Fu","Xiaowei Huang","Wenjie Ruan"],"pdf_url":"https://arxiv.org/pdf/2412.13913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10929v6","updated":"2024-12-19T13:39:55Z","published":"2024-10-14T16:35:27Z","title":"ASTM :Autonomous Smart Traffic Management System Using Artificial\n  Intelligence CNN and LSTM","summary":"  In the modern world, the development of Artificial Intelligence (AI) has\ncontributed to improvements in various areas, including automation, computer\nvision, fraud detection, and more. AI can be leveraged to enhance the\nefficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce\ntraffic congestion rates. This paper presents an Autonomous Smart Traffic\nManagement (STM) system that uses AI to improve traffic flow rates. The system\nemploys the YOLO V5 Convolutional Neural Network to detect vehicles in traffic\nmanagement images. Additionally, it predicts the number of vehicles for the\nnext 12 hours using a Recurrent Neural Network with Long Short-Term Memory\n(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the\ntraffic cycle length based on these vehicle predictions, aided by AI. From the\nresults of the RNN-LSTM model for predicting vehicle numbers over the next 12\nhours, we observe that the model predicts traffic with a Mean Squared Error\n(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.\nAfter simulating the STM system in the CARLA simulation environment, we found\nthat the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per\nminute) is 50\\% higher than the rate without STM (around 15 vehicles per\nminute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5\nseconds per vehicle) is 70\\% lower than without STM (around 12 seconds per\nvehicle). These results demonstrate that the STM system using AI can increase\ntraffic flow by 50\\% and reduce vehicle pass delays by 70\\%.\n","authors":["Christofel Rio Goenawan"],"pdf_url":"https://arxiv.org/pdf/2410.10929v6.pdf","comment":"In process to IEEE Intelligent Vehicle Symposium 2025"},{"id":"http://arxiv.org/abs/2412.14846v1","updated":"2024-12-19T13:38:20Z","published":"2024-12-19T13:38:20Z","title":"Head and Neck Tumor Segmentation of MRI from Pre- and Mid-radiotherapy\n  with Pre-training, Data Augmentation and Dual Flow UNet","summary":"  Head and neck tumors and metastatic lymph nodes are crucial for treatment\nplanning and prognostic analysis. Accurate segmentation and quantitative\nanalysis of these structures require pixel-level annotation, making automated\nsegmentation techniques essential for the diagnosis and treatment of head and\nneck cancer. In this study, we investigated the effects of multiple strategies\non the segmentation of pre-radiotherapy (pre-RT) and mid-radiotherapy (mid-RT)\nimages. For the segmentation of pre-RT images, we utilized: 1) a fully\nsupervised learning approach, and 2) the same approach enhanced with\npre-trained weights and the MixUp data augmentation technique. For mid-RT\nimages, we introduced a novel computational-friendly network architecture that\nfeatures separate encoders for mid-RT images and registered pre-RT images with\ntheir labels. The mid-RT encoder branch integrates information from pre-RT\nimages and labels progressively during the forward propagation. We selected the\nhighest-performing model from each fold and used their predictions to create an\nensemble average for inference. In the final test, our models achieved a\nsegmentation performance of 82.38% for pre-RT and 72.53% for mid-RT on\naggregated Dice Similarity Coefficient (DSC) as HiLab. Our code is available at\nhttps://github.com/WltyBY/HNTS-MRG2024_train_code.\n","authors":["Litingyu Wang","Wenjun Liao","Shichuan Zhang","Guotai Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14837v1","updated":"2024-12-19T13:27:58Z","published":"2024-12-19T13:27:58Z","title":"ObjVariantEnsemble: Advancing Point Cloud LLM Evaluation in Challenging\n  Scenes with Subtly Distinguished Objects","summary":"  3D scene understanding is an important task, and there has been a recent\nsurge of research interest in aligning 3D representations of point clouds with\ntext to empower embodied AI. However, due to the lack of comprehensive 3D\nbenchmarks, the capabilities of 3D models in real-world scenes, particularly\nthose that are challenging with subtly distinguished objects, remain\ninsufficiently investigated. To facilitate a more thorough evaluation of 3D\nmodels' capabilities, we propose a scheme, ObjVariantEnsemble, to\nsystematically introduce more scenes with specified object classes, colors,\nshapes, quantities, and spatial relationships to meet model evaluation needs.\nMore importantly, we intentionally construct scenes with similar objects to a\ncertain degree and design an LLM-VLM-cooperated annotator to capture key\ndistinctions as annotations. The resultant benchmark can better challenge 3D\nmodels, reveal their shortcomings in understanding, and potentially aid in the\nfurther development of 3D models.\n","authors":["Qihang Cao","Huangxun Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14837v1.pdf","comment":"Accepted to AAAI2025"},{"id":"http://arxiv.org/abs/2412.14835v1","updated":"2024-12-19T13:25:39Z","published":"2024-12-19T13:25:39Z","title":"Progressive Multimodal Reasoning via Active Retrieval","summary":"  Multi-step multimodal reasoning tasks pose significant challenges for\nmultimodal large language models (MLLMs), and finding effective ways to enhance\ntheir performance in such scenarios remains an unresolved issue. In this paper,\nwe propose AR-MCTS, a universal framework designed to progressively improve the\nreasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo\nTree Search (MCTS). Our approach begins with the development of a unified\nretrieval module that retrieves key supporting insights for solving complex\nreasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in\nautomated multimodal reasoning verification, we employ the MCTS algorithm\ncombined with an active retrieval mechanism, which enables the automatic\ngeneration of step-wise annotations. This strategy dynamically retrieves key\ninsights for each reasoning step, moving beyond traditional beam search\nsampling to improve the diversity and reliability of the reasoning space.\nAdditionally, we introduce a process reward model that aligns progressively to\nsupport the automatic verification of multimodal reasoning tasks. Experimental\nresults across three complex multimodal reasoning benchmarks confirm the\neffectiveness of the AR-MCTS framework in enhancing the performance of various\nmultimodal models. Further analysis demonstrates that AR-MCTS can optimize\nsampling diversity and accuracy, yielding reliable multimodal reasoning.\n","authors":["Guanting Dong","Chenghao Zhang","Mengjie Deng","Yutao Zhu","Zhicheng Dou","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2412.14835v1.pdf","comment":"Working in progress"},{"id":"http://arxiv.org/abs/2412.14833v1","updated":"2024-12-19T13:21:04Z","published":"2024-12-19T13:21:04Z","title":"Synchronized and Fine-Grained Head for Skeleton-Based Ambiguous Action\n  Recognition","summary":"  Skeleton-based action recognition using GCNs has achieved remarkable\nperformance, but recognizing ambiguous actions, such as \"waving\" and\n\"saluting\", remains a significant challenge. Existing methods typically rely on\na serial combination of GCNs and TCNs, where spatial and temporal features are\nextracted independently, leading to an unbalanced spatial-temporal information,\nwhich hinders accurate action recognition. Moreover, existing methods for\nambiguous actions often overemphasize local details, resulting in the loss of\ncrucial global context, which further complicates the task of differentiating\nambiguous actions. To address these challenges, we propose a lightweight\nplug-and-play module called Synchronized and Fine-grained Head (SF-Head),\ninserted between GCN and TCN layers. SF-Head first conducts Synchronized\nSpatial-Temporal Extraction (SSTE) with a Feature Redundancy Loss (F-RL),\nensuring a balanced interaction between the two types of features. It then\nperforms Adaptive Cross-dimensional Feature Aggregation (AC-FA), with a Feature\nConsistency Loss (F-CL), which aligns the aggregated feature with their\noriginal spatial-temporal feature. This aggregation step effectively combines\nboth global context and local details. Experimental results on NTU RGB+D 60,\nNTU RGB+D 120, and NW-UCLA datasets demonstrate significant improvements in\ndistinguishing ambiguous actions. Our code will be made available at\nhttps://github.com/HaoHuang2003/SFHead.\n","authors":["Hao Huang","Yujie Lin","Siyu Chen","Haiyang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14833v1.pdf","comment":"20pages, 5 figures"},{"id":"http://arxiv.org/abs/2412.14821v1","updated":"2024-12-19T13:12:15Z","published":"2024-12-19T13:12:15Z","title":"PC-BEV: An Efficient Polar-Cartesian BEV Fusion Framework for LiDAR\n  Semantic Segmentation","summary":"  Although multiview fusion has demonstrated potential in LiDAR segmentation,\nits dependence on computationally intensive point-based interactions, arising\nfrom the lack of fixed correspondences between views such as range view and\nBird's-Eye View (BEV), hinders its practical deployment. This paper challenges\nthe prevailing notion that multiview fusion is essential for achieving high\nperformance. We demonstrate that significant gains can be realized by directly\nfusing Polar and Cartesian partitioning strategies within the BEV space. Our\nproposed BEV-only segmentation model leverages the inherent fixed grid\ncorrespondences between these partitioning schemes, enabling a fusion process\nthat is orders of magnitude faster (170$\\times$ speedup) than conventional\npoint-based methods. Furthermore, our approach facilitates dense feature\nfusion, preserving richer contextual information compared to sparse point-based\nalternatives. To enhance scene understanding while maintaining inference\nefficiency, we also introduce a hybrid Transformer-CNN architecture. Extensive\nevaluation on the SemanticKITTI and nuScenes datasets provides compelling\nevidence that our method outperforms previous multiview fusion approaches in\nterms of both performance and inference speed, highlighting the potential of\nBEV-based fusion for LiDAR segmentation. Code is available at\n\\url{https://github.com/skyshoumeng/PC-BEV.}\n","authors":["Shoumeng Qiu","Xinrun Li","XiangYang Xue","Jian Pu"],"pdf_url":"https://arxiv.org/pdf/2412.14821v1.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14819v1","updated":"2024-12-19T13:10:38Z","published":"2024-12-19T13:10:38Z","title":"Multi-Level Embedding and Alignment Network with Consistency and\n  Invariance Learning for Cross-View Geo-Localization","summary":"  Cross-View Geo-Localization (CVGL) involves determining the localization of\ndrone images by retrieving the most similar GPS-tagged satellite images.\nHowever, the imaging gaps between platforms are often significant and the\nvariations in viewpoints are substantial, which limits the ability of existing\nmethods to effectively associate cross-view features and extract consistent and\ninvariant characteristics. Moreover, existing methods often overlook the\nproblem of increased computational and storage requirements when improving\nmodel performance. To handle these limitations, we propose a lightweight\nenhanced alignment network, called the Multi-Level Embedding and Alignment\nNetwork (MEAN). The MEAN network uses a progressive multi-level enhancement\nstrategy, global-to-local associations, and cross-domain alignment, enabling\nfeature communication across levels. This allows MEAN to effectively connect\nfeatures at different levels and learn robust cross-view consistent mappings\nand modality-invariant features. Moreover, MEAN adopts a shallow backbone\nnetwork combined with a lightweight branch design, effectively reducing\nparameter count and computational complexity. Experimental results on the\nUniversity-1652 and SUES-200 datasets demonstrate that MEAN reduces parameter\ncount by 62.17% and computational complexity by 70.99% compared to\nstate-of-the-art models, while maintaining competitive or even superior\nperformance. The codes will be released soon.\n","authors":["Zhongwei Chen","Zhao-Xu Yang","Hai-Jun Rong"],"pdf_url":"https://arxiv.org/pdf/2412.14819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14816v1","updated":"2024-12-19T13:10:03Z","published":"2024-12-19T13:10:03Z","title":"Explainable Tampered Text Detection via Multimodal Large Models","summary":"  Recently, tampered text detection has attracted increasing attention due to\nits essential role in information security. Although existing methods can\ndetect the tampered text region, the interpretation of such detection remains\nunclear, making the prediction unreliable. To address this black-box problem,\nwe propose to explain the basis of tampered text detection with natural\nlanguage via large multimodal models. To fill the data gap for this task, we\npropose a large-scale, comprehensive dataset, ETTD, which contains both\npixel-level annotations indicating the tampered text region and natural\nlanguage annotations describing the anomaly of the tampered text. Multiple\nmethods are employed to improve the quality of the proposed data. For example,\na fused mask prompt is proposed to reduce confusion when querying GPT4o to\ngenerate anomaly descriptions. By weighting the input image with the mask\nannotation, the tampered region can be clearly indicated and the content in and\naround the tampered region can also be preserved. We also propose prompting\nGPT4o to recognize tampered texts and filtering out the responses with low OCR\naccuracy, which can effectively improve annotation quality in an automatic\nmanner. To further improve explainable tampered text detection, we propose a\nsimple yet effective model called TTD, which benefits from improved\nfine-grained perception by paying attention to the suspected region with\nauxiliary reference grounding query. Extensive experiments on both the ETTD\ndataset and the public dataset have verified the effectiveness of the proposed\nmethods. In-depth analysis is also provided to inspire further research. The\ndataset and code will be made publicly available.\n","authors":["Chenfan Qu","Jian Liu","Haoxing Chen","Baihan Yu","Jingjing Liu","Weiqiang Wang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2412.14816v1.pdf","comment":"The first work for explainable tampered text detection"},{"id":"http://arxiv.org/abs/2304.02488v4","updated":"2024-12-19T13:00:35Z","published":"2023-04-05T15:02:30Z","title":"SCB-dataset: A Dataset for Detecting Student Classroom Behavior","summary":"  The use of deep learning methods for automatic detection of students'\nclassroom behavior is a promising approach to analyze their class performance\nand enhance teaching effectiveness. However, the lack of publicly available\ndatasets on student behavior poses a challenge for researchers in this field.\nTo address this issue, we propose a Student Classroom Behavior dataset\n(SCB-dataset) that reflects real-life scenarios. Our dataset includes 11,248\nlabels and 4,003 images, with a focus on hand-raising behavior. We evaluated\nthe dataset using the YOLOv7 algorithm, achieving a mean average precision\n(map) of up to 85.3%. We believe that our dataset can serve as a robust\nfoundation for future research in the field of student behavior detection and\npromote further advancements in this area.Our SCB-dataset can be downloaded\nfrom: https://github.com/Whiffe/SCB-dataset\n","authors":["Fan Yang"],"pdf_url":"https://arxiv.org/pdf/2304.02488v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13735v2","updated":"2024-12-19T12:59:31Z","published":"2024-12-18T11:14:01Z","title":"3D Registration in 30 Years: A Survey","summary":"  3D point cloud registration is a fundamental problem in computer vision,\ncomputer graphics, robotics, remote sensing, and etc. Over the last thirty\nyears, we have witnessed the amazing advancement in this area with numerous\nkinds of solutions. Although a handful of relevant surveys have been conducted,\ntheir coverage is still limited. In this work, we present a comprehensive\nsurvey on 3D point cloud registration, covering a set of sub-areas such as\npairwise coarse registration, pairwise fine registration, multi-view\nregistration, cross-scale registration, and multi-instance registration. The\ndatasets, evaluation metrics, method taxonomy, discussions of the merits and\ndemerits, insightful thoughts of future directions are comprehensively\npresented in this survey. The regularly updated project page of the survey is\navailable at https://github.com/Amyyyy11/3D-Registration-in-30-Years-A-Survey.\n","authors":["Jiaqi Yang","Chu'ai Zhang","Zhengbao Wang","Xinyue Cao","Xuan Ouyang","Xiyu Zhang","Zhenxuan Zeng","Zhao Zeng","Borui Lu","Zhiyi Xia","Qian Zhang","Yulan Guo","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.13735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14803v1","updated":"2024-12-19T12:48:40Z","published":"2024-12-19T12:48:40Z","title":"Video Prediction Policy: A Generalist Robot Policy with Predictive\n  Visual Representations","summary":"  Recent advancements in robotics have focused on developing generalist\npolicies capable of performing multiple tasks. Typically, these policies\nutilize pre-trained vision encoders to capture crucial information from current\nobservations. However, previous vision encoders, which trained on two-image\ncontrastive learning or single-image reconstruction, can not perfectly capture\nthe sequential information essential for embodied tasks. Recently, video\ndiffusion models (VDMs) have demonstrated the capability to accurately predict\nfuture image sequences, exhibiting a good understanding of physical dynamics.\nMotivated by the strong visual prediction capabilities of VDMs, we hypothesize\nthat they inherently possess visual representations that reflect the evolution\nof the physical world, which we term predictive visual representations.\nBuilding on this hypothesis, we propose the Video Prediction Policy (VPP), a\ngeneralist robotic policy conditioned on the predictive visual representations\nfrom VDMs. To further enhance these representations, we incorporate diverse\nhuman or robotic manipulation datasets, employing unified video-generation\ntraining objectives. VPP consistently outperforms existing methods across two\nsimulated and two real-world benchmarks. Notably, it achieves a 28.1\\% relative\nimprovement in the Calvin ABC-D benchmark compared to the previous\nstate-of-the-art and delivers a 28.8\\% increase in success rates for complex\nreal-world dexterous manipulation tasks.\n","authors":["Yucheng Hu","Yanjiang Guo","Pengchao Wang","Xiaoyu Chen","Yen-Jen Wang","Jianke Zhang","Koushil Sreenath","Chaochao Lu","Jianyu Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14803v1.pdf","comment":"The first two authors contribute equally. Project Page at\n  https://video-prediction-policy.github.io/"},{"id":"http://arxiv.org/abs/2410.05317v3","updated":"2024-12-19T12:38:23Z","published":"2024-10-05T03:47:06Z","title":"Accelerating Diffusion Transformers with Token-wise Feature Caching","summary":"  Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.\n","authors":["Chang Zou","Xuyang Liu","Ting Liu","Siteng Huang","Linfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.05317v3.pdf","comment":"In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix"},{"id":"http://arxiv.org/abs/2412.13803v2","updated":"2024-12-19T12:31:34Z","published":"2024-12-18T12:50:11Z","title":"M$^3$-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object\n  Segmentation","summary":"  Intelligent robots need to interact with diverse objects across various\nenvironments. The appearance and state of objects frequently undergo complex\ntransformations depending on the object properties, e.g., phase transitions.\nHowever, in the vision community, segmenting dynamic objects with phase\ntransitions is overlooked. In light of this, we introduce the concept of phase\nin segmentation, which categorizes real-world objects based on their visual\ncharacteristics and potential morphological and appearance changes. Then, we\npresent a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video\nObject Segmentation (M$^3$-VOS), to verify the ability of models to understand\nobject phases, which consists of 479 high-resolution videos spanning over 10\ndistinct everyday scenarios. It provides dense instance mask annotations that\ncapture both object phases and their transitions. We evaluate state-of-the-art\nmethods on M$^3$-VOS, yielding several key insights. Notably, current\nappearancebased approaches show significant room for improvement when handling\nobjects with phase transitions. The inherent changes in disorder suggest that\nthe predictive performance of the forward entropy-increasing process can be\nimproved through a reverse entropy-reducing process. These findings lead us to\npropose ReVOS, a new plug-andplay model that improves its performance by\nreversal refinement. Our data and code will be publicly available at\nhttps://zixuan-chen.github.io/M-cubeVOS.github.io/.\n","authors":["Zixuan Chen","Jiaxin Li","Liming Tan","Yejie Guo","Junxuan Liang","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2412.13803v2.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2412.14790v1","updated":"2024-12-19T12:29:31Z","published":"2024-12-19T12:29:31Z","title":"YOLOv11 Optimization for Efficient Resource Utilization","summary":"  The objective of this research is to optimize the eleventh iteration of You\nOnly Look Once (YOLOv11) by developing size-specific modified versions of the\narchitecture. These modifications involve pruning unnecessary layers and\nreconfiguring the main architecture of YOLOv11. Each proposed version is\ntailored to detect objects of specific size ranges, from small to large. To\nensure proper model selection based on dataset characteristics, we introduced\nan object classifier program. This program identifies the most suitable\nmodified version for a given dataset. The proposed models were evaluated on\nvarious datasets and compared with the original YOLOv11 and YOLOv8 models. The\nexperimental results highlight significant improvements in computational\nresource efficiency, with the proposed models maintaining the accuracy of the\noriginal YOLOv11. In some cases, the modified versions outperformed the\noriginal model regarding detection performance. Furthermore, the proposed\nmodels demonstrated reduced model sizes and faster inference times. Models\nweights and the object size classifier can be found in this repository\n","authors":["Areeg Fagad Rasheed","M. Zarkoosh"],"pdf_url":"https://arxiv.org/pdf/2412.14790v1.pdf","comment":"12 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2412.09401v2","updated":"2024-12-19T12:23:39Z","published":"2024-12-12T16:08:03Z","title":"SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos","summary":"  In this paper, we introduce SLAM3R, a novel and effective monocular RGB SLAM\nsystem for real-time and high-quality dense 3D reconstruction. SLAM3R provides\nan end-to-end solution by seamlessly integrating local 3D reconstruction and\nglobal coordinate registration through feed-forward neural networks. Given an\ninput video, the system first converts it into overlapping clips using a\nsliding window mechanism. Unlike traditional pose optimization-based methods,\nSLAM3R directly regresses 3D pointmaps from RGB images in each window and\nprogressively aligns and deforms these local pointmaps to create a globally\nconsistent scene reconstruction - all without explicitly solving any camera\nparameters. Experiments across datasets consistently show that SLAM3R achieves\nstate-of-the-art reconstruction accuracy and completeness while maintaining\nreal-time performance at 20+ FPS. Code and weights at:\nhttps://github.com/PKU-VCL-3DV/SLAM3R.\n","authors":["Yuzheng Liu","Siyan Dong","Shuzhe Wang","Yanchao Yang","Qingnan Fan","Baoquan Chen"],"pdf_url":"https://arxiv.org/pdf/2412.09401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16302v2","updated":"2024-12-19T12:14:03Z","published":"2024-07-23T08:57:11Z","title":"DeepClean: Integrated Distortion Identification and Algorithm Selection\n  for Rectifying Image Corruptions","summary":"  Distortion identification and rectification in images and videos is vital for\nachieving good performance in downstream vision applications. Instead of\nrelying on fixed trial-and-error based image processing pipelines, we propose a\ntwo-level sequential planning approach for automated image distortion\nclassification and rectification. At the higher level it detects the class of\ncorruptions present in the input image, if any. The lower level selects a\nspecific algorithm to be applied, from a set of externally provided candidate\nalgorithms. The entire two-level setup runs in the form of a single forward\npass during inference and it is to be queried iteratively until the retrieval\nof the original image. We demonstrate improvements compared to three baselines\non the object detection task on COCO image dataset with rich set of\ndistortions. The advantage of our approach is its dynamic reconfiguration,\nconditioned on the input image and generalisability to unseen candidate\nalgorithms at inference time, since it relies only on the comparison of their\noutput of the image embeddings.\n","authors":["Aditya Kapoor","Harshad Khadilkar","Jayvardhana Gubbi"],"pdf_url":"https://arxiv.org/pdf/2407.16302v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2412.14168v2","updated":"2024-12-19T11:59:46Z","published":"2024-12-18T18:59:50Z","title":"FashionComposer: Compositional Fashion Image Generation","summary":"  We present FashionComposer for compositional fashion image generation. Unlike\nprevious methods, FashionComposer is highly flexible. It takes multi-modal\ninput (i.e., text prompt, parametric human model, garment image, and face\nimage) and supports personalizing the appearance, pose, and figure of the human\nand assigning multiple garments in one pass. To achieve this, we first develop\na universal framework capable of handling diverse input modalities. We\nconstruct scaled training data to enhance the model's robust compositional\ncapabilities. To accommodate multiple reference images (garments and faces)\nseamlessly, we organize these references in a single image as an \"asset\nlibrary\" and employ a reference UNet to extract appearance features. To inject\nthe appearance features into the correct pixels in the generated result, we\npropose subject-binding attention. It binds the appearance features from\ndifferent \"assets\" with the corresponding text features. In this way, the model\ncould understand each asset according to their semantics, supporting arbitrary\nnumbers and types of reference images. As a comprehensive solution,\nFashionComposer also supports many other applications like human album\ngeneration, diverse virtual try-on tasks, etc.\n","authors":["Sihui Ji","Yiyang Wang","Xi Chen","Xiaogang Xu","Hao Luo","Hengshuang Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14168v2.pdf","comment":"https://sihuiji.github.io/FashionComposer-Page"},{"id":"http://arxiv.org/abs/2412.14768v1","updated":"2024-12-19T11:51:45Z","published":"2024-12-19T11:51:45Z","title":"FLAMe: Federated Learning with Attention Mechanism using Spatio-Temporal\n  Keypoint Transformers for Pedestrian Fall Detection in Smart Cities","summary":"  In smart cities, detecting pedestrian falls is a major challenge to ensure\nthe safety and quality of life of citizens. In this study, we propose a novel\nfall detection system using FLAMe (Federated Learning with Attention\nMechanism), a federated learning (FL) based algorithm. FLAMe trains around\nimportant keypoint information and only transmits the trained important weights\nto the server, reducing communication costs and preserving data privacy.\nFurthermore, the lightweight keypoint transformer model is integrated into the\nFL framework to effectively learn spatio-temporal features. We validated the\nexperiment using 22,672 video samples from the \"Fall Accident Risk Behavior\nVideo-Sensor Pair data\" dataset from AI-Hub. As a result of the experiment, the\nFLAMe-based system achieved an accuracy of 94.02% with about 190,000\ntransmission parameters, maintaining performance similar to that of existing\ncentralized learning while maximizing efficiency by reducing communication\ncosts by about 40% compared to the existing FL algorithm, FedAvg. Therefore,\nthe FLAMe algorithm has demonstrated that it provides robust performance in the\ndistributed environment of smart cities and is a practical and effective\nsolution for public safety.\n","authors":["Byeonghun Kim","Byeongjoon Noh"],"pdf_url":"https://arxiv.org/pdf/2412.14768v1.pdf","comment":"8 pages, 7 figures, AAAI 2025 FLUID Workshop"},{"id":"http://arxiv.org/abs/2408.01812v3","updated":"2024-12-19T11:29:09Z","published":"2024-08-03T15:43:56Z","title":"SkyDiffusion: Ground-to-Aerial Image Synthesis with Diffusion Models and\n  BEV Paradigm","summary":"  Ground-to-aerial image synthesis focuses on generating realistic aerial\nimages from corresponding ground street view images while maintaining\nconsistent content layout, simulating a top-down view. The significant\nviewpoint difference leads to domain gaps between views, and dense urban scenes\nlimit the visible range of street views, making this cross-view generation task\nparticularly challenging. In this paper, we introduce SkyDiffusion, a novel\ncross-view generation method for synthesizing aerial images from street view\nimages, utilizing a diffusion model and the Bird's-Eye View (BEV) paradigm. The\nCurved-BEV method in SkyDiffusion converts street-view images into a BEV\nperspective, effectively bridging the domain gap, and employs a \"multi-to-one\"\nmapping strategy to address occlusion issues in dense urban scenes. Next,\nSkyDiffusion designed a BEV-guided diffusion model to generate\ncontent-consistent and realistic aerial images. Additionally, we introduce a\nnovel dataset, Ground2Aerial-3, designed for diverse ground-to-aerial image\nsynthesis applications, including disaster scene aerial synthesis, historical\nhigh-resolution satellite image synthesis, and low-altitude UAV image synthesis\ntasks. Experimental results demonstrate that SkyDiffusion outperforms\nstate-of-the-art methods on cross-view datasets across natural (CVUSA),\nsuburban (CVACT), urban (VIGOR-Chicago), and various application scenarios\n(G2A-3), achieving realistic and content-consistent aerial image generation.\nMore result and dataset information can be found at\nhttps://opendatalab.github.io/skydiffusion/ .\n","authors":["Junyan Ye","Jun He","Weijia Li","Zhutao Lv","Yi Lin","Jinhua Yu","Haote Yang","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2408.01812v3.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.17981v3","updated":"2024-12-19T11:25:34Z","published":"2024-01-31T16:38:32Z","title":"From Training-Free to Adaptive: Empirical Insights into MLLMs'\n  Understanding of Detection Information","summary":"  Despite the impressive capabilities of Multimodal Large Language Models\n(MLLMs) in integrating text and image modalities, challenges remain in\naccurately interpreting detailed visual elements. Vision detection models excel\nat recognizing fine-grained image details, prompting researchers to use them to\nenhance MLLMs. One effective strategy is to infuse detection information in\ntext format, which has proven simple and effective. However, most studies\nutilize this method without training, leaving the potential of adaptive\ntraining largely unexplored. Adaptive training could significantly enhance\nMLLMs' comprehension of unique inputs while filtering out irrelevant\ninformation. This paper addresses the crucial question: How does training\nimpact MLLMs' understanding of infused textual detection information? We\nsystematically experiment with various representative models to evaluate the\neffects of training-free, retraining, and fine-tuning strategies. We also\nexamine the influence of training on MLLMs' original abilities and the\ninterchangeability of detection models. Our findings indicate that fine-tuning\na pre-trained MLLM to incorporate textual detection information delivers\nsuperior results compared to training-free and retraining methods, improving\nperformance by 6.71% across 10 widely recognized benchmarks. Furthermore,\nfine-tuning enables MLLMs to retain performance enhancements even when\ndetection models are swapped, indicating improved understanding of formatted\ntextual data. We release our codes to support further exploration of fusion\nstrategies for vision detection models and the enhancement of MLLMs'\nfine-grained multimodal capabilities.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2401.17981v3.pdf","comment":"32 pages, 22 tables, 7 figures"},{"id":"http://arxiv.org/abs/2410.23091v5","updated":"2024-12-19T11:18:58Z","published":"2024-10-30T15:06:44Z","title":"CausalDiff: Causality-Inspired Disentanglement via Diffusion Model for\n  Adversarial Defense","summary":"  Despite ongoing efforts to defend neural classifiers from adversarial\nattacks, they remain vulnerable, especially to unseen attacks. In contrast,\nhumans are difficult to be cheated by subtle manipulations, since we make\njudgments only based on essential factors. Inspired by this observation, we\nattempt to model label generation with essential label-causative factors and\nincorporate label-non-causative factors to assist data generation. For an\nadversarial example, we aim to discriminate the perturbations as non-causative\nfactors and make predictions only based on the label-causative factors.\nConcretely, we propose a casual diffusion model (CausalDiff) that adapts\ndiffusion models for conditional data generation and disentangles the two types\nof casual factors by learning towards a novel casual information bottleneck\nobjective. Empirically, CausalDiff has significantly outperformed\nstate-of-the-art defense methods on various unseen attacks, achieving an\naverage robustness of 86.39% (+4.01%) on CIFAR-10, 56.25% (+3.13%) on\nCIFAR-100, and 82.62% (+4.93%) on GTSRB (German Traffic Sign Recognition\nBenchmark). The code is available at\nhttps://github.com/CAS-AISafetyBasicResearchGroup/CausalDiff\n","authors":["Mingkun Zhang","Keping Bi","Wei Chen","Quanrun Chen","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.23091v5.pdf","comment":"accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2409.11383v2","updated":"2024-12-19T11:12:30Z","published":"2024-09-17T17:34:24Z","title":"Training Datasets Generation for Machine Learning: Application to Vision\n  Based Navigation","summary":"  Vision Based Navigation consists in utilizing cameras as precision sensors\nfor GNC after extracting information from images. To enable the adoption of\nmachine learning for space applications, one of obstacles is the demonstration\nthat available training datasets are adequate to validate the algorithms. The\nobjective of the study is to generate datasets of images and metadata suitable\nfor training machine learning algorithms. Two use cases were selected and a\nrobust methodology was developed to validate the datasets including the ground\ntruth. The first use case is in-orbit rendezvous with a man-made object: a\nmockup of satellite ENVISAT. The second use case is a Lunar landing scenario.\nDatasets were produced from archival datasets (Chang'e 3), from the laboratory\nat DLR TRON facility and at Airbus Robotic laboratory, from SurRender software\nhigh fidelity image simulator using Model Capture and from Generative\nAdversarial Networks. The use case definition included the selection of\nalgorithms as benchmark: an AI-based pose estimation algorithm and a dense\noptical flow algorithm were selected. Eventually it is demonstrated that\ndatasets produced with SurRender and selected laboratory facilities are\nadequate to train machine learning algorithms.\n","authors":["Jérémy Lebreton","Ingo Ahrns","Roland Brochard","Christoph Haskamp","Hans Krüger","Matthieu Le Goff","Nicolas Menga","Nicolas Ollagnier","Ralf Regele","Francesco Capolupo","Massimo Casasco"],"pdf_url":"https://arxiv.org/pdf/2409.11383v2.pdf","comment":"6 pages, 4 figures, preprint of the proceedings of ESA SPAICE\n  conference 2024"},{"id":"http://arxiv.org/abs/2408.04594v3","updated":"2024-12-19T11:04:20Z","published":"2024-08-08T17:10:16Z","title":"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models","summary":"  High-performance Multimodal Large Language Models (MLLMs) are heavily\ndependent on data quality. To advance fine-grained image recognition within\nMLLMs, we introduce a novel data synthesis method inspired by contrastive\nlearning and image difference captioning. Our key idea involves challenging the\nmodel to discern both matching and distinct elements by scrutinizing object\ndifferences in detailed regions across similar images. We begin by generating\npairs of similar images that emphasize object variations. Following this, we\nemploy a Difference Area Generator to pinpoint object differences, and\nsubsequently, a Difference Captions Generator to articulate these differences.\nThis process results in a high-quality dataset of \"object replacement\" samples,\ntermed Img-Diff, which can be scaled as needed due to its automated nature. We\nleverage this generated dataset to fine-tune state-of-the-art (SOTA) MLLMs,\nsuch as InternVL2, achieving substantial improvements across various image\ndifference and Visual Question Answering tasks. Notably, the trained models\nsignificantly outperform existing SOTA models like GPT-4V and Gemini on the\nMMVP benchmark. Additionally, we conduct comprehensive evaluations to validate\nthe dataset's diversity, quality, and robustness, offering several insights\ninto the synthesis of such contrastive datasets. We release our codes and\ndataset to encourage further research on multimodal data synthesis and MLLMs'\nfundamental capabilities for image understanding.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Bolin Ding","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04594v3.pdf","comment":"22 pages, 10 figures, 16 tables"},{"id":"http://arxiv.org/abs/2412.10681v2","updated":"2024-12-19T10:59:24Z","published":"2024-12-14T05:01:46Z","title":"One Pixel is All I Need","summary":"  Vision Transformers (ViTs) have achieved record-breaking performance in\nvarious visual tasks. However, concerns about their robustness against backdoor\nattacks have grown. Backdoor attacks involve associating a specific trigger\nwith a target label, causing the model to predict the attacker-specified label\nwhen the trigger is present, while correctly identifying clean images.We found\nthat ViTs exhibit higher attack success rates for quasi-triggers(patterns\ndifferent from but similar to the original training triggers)compared to CNNs.\nMoreover, some backdoor features in clean samples can suppress the original\ntrigger, making quasi-triggers more effective.To better understand and exploit\nthese vulnerabilities, we developed a tool called the Perturbation Sensitivity\nDistribution Map (PSDM). PSDM computes and sums gradients over many inputs to\nshow how sensitive the model is to small changes in the input. In ViTs, PSDM\nreveals a patch-like pattern where central pixels are more sensitive than\nedges. We use PSDM to guide the creation of quasi-triggers.Based on these\nfindings, we designed \"WorstVIT,\" a simple yet effective data poisoning\nbackdoor for ViT models. This attack requires an extremely low poisoning rate,\ntrains for just one epoch, and modifies a single pixel to successfully attack\nall validation images.\n","authors":["Deng Siqin","Zhou Xiaoyi"],"pdf_url":"https://arxiv.org/pdf/2412.10681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.16729v3","updated":"2024-12-19T10:53:48Z","published":"2024-08-29T17:20:59Z","title":"Prediction-Feedback DETR for Temporal Action Detection","summary":"  Temporal Action Detection (TAD) is fundamental yet challenging for real-world\nvideo applications. Leveraging the unique benefits of transformers, various\nDETR-based approaches have been adopted in TAD. However, it has recently been\nidentified that the attention collapse in self-attention causes the performance\ndegradation of DETR for TAD. Building upon previous research, this paper newly\naddresses the attention collapse problem in cross-attention within DETR-based\nTAD methods. Moreover, our findings reveal that cross-attention exhibits\npatterns distinct from predictions, indicating a short-cut phenomenon. To\nresolve this, we propose a new framework, Prediction-Feedback DETR (Pred-DETR),\nwhich utilizes predictions to restore the collapse and align the cross- and\nself-attention with predictions. Specifically, we devise novel\nprediction-feedback objectives using guidance from the relations of the\npredictions. As a result, Pred-DETR significantly alleviates the collapse and\nachieves state-of-the-art performance among DETR-based methods on various\nchallenging benchmarks including THUMOS14, ActivityNet-v1.3, HACS, and\nFineAction.\n","authors":["Jihwan Kim","Miso Lee","Cheol-Ho Cho","Jihyun Lee","Jae-Pil Heo"],"pdf_url":"https://arxiv.org/pdf/2408.16729v3.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2406.02507v3","updated":"2024-12-19T10:43:11Z","published":"2024-06-04T17:25:59Z","title":"Guiding a Diffusion Model with a Bad Version of Itself","summary":"  The primary axes of interest in image-generating diffusion models are image\nquality, the amount of variation in the results, and how well the results align\nwith a given condition, e.g., a class label or a text prompt. The popular\nclassifier-free guidance approach uses an unconditional model to guide a\nconditional model, leading to simultaneously better prompt alignment and\nhigher-quality images at the cost of reduced variation. These effects seem\ninherently entangled, and thus hard to control. We make the surprising\nobservation that it is possible to obtain disentangled control over image\nquality without compromising the amount of variation by guiding generation\nusing a smaller, less-trained version of the model itself rather than an\nunconditional model. This leads to significant improvements in ImageNet\ngeneration, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using\npublicly available networks. Furthermore, the method is also applicable to\nunconditional diffusion models, drastically improving their quality.\n","authors":["Tero Karras","Miika Aittala","Tuomas Kynkäänniemi","Jaakko Lehtinen","Timo Aila","Samuli Laine"],"pdf_url":"https://arxiv.org/pdf/2406.02507v3.pdf","comment":"NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14719v1","updated":"2024-12-19T10:41:24Z","published":"2024-12-19T10:41:24Z","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","summary":"  Micro-Action Recognition (MAR) has gained increasing attention due to its\ncrucial role as a form of non-verbal communication in social interactions, with\npromising potential for applications in human communication and emotion\nanalysis. However, current approaches often overlook the inherent ambiguity in\nmicro-actions, which arises from the wide category range and subtle visual\ndifferences between categories. This oversight hampers the accuracy of\nmicro-action recognition. In this paper, we propose a novel Prototypical\nCalibrating Ambiguous Network (\\textbf{PCAN}) to unleash and mitigate the\nambiguity of MAR. \\textbf{Firstly}, we employ a hierarchical action-tree to\nidentify the ambiguous sample, categorizing them into distinct sets of\nambiguous samples of false negatives and false positives, considering both\nbody- and action-level categories. \\textbf{Secondly}, we implement an ambiguous\ncontrastive refinement module to calibrate these ambiguous samples by\nregulating the distance between ambiguous samples and their corresponding\nprototypes. This calibration process aims to pull false negative\n($\\mathbb{FN}$) samples closer to their respective prototypes and push false\npositive ($\\mathbb{FP}$) samples apart from their affiliated prototypes. In\naddition, we propose a new prototypical diversity amplification loss to\nstrengthen the model's capacity by amplifying the differences between different\nprototypes. \\textbf{Finally}, we propose a prototype-guided rectification to\nrectify prediction by incorporating the representability of prototypes.\nExtensive experiments conducted on the benchmark dataset demonstrate the\nsuperior performance of our method compared to existing approaches. The code is\navailable at https://github.com/kunli-cs/PCAN.\n","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14719v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14706v1","updated":"2024-12-19T10:19:43Z","published":"2024-12-19T10:19:43Z","title":"EnergyMoGen: Compositional Human Motion Generation with Energy-Based\n  Diffusion Model in Latent Space","summary":"  Diffusion models, particularly latent diffusion models, have demonstrated\nremarkable success in text-driven human motion generation. However, it remains\nchallenging for latent diffusion models to effectively compose multiple\nsemantic concepts into a single, coherent motion sequence. To address this\nissue, we propose EnergyMoGen, which includes two spectrums of Energy-Based\nModels: (1) We interpret the diffusion model as a latent-aware energy-based\nmodel that generates motions by composing a set of diffusion models in latent\nspace; (2) We introduce a semantic-aware energy model based on cross-attention,\nwhich enables semantic composition and adaptive gradient descent for text\nembeddings. To overcome the challenges of semantic inconsistency and motion\ndistortion across these two spectrums, we introduce Synergistic Energy Fusion.\nThis design allows the motion latent diffusion model to synthesize\nhigh-quality, complex motions by combining multiple energy terms corresponding\nto textual descriptions. Experiments show that our approach outperforms\nexisting state-of-the-art models on various motion generation tasks, including\ntext-to-motion generation, compositional motion generation, and multi-concept\nmotion generation. Additionally, we demonstrate that our method can be used to\nextend motion datasets and improve the text-to-motion task.\n","authors":["Jianrong Zhang","Hehe Fan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2412.14706v1.pdf","comment":"Project page: https://jiro-zhang.github.io/EnergyMoGen/"},{"id":"http://arxiv.org/abs/2412.14705v1","updated":"2024-12-19T10:17:50Z","published":"2024-12-19T10:17:50Z","title":"Event-assisted 12-stop HDR Imaging of Dynamic Scene","summary":"  High dynamic range (HDR) imaging is a crucial task in computational\nphotography, which captures details across diverse lighting conditions.\nTraditional HDR fusion methods face limitations in dynamic scenes with extreme\nexposure differences, as aligning low dynamic range (LDR) frames becomes\nchallenging due to motion and brightness variation. In this work, we propose a\nnovel 12-stop HDR imaging approach for dynamic scenes, leveraging a dual-camera\nsystem with an event camera and an RGB camera. The event camera provides\ntemporally dense, high dynamic range signals that improve alignment between LDR\nframes with large exposure differences, reducing ghosting artifacts caused by\nmotion. Also, a real-world finetuning strategy is proposed to increase the\ngeneralization of alignment module on real-world events. Additionally, we\nintroduce a diffusion-based fusion module that incorporates image priors from\npre-trained diffusion models to address artifacts in high-contrast regions and\nminimize errors from the alignment process. To support this work, we developed\nthe ESHDR dataset, the first dataset for 12-stop HDR imaging with synchronized\nevent signals, and validated our approach on both simulated and real-world\ndata. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, successfully extending HDR imaging to 12 stops in\ndynamic scenes.\n","authors":["Shi Guo","Zixuan Chen","Ziran Zhang","Yutian Chen","Gangwei Xu","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2412.14705v1.pdf","comment":"Project page:\n  https://openimaginglab.github.io/Event-Assisted-12stops-HDR/"},{"id":"http://arxiv.org/abs/2410.17098v2","updated":"2024-12-19T10:03:18Z","published":"2024-10-22T15:22:53Z","title":"Activity Recognition on Avatar-Anonymized Datasets with Masked\n  Differential Privacy","summary":"  Privacy-preserving computer vision is an important emerging problem in\nmachine learning and artificial intelligence. Prevalent methods tackling this\nproblem use differential privacy (DP) or obfuscation techniques to protect the\nprivacy of individuals. In both cases, the utility of the trained model is\nsacrificed heavily in this process. In this work, we present an anonymization\npipeline that replaces sensitive human subjects in video datasets with\nsynthetic avatars within context, employing a combined rendering and stable\ndiffusion-based strategy. Additionally we propose masked differential privacy\n({MaskDP}) to protect non-anonymized but privacy sensitive background\ninformation. MaskDP allows for controlling sensitive regions where differential\nprivacy is applied, in contrast to applying DP on the entire input. This\ncombined methodology provides strong privacy protection while minimizing the\nusual performance penalty of privacy preserving methods. Experiments on\nmultiple challenging action recognition datasets demonstrate that our proposed\ntechniques result in better utility-privacy trade-offs compared to standard\ndifferentially private training in the especially demanding $\\epsilon<1$\nregime.\n","authors":["David Schneider","Sina Sajadmanesh","Vikash Sehwag","Saquib Sarfraz","Rainer Stiefelhagen","Lingjuan Lyu","Vivek Sharma"],"pdf_url":"https://arxiv.org/pdf/2410.17098v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14692v1","updated":"2024-12-19T09:51:45Z","published":"2024-12-19T09:51:45Z","title":"Explicit Relational Reasoning Network for Scene Text Detection","summary":"  Connected component (CC) is a proper text shape representation that aligns\nwith human reading intuition. However, CC-based text detection methods have\nrecently faced a developmental bottleneck that their time-consuming\npost-processing is difficult to eliminate. To address this issue, we introduce\nan explicit relational reasoning network (ERRNet) to elegantly model the\ncomponent relationships without post-processing. Concretely, we first represent\neach text instance as multiple ordered text components, and then treat these\ncomponents as objects in sequential movement. In this way, scene text detection\ncan be innovatively viewed as a tracking problem. From this perspective, we\ndesign an end-to-end tracking decoder to achieve a CC-based method dispensing\nwith post-processing entirely. Additionally, we observe that there is an\ninconsistency between classification confidence and localization quality, so we\npropose a Polygon Monte-Carlo method to quickly and accurately evaluate the\nlocalization quality. Based on this, we introduce a position-supervised\nclassification loss to guide the task-aligned learning of ERRNet. Experiments\non challenging benchmarks demonstrate the effectiveness of our ERRNet. It\nconsistently achieves state-of-the-art accuracy while holding highly\ncompetitive inference speed.\n","authors":["Yuchen Su","Zhineng Chen","Yongkun Du","Zhilong Ji","Kai Hu","Jinfeng Bai","Xieping Gao"],"pdf_url":"https://arxiv.org/pdf/2412.14692v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14680v1","updated":"2024-12-19T09:32:53Z","published":"2024-12-19T09:32:53Z","title":"A Light-Weight Framework for Open-Set Object Detection with Decoupled\n  Feature Alignment in Joint Space","summary":"  Open-set object detection (OSOD) is highly desirable for robotic manipulation\nin unstructured environments. However, existing OSOD methods often fail to meet\nthe requirements of robotic applications due to their high computational burden\nand complex deployment. To address this issue, this paper proposes a\nlight-weight framework called Decoupled OSOD (DOSOD), which is a practical and\nhighly efficient solution to support real-time OSOD tasks in robotic systems.\nSpecifically, DOSOD builds upon the YOLO-World pipeline by integrating a\nvision-language model (VLM) with a detector. A Multilayer Perceptron (MLP)\nadaptor is developed to transform text embeddings extracted by the VLM into a\njoint space, within which the detector learns the region representations of\nclass-agnostic proposals. Cross-modality features are directly aligned in the\njoint space, avoiding the complex feature interactions and thereby improving\ncomputational efficiency. DOSOD operates like a traditional closed-set detector\nduring the testing phase, effectively bridging the gap between closed-set and\nopen-set detection. Compared to the baseline YOLO-World, the proposed DOSOD\nsignificantly enhances real-time performance while maintaining comparable\naccuracy. The slight DOSOD-S model achieves a Fixed AP of $26.7\\%$, compared to\n$26.2\\%$ for YOLO-World-v1-S and $22.7\\%$ for YOLO-World-v2-S, using similar\nbackbones on the LVIS minival dataset. Meanwhile, the FPS of DOSOD-S is\n$57.1\\%$ higher than YOLO-World-v1-S and $29.6\\%$ higher than YOLO-World-v2-S.\nMeanwhile, we demonstrate that the DOSOD model facilitates the deployment of\nedge devices. The codes and models are publicly available at\nhttps://github.com/D-Robotics-AI-Lab/DOSOD.\n","authors":["Yonghao He","Hu Su","Haiyong Yu","Cong Yang","Wei Sui","Cong Wang","Song Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14678v1","updated":"2024-12-19T09:31:53Z","published":"2024-12-19T09:31:53Z","title":"Efficient Few-Shot Neural Architecture Search by Counting the Number of\n  Nonlinear Functions","summary":"  Neural architecture search (NAS) enables finding the best-performing\narchitecture from a search space automatically. Most NAS methods exploit an\nover-parameterized network (i.e., a supernet) containing all possible\narchitectures (i.e., subnets) in the search space. However, the subnets that\nshare the same set of parameters are likely to have different characteristics,\ninterfering with each other during training. To address this, few-shot NAS\nmethods have been proposed that divide the space into a few subspaces and\nemploy a separate supernet for each subspace to limit the extent of weight\nsharing. They achieve state-of-the-art performance, but the computational cost\nincreases accordingly. We introduce in this paper a novel few-shot NAS method\nthat exploits the number of nonlinear functions to split the search space. To\nbe specific, our method divides the space such that each subspace consists of\nsubnets with the same number of nonlinear functions. Our splitting criterion is\nefficient, since it does not require comparing gradients of a supernet to split\nthe space. In addition, we have found that dividing the space allows us to\nreduce the channel dimensions required for each supernet, which enables\ntraining multiple supernets in an efficient manner. We also introduce a\nsupernet-balanced sampling (SBS) technique, sampling several subnets at each\ntraining step, to train different supernets evenly within a limited number of\ntraining steps. Extensive experiments on standard NAS benchmarks demonstrate\nthe effectiveness of our approach. Our code is available at\nhttps://cvlab.yonsei.ac.kr/projects/EFS-NAS.\n","authors":["Youngmin Oh","Hyunju Lee","Bumsub Ham"],"pdf_url":"https://arxiv.org/pdf/2412.14678v1.pdf","comment":"Accepted to AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14672v1","updated":"2024-12-19T09:24:10Z","published":"2024-12-19T09:24:10Z","title":"FiVL: A Framework for Improved Vision-Language Alignment","summary":"  Large Vision Language Models (LVLMs) have achieved significant progress in\nintegrating visual and textual inputs for multimodal reasoning. However, a\nrecurring challenge is ensuring these models utilize visual information as\neffectively as linguistic content when both modalities are necessary to\nformulate an accurate answer. We hypothesize that hallucinations arise due to\nthe lack of effective visual grounding in current LVLMs. This issue extends to\nvision-language benchmarks, where it is difficult to make the image\nindispensable for accurate answer generation, particularly in vision\nquestion-answering tasks. In this work, we introduce FiVL, a novel method for\nconstructing datasets designed to train LVLMs for enhanced visual grounding and\nto evaluate their effectiveness in achieving it. These datasets can be utilized\nfor both training and assessing an LVLM's ability to use image content as\nsubstantive evidence rather than relying solely on linguistic priors, providing\ninsights into the model's reliance on visual information. To demonstrate the\nutility of our dataset, we introduce an innovative training task that\noutperforms baselines alongside a validation method and application for\nexplainability. The code is available at https://github.com/IntelLabs/fivl.\n","authors":["Estelle Aflalo","Gabriela Ben Melech Stan","Tiep Le","Man Luo","Shachar Rosenman","Sayak Paul","Shao-Yen Tseng","Vasudev Lal"],"pdf_url":"https://arxiv.org/pdf/2412.14672v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14671v1","updated":"2024-12-19T09:22:19Z","published":"2024-12-19T09:22:19Z","title":"MUSTER: Longitudinal Deformable Registration by Composition of\n  Consecutive Deformations","summary":"  Longitudinal imaging allows for the study of structural changes over time.\nOne approach to detecting such changes is by non-linear image registration.\nThis study introduces Multi-Session Temporal Registration (MUSTER), a novel\nmethod that facilitates longitudinal analysis of changes in extended series of\nmedical images. MUSTER improves upon conventional pairwise registration by\nincorporating more than two imaging sessions to recover longitudinal\ndeformations. Longitudinal analysis at a voxel-level is challenging due to\neffects of a changing image contrast as well as instrumental and environmental\nsources of bias between sessions. We show that local normalized\ncross-correlation as an image similarity metric leads to biased results and\npropose a robust alternative. We test the performance of MUSTER on a synthetic\nmulti-site, multi-session neuroimaging dataset and show that, in various\nscenarios, using MUSTER significantly enhances the estimated deformations\nrelative to pairwise registration. Additionally, we apply MUSTER on a sample of\nolder adults from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.\nThe results show that MUSTER can effectively identify patterns of\nneuro-degeneration from T1-weighted images and that these changes correlate\nwith changes in cognition, matching the performance of state of the art\nsegmentation methods. By leveraging GPU acceleration, MUSTER efficiently\nhandles large datasets, making it feasible also in situations with limited\ncomputational resources.\n","authors":["Edvard O. S. Grødem","Donatas Sederevičius","Esten H. Leonardsen","Bradley J. MacIntosh","Atle Bjørnerud","Till Schellhorn","Øystein Sørensen","Inge Amlien","Pablo F. Garrido","Anders M. Fjell"],"pdf_url":"https://arxiv.org/pdf/2412.14671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01220v3","updated":"2024-12-19T09:16:19Z","published":"2024-07-01T12:07:26Z","title":"Fast and Efficient: Mask Neural Fields for 3D Scene Segmentation","summary":"  Understanding 3D scenes is a crucial challenge in computer vision research\nwith applications spanning multiple domains. Recent advancements in distilling\n2D vision-language foundation models into neural fields, like NeRF and 3DGS,\nenable open-vocabulary segmentation of 3D scenes from 2D multi-view images\nwithout the need for precise 3D annotations. However, while effective, these\nmethods typically rely on the per-pixel distillation of high-dimensional CLIP\nfeatures, introducing ambiguity and necessitating complex regularization\nstrategies, which adds inefficiency during training. This paper presents\nMaskField, which enables efficient 3D open-vocabulary segmentation with neural\nfields from a novel perspective. Unlike previous methods, MaskField decomposes\nthe distillation of mask and semantic features from foundation models by\nformulating a mask feature field and queries. MaskField overcomes ambiguous\nobject boundaries by naturally introducing SAM segmented object shapes without\nextra regularization during training. By circumventing the direct handling of\ndense high-dimensional CLIP features during training, MaskField is particularly\ncompatible with explicit scene representations like 3DGS. Our extensive\nexperiments show that MaskField not only surpasses prior state-of-the-art\nmethods but also achieves remarkably fast convergence. We hope that MaskField\nwill inspire further exploration into how neural fields can be trained to\ncomprehend 3D scenes from 2D models.\n","authors":["Zihan Gao","Lingling Li","Licheng Jiao","Fang Liu","Xu Liu","Wenping Ma","Yuwei Guo","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2407.01220v3.pdf","comment":"15 pages, 9 figures, Code:https://github.com/keloee/MaskField"},{"id":"http://arxiv.org/abs/2412.14660v1","updated":"2024-12-19T09:10:07Z","published":"2024-12-19T09:10:07Z","title":"Unveiling Uncertainty: A Deep Dive into Calibration and Performance of\n  Multimodal Large Language Models","summary":"  Multimodal large language models (MLLMs) combine visual and textual data for\ntasks such as image captioning and visual question answering. Proper\nuncertainty calibration is crucial, yet challenging, for reliable use in areas\nlike healthcare and autonomous driving. This paper investigates representative\nMLLMs, focusing on their calibration across various scenarios, including before\nand after visual fine-tuning, as well as before and after multimodal training\nof the base LLMs. We observed miscalibration in their performance, and at the\nsame time, no significant differences in calibration across these scenarios. We\nalso highlight how uncertainty differs between text and images and how their\nintegration affects overall uncertainty. To better understand MLLMs'\nmiscalibration and their ability to self-assess uncertainty, we construct the\nIDK (I don't know) dataset, which is key to evaluating how they handle\nunknowns. Our findings reveal that MLLMs tend to give answers rather than admit\nuncertainty, but this self-assessment improves with proper prompt adjustments.\nFinally, to calibrate MLLMs and enhance model reliability, we propose\ntechniques such as temperature scaling and iterative prompt optimization. Our\nresults provide insights into improving MLLMs for effective and responsible\ndeployment in multimodal applications. Code and IDK dataset:\n\\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.\n","authors":["Zijun Chen","Wenbo Hu","Guande He","Zhijie Deng","Zheng Zhang","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2412.14660v1.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2403.15031v2","updated":"2024-12-19T09:00:34Z","published":"2024-03-22T08:26:31Z","title":"Image Classification with Rotation-Invariant Variational Quantum\n  Circuits","summary":"  Variational quantum algorithms are gaining attention as an early application\nof Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of\nvariational methods lies in the phenomenon of Barren Plateaus, present in the\noptimization of variational parameters. Adding geometric inductive bias to the\nquantum models has been proposed as a potential solution to mitigate this\nproblem, leading to a new field called Geometric Quantum Machine Learning. In\nthis work, an equivariant architecture for variational quantum classifiers is\nintroduced to create a label-invariant model for image classification with\n$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against\ntwo different architectures, and it is experimentally observed that the\ngeometric approach boosts the model's performance. Finally, a classical\nequivariant convolution operation is proposed to extend the quantum model for\nthe processing of larger images, employing the resources available in NISQ\ndevices.\n","authors":["Paul San Sebastian","Mikel Cañizo","Román Orús"],"pdf_url":"https://arxiv.org/pdf/2403.15031v2.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.00143v2","updated":"2024-12-19T08:58:15Z","published":"2024-05-31T19:13:09Z","title":"Diversifying Query: Region-Guided Transformer for Temporal Sentence\n  Grounding","summary":"  Temporal sentence grounding is a challenging task that aims to localize the\nmoment spans relevant to a language description. Although recent DETR-based\nmodels have achieved notable progress by leveraging multiple learnable moment\nqueries, they suffer from overlapped and redundant proposals, leading to\ninaccurate predictions. We attribute this limitation to the lack of\ntask-related guidance for the learnable queries to serve a specific mode.\nFurthermore, the complex solution space generated by variable and\nopen-vocabulary language descriptions complicates optimization, making it\nharder for learnable queries to distinguish each other adaptively. To tackle\nthis limitation, we present a Region-Guided TRansformer (RGTR) for temporal\nsentence grounding, which diversifies moment queries to eliminate overlapped\nand redundant predictions. Instead of using learnable queries, RGTR adopts a\nset of anchor pairs as moment queries to introduce explicit regional guidance.\nEach anchor pair takes charge of moment prediction for a specific temporal\nregion, which reduces the optimization difficulty and ensures the diversity of\nthe final predictions. In addition, we design an IoU-aware scoring head to\nimprove proposal quality. Extensive experiments demonstrate the effectiveness\nof RGTR, outperforming state-of-the-art methods on QVHighlights, Charades-STA\nand TACoS datasets. Codes are available at https://github.com/TensorsSun/RGTR\n","authors":["Xiaolong Sun","Liushuai Shi","Le Wang","Sanping Zhou","Kun Xia","Yabing Wang","Gang Hua"],"pdf_url":"https://arxiv.org/pdf/2406.00143v2.pdf","comment":"Accepted by AAAI-25. Code is available at\n  https://github.com/TensorsSun/RGTR"},{"id":"http://arxiv.org/abs/2412.11953v2","updated":"2024-12-19T08:52:56Z","published":"2024-12-16T16:37:03Z","title":"Reliable Breast Cancer Molecular Subtype Prediction based on\n  uncertainty-aware Bayesian Deep Learning by Mammography","summary":"  Breast cancer is a heterogeneous disease with different molecular subtypes,\nclinical behavior, treatment responses as well as survival outcomes. The\ndevelopment of a reliable, accurate, available and inexpensive method to\npredict the molecular subtypes using medical images plays an important role in\nthe diagnosis and prognosis of breast cancer. Recently, deep learning methods\nhave shown good performance in the breast cancer classification tasks using\nvarious medical images. Despite all that success, classical deep learning\ncannot deliver the predictive uncertainty. The uncertainty represents the\nvalidity of the predictions. Therefore, the high predicted uncertainty might\ncause a negative effect in the accurate diagnosis of breast cancer molecular\nsubtypes. To overcome this, uncertainty quantification methods are used to\ndetermine the predictive uncertainty. Accordingly, in this study, we proposed\nan uncertainty-aware Bayesian deep learning model using the full mammogram\nimages. In addition, to increase the performance of the multi-class molecular\nsubtype classification task, we proposed a novel hierarchical classification\nstrategy, named the two-stage classification strategy. The separate AUC of the\nproposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched,\nluminal and triple-negative classes, respectively. The proposed model not only\nhas a comparable performance to other studies in the field of breast cancer\nmolecular subtypes prediction, even using full mammography images, but it is\nalso more reliable, due to quantify the predictive uncertainty.\n","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"pdf_url":"https://arxiv.org/pdf/2412.11953v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14643v1","updated":"2024-12-19T08:51:57Z","published":"2024-12-19T08:51:57Z","title":"RefHCM: A Unified Model for Referring Perceptions in Human-Centric\n  Scenarios","summary":"  Human-centric perceptions play a crucial role in real-world applications.\nWhile recent human-centric works have achieved impressive progress, these\nefforts are often constrained to the visual domain and lack interaction with\nhuman instructions, limiting their applicability in broader scenarios such as\nchatbots and sports analysis. This paper introduces Referring Human\nPerceptions, where a referring prompt specifies the person of interest in an\nimage. To tackle the new task, we propose RefHCM (Referring Human-Centric\nModel), a unified framework to integrate a wide range of human-centric\nreferring tasks. Specifically, RefHCM employs sequence mergers to convert raw\nmultimodal data -- including images, text, coordinates, and parsing maps --\ninto semantic tokens. This standardized representation enables RefHCM to\nreformulate diverse human-centric referring tasks into a sequence-to-sequence\nparadigm, solved using a plain encoder-decoder transformer architecture.\nBenefiting from a unified learning strategy, RefHCM effectively facilitates\nknowledge transfer across tasks and exhibits unforeseen capabilities in\nhandling complex reasoning. This work represents the first attempt to address\nreferring human perceptions with a general-purpose framework, while\nsimultaneously establishing a corresponding benchmark that sets new standards\nfor the field. Extensive experiments showcase RefHCM's competitive and even\nsuperior performance across multiple human-centric referring tasks. The code\nand data are publicly at https://github.com/JJJYmmm/RefHCM.\n","authors":["Jie Huang","Ruibing Hou","Jiahe Zhao","Hong Chang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2412.14643v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2412.14640v1","updated":"2024-12-19T08:51:01Z","published":"2024-12-19T08:51:01Z","title":"Adaptive Prompt Tuning: Vision Guided Prompt Tuning with Cross-Attention\n  for Fine-Grained Few-Shot Learning","summary":"  Few-shot, fine-grained classification in computer vision poses significant\nchallenges due to the need to differentiate subtle class distinctions with\nlimited data. This paper presents a novel method that enhances the Contrastive\nLanguage-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided\nby real-time visual inputs. Unlike existing techniques such as Context\nOptimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by\nstatic prompts or visual token reliance, the proposed approach leverages a\ncross-attention mechanism to dynamically refine text prompts for the image at\nhand. This enables an image-specific alignment of textual features with image\npatches extracted from the Vision Transformer, making the model more effective\nfor datasets with high intra-class variance and low inter-class differences.\nThe method is evaluated on several datasets, including CUBirds, Oxford Flowers,\nand FGVC Aircraft, showing significant performance gains over static prompt\ntuning approaches. To ensure these performance gains translate into trustworthy\npredictions, we integrate Monte-Carlo Dropout in our approach to improve the\nreliability of the model predictions and uncertainty estimates. This\nintegration provides valuable insights into the model's predictive confidence,\nhelping to identify when predictions can be trusted and when additional\nverification is necessary. This dynamic approach offers a robust solution,\nadvancing the state-of-the-art for few-shot fine-grained classification.\n","authors":["Eric Brouwer","Jan Erik van Woerden","Gertjan Burghouts","Matias Valedenegro-Toro","Marco Zullich"],"pdf_url":"https://arxiv.org/pdf/2412.14640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18459v3","updated":"2024-12-19T08:47:07Z","published":"2024-04-29T06:35:34Z","title":"Chameleon: A Data-Efficient Generalist for Dense Visual Prediction in\n  the Wild","summary":"  Large language models have evolved data-efficient generalists, benefiting\nfrom the universal language interface and large-scale pre-training. However,\nconstructing a data-efficient generalist for dense visual prediction presents a\ndistinct challenge due to the variation in label structures across different\ntasks. Consequently, generalization to unseen dense prediction tasks in the\nlow-data regime is not straightforward and has received less attention from\nprevious vision generalists. In this study, we explore a universal model that\ncan flexibly adapt to unseen dense label structures with a few examples,\nenabling it to serve as a data-efficient vision generalist in diverse\nreal-world scenarios. To this end, we base our method on a powerful\nmeta-learning framework and explore several axes to improve its performance and\nversatility for real-world problems, such as flexible adaptation mechanisms and\nscalability. We evaluate our model across a spectrum of unseen real-world\nscenarios where low-shot learning is desirable, including video, 3D, medical,\nbiological, and user-interactive tasks. Equipped with a generic architecture\nand an effective adaptation mechanism, our model flexibly adapts to all of\nthese tasks with at most 50 labeled images, showcasing a significant\nadvancement over existing data-efficient generalist approaches. Codes are\navailable at https://github.com/GitGyun/chameleon.\n","authors":["Donggyun Kim","Seongwoong Cho","Semin Kim","Chong Luo","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2404.18459v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12974v3","updated":"2024-12-19T08:41:19Z","published":"2024-12-17T14:56:59Z","title":"Attentive Eraser: Unleashing Diffusion Model's Object Removal Potential\n  via Self-Attention Redirection Guidance","summary":"  Recently, diffusion models have emerged as promising newcomers in the field\nof generative models, shining brightly in image generation. However, when\nemployed for object removal tasks, they still encounter issues such as\ngenerating random artifacts and the incapacity to repaint foreground object\nareas with appropriate content after removal. To tackle these problems, we\npropose Attentive Eraser, a tuning-free method to empower pre-trained diffusion\nmodels for stable and effective object removal. Firstly, in light of the\nobservation that the self-attention maps influence the structure and shape\ndetails of the generated images, we propose Attention Activation and\nSuppression (ASS), which re-engineers the self-attention mechanism within the\npre-trained diffusion models based on the given mask, thereby prioritizing the\nbackground over the foreground object during the reverse generation process.\nMoreover, we introduce Self-Attention Redirection Guidance (SARG), which\nutilizes the self-attention redirected by ASS to guide the generation process,\neffectively removing foreground objects within the mask while simultaneously\ngenerating content that is both plausible and coherent. Experiments demonstrate\nthe stability and effectiveness of Attentive Eraser in object removal across a\nvariety of pre-trained diffusion models, outperforming even training-based\nmethods. Furthermore, Attentive Eraser can be implemented in various diffusion\nmodel architectures and checkpoints, enabling excellent scalability. Code is\navailable at https://github.com/Anonym0u3/AttentiveEraser.\n","authors":["Wenhao Sun","Benlei Cui","Xue-Mei Dong","Jingqun Tang"],"pdf_url":"https://arxiv.org/pdf/2412.12974v3.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14633v1","updated":"2024-12-19T08:38:59Z","published":"2024-12-19T08:38:59Z","title":"Progressive Fine-to-Coarse Reconstruction for Accurate Low-Bit\n  Post-Training Quantization in Vision Transformers","summary":"  Due to its efficiency, Post-Training Quantization (PTQ) has been widely\nadopted for compressing Vision Transformers (ViTs). However, when quantized\ninto low-bit representations, there is often a significant performance drop\ncompared to their full-precision counterparts. To address this issue,\nreconstruction methods have been incorporated into the PTQ framework to improve\nperformance in low-bit quantization settings. Nevertheless, existing related\nmethods predefine the reconstruction granularity and seldom explore the\nprogressive relationships between different reconstruction granularities, which\nleads to sub-optimal quantization results in ViTs. To this end, in this paper,\nwe propose a Progressive Fine-to-Coarse Reconstruction (PFCR) method for\naccurate PTQ, which significantly improves the performance of low-bit quantized\nvision transformers. Specifically, we define multi-head self-attention and\nmulti-layer perceptron modules along with their shortcuts as the finest\nreconstruction units. After reconstructing these two fine-grained units, we\ncombine them to form coarser blocks and reconstruct them at a coarser\ngranularity level. We iteratively perform this combination and reconstruction\nprocess, achieving progressive fine-to-coarse reconstruction. Additionally, we\nintroduce a Progressive Optimization Strategy (POS) for PFCR to alleviate the\ndifficulty of training, thereby further enhancing model performance.\nExperimental results on the ImageNet dataset demonstrate that our proposed\nmethod achieves the best Top-1 accuracy among state-of-the-art methods,\nparticularly attaining 75.61% for 3-bit quantized ViT-B in PTQ. Besides,\nquantization results on the COCO dataset reveal the effectiveness and\ngeneralization of our proposed method on other computer vision tasks like\nobject detection and instance segmentation.\n","authors":["Rui Ding","Liang Yong","Sihuan Zhao","Jing Nie","Lihui Chen","Haijun Liu","Xichuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14631v1","updated":"2024-12-19T08:36:32Z","published":"2024-12-19T08:36:32Z","title":"Review of Fruit Tree Image Segmentation","summary":"  Fruit tree image segmentation is an essential problem in automating a variety\nof agricultural tasks such as phenotyping, harvesting, spraying, and pruning.\nMany research papers have proposed a diverse spectrum of solutions suitable to\nspecific tasks and environments. The review scope of this paper is confined to\nthe front views of fruit trees and based on 158 relevant papers collected using\na newly designed crawling review method. These papers are systematically\nreviewed based on a taxonomy that sequentially considers the method, image,\ntask, and fruit. This taxonomy will assist readers to intuitively grasp the big\npicture of these research activities. Our review reveals that the most\nnoticeable deficiency of the previous studies was the lack of a versatile\ndataset and segmentation model that could be applied to a variety of tasks and\nenvironments. Six important future research tasks are suggested, with the\nexpectation that these will pave the way to building a versatile tree\nsegmentation module.\n","authors":["Il-Seok Oh"],"pdf_url":"https://arxiv.org/pdf/2412.14631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14630v1","updated":"2024-12-19T08:33:33Z","published":"2024-12-19T08:33:33Z","title":"Unified Image Restoration and Enhancement: Degradation Calibrated Cycle\n  Reconstruction Diffusion Model","summary":"  Image restoration and enhancement are pivotal for numerous computer vision\napplications, yet unifying these tasks efficiently remains a significant\nchallenge. Inspired by the iterative refinement capabilities of diffusion\nmodels, we propose CycleRDM, a novel framework designed to unify restoration\nand enhancement tasks while achieving high-quality mapping. Specifically,\nCycleRDM first learns the mapping relationships among the degraded domain, the\nrough normal domain, and the normal domain through a two-stage diffusion\ninference process. Subsequently, we transfer the final calibration process to\nthe wavelet low-frequency domain using discrete wavelet transform, performing\nfine-grained calibration from a frequency domain perspective by leveraging\ntask-specific frequency spaces. To improve restoration quality, we design a\nfeature gain module for the decomposed wavelet high-frequency domain to\neliminate redundant features. Additionally, we employ multimodal textual\nprompts and Fourier transform to drive stable denoising and reduce randomness\nduring the inference process. After extensive validation, CycleRDM can be\neffectively generalized to a wide range of image restoration and enhancement\ntasks while requiring only a small number of training samples to be\nsignificantly superior on various benchmarks of reconstruction quality and\nperceptual quality. The source code will be available at\nhttps://github.com/hejh8/CycleRDM.\n","authors":["Minglong Xue","Jinhong He","Shivakumara Palaiahnakote","Mingliang Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11216v2","updated":"2024-12-19T08:32:20Z","published":"2024-12-15T15:13:14Z","title":"Distribution-Consistency-Guided Multi-modal Hashing","summary":"  Multi-modal hashing methods have gained popularity due to their fast speed\nand low storage requirements. Among them, the supervised methods demonstrate\nbetter performance by utilizing labels as supervisory signals compared with\nunsupervised methods. Currently, for almost all supervised multi-modal hashing\nmethods, there is a hidden assumption that training sets have no noisy labels.\nHowever, labels are often annotated incorrectly due to manual labeling in\nreal-world scenarios, which will greatly harm the retrieval performance. To\naddress this issue, we first discover a significant distribution consistency\npattern through experiments, i.e., the 1-0 distribution of the presence or\nabsence of each category in the label is consistent with the high-low\ndistribution of similarity scores of the hash codes relative to category\ncenters. Then, inspired by this pattern, we propose a novel\nDistribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to\nfilter and reconstruct noisy labels to enhance retrieval performance.\nSpecifically, the proposed method first randomly initializes several category\ncenters, which are used to compute the high-low distribution of similarity\nscores; Noisy and clean labels are then separately filtered out via the\ndiscovered distribution consistency pattern to mitigate the impact of noisy\nlabels; Subsequently, a correction strategy, which is indirectly designed via\nthe distribution consistency pattern, is applied to the filtered noisy labels,\ncorrecting high-confidence ones while treating low-confidence ones as unlabeled\nfor unsupervised learning, thereby further enhancing the model's performance.\nExtensive experiments on three widely used datasets demonstrate the superiority\nof the proposed method compared to state-of-the-art baselines in multi-modal\nretrieval tasks. The code is available at\nhttps://github.com/LiuJinyu1229/DCGMH.\n","authors":["Jin-Yu Liu","Xian-Ling Mao","Tian-Yi Che","Rong-Cheng Tu"],"pdf_url":"https://arxiv.org/pdf/2412.11216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14629v1","updated":"2024-12-19T08:31:42Z","published":"2024-12-19T08:31:42Z","title":"Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix\n  Factorization","summary":"  Robust Principal Component Analysis (RPCA) is a fundamental technique for\ndecomposing data into low-rank and sparse components, which plays a critical\nrole for applications such as image processing and anomaly detection.\nTraditional RPCA methods commonly use $\\ell_1$ norm regularization to enforce\nsparsity, but this approach can introduce bias and result in suboptimal\nestimates, particularly in the presence of significant noise or outliers.\nNon-convex regularization methods have been proposed to mitigate these\nchallenges, but they tend to be complex to optimize and sensitive to initial\nconditions, leading to potential instability in solutions. To overcome these\nchallenges, in this paper, we propose a novel RPCA model that integrates\nadaptive weighted least squares (AWLS) and low-rank matrix factorization\n(LRMF). The model employs a {self-attention-inspired} mechanism in its weight\nupdate process, allowing the weight matrix to dynamically adjust and emphasize\nsignificant components during each iteration. By employing a weighted F-norm\nfor the sparse component, our method effectively reduces bias while simplifying\nthe computational process compared to traditional $\\ell_1$-norm-based methods.\nWe use an alternating minimization algorithm, where each subproblem has an\nexplicit solution, thereby improving computational efficiency. Despite its\nsimplicity, numerical experiments demonstrate that our method outperforms\nexisting non-convex regularization approaches, offering superior performance\nand stability, as well as enhanced accuracy and robustness in practical\napplications.\n","authors":["Kexin Li","You-wei Wen","Xu Xiao","Mingchao Zhao"],"pdf_url":"https://arxiv.org/pdf/2412.14629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14628v1","updated":"2024-12-19T08:30:54Z","published":"2024-12-19T08:30:54Z","title":"Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models","summary":"  Diffusion Models (DM) have democratized AI image generation through an\niterative denoising process. Quantization is a major technique to alleviate the\ninference cost and reduce the size of DM denoiser networks. However, as\ndenoisers evolve from variants of convolutional U-Nets toward newer Transformer\narchitectures, it is of growing importance to understand the quantization\nsensitivity of different weight layers, operations and architecture types to\nperformance. In this work, we address this challenge with Qua$^2$SeDiMo, a\nmixed-precision Post-Training Quantization framework that generates explainable\ninsights on the cost-effectiveness of various model weight quantization methods\nfor different denoiser operation types and block structures. We leverage these\ninsights to make high-quality mixed-precision quantization decisions for a\nmyriad of diffusion models ranging from foundational U-Nets to state-of-the-art\nTransformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,\n3.65-bit and 3.7-bit weight quantization on PixArt-${\\alpha}$,\nPixArt-${\\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our\nweight-quantization configurations with 6-bit activation quantization and\noutperform existing approaches in terms of quantitative metrics and generative\nimage quality.\n","authors":["Keith G. Mills","Mohammad Salameh","Ruichen Chen","Negar Hassanpour","Wei Lu","Di Niu"],"pdf_url":"https://arxiv.org/pdf/2412.14628v1.pdf","comment":"AAAI 2025; version includes supplementary material; 22 Pages, 18\n  Figures, 8 Tables"},{"id":"http://arxiv.org/abs/2412.14623v1","updated":"2024-12-19T08:21:28Z","published":"2024-12-19T08:21:28Z","title":"FRIDAY: Mitigating Unintentional Facial Identity in Deepfake Detectors\n  Guided by Facial Recognizers","summary":"  Previous Deepfake detection methods perform well within their training\ndomains, but their effectiveness diminishes significantly with new synthesis\ntechniques. Recent studies have revealed that detection models often create\ndecision boundaries based on facial identity rather than synthetic artifacts,\nresulting in poor performance on cross-domain datasets. To address this\nlimitation, we propose Facial Recognition Identity Attenuation (FRIDAY), a\nnovel training method that mitigates facial identity influence using a face\nrecognizer. Specifically, we first train a face recognizer using the same\nbackbone as the Deepfake detector. The recognizer is then frozen and employed\nduring the detector's training to reduce facial identity information. This is\nachieved by feeding input images into both the recognizer and the detector, and\nminimizing the similarity of their feature embeddings through our Facial\nIdentity Attenuating loss. This process encourages the detector to generate\nembeddings distinct from the recognizer, effectively reducing the impact of\nfacial identity. Extensive experiments demonstrate that our approach\nsignificantly enhances detection performance on both in-domain and cross-domain\ndatasets.\n","authors":["Younhun Kim","Myung-Joon Kwon","Wonjun Lee","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2412.14623v1.pdf","comment":"5 pages, 4 figures. In 2024 IEEE International Conference on Visual\n  Communications and Image Processing (VCIP) Oral"},{"id":"http://arxiv.org/abs/2409.17671v3","updated":"2024-12-19T08:19:41Z","published":"2024-09-26T09:30:37Z","title":"Leveraging Anthropometric Measurements to Improve Human Mesh Estimation\n  and Ensure Consistent Body Shapes","summary":"  The basic body shape (i.e., the body shape in T-pose) of a person does not\nchange within a single video. However, most SOTA human mesh estimation (HME)\nmodels output a slightly different, thus inconsistent basic body shape for each\nvideo frame. Furthermore, we find that SOTA 3D human pose estimation (HPE)\nmodels outperform HME models regarding the precision of the estimated 3D\nkeypoint positions. We solve the problem of inconsistent body shapes by\nleveraging anthropometric measurements like taken by tailors from humans. We\ncreate a model called A2B that converts given anthropometric measurements to\nbasic body shape parameters of human mesh models. We obtain superior and\nconsistent human meshes by combining the A2B model results with the keypoints\nof 3D HPE models using inverse kinematics. We evaluate our approach on\nchallenging datasets like ASPset or fit3D, where we can lower the MPJPE by over\n30 mm compared to SOTA HME models. Further, replacing estimates of the body\nshape parameters from existing HME models with A2B results not only increases\nthe performance of these HME models, but also guarantees consistent body\nshapes.\n","authors":["Katja Ludwig","Julian Lorenz","Daniel Kienzle","Tuan Bui","Rainer Lienhart"],"pdf_url":"https://arxiv.org/pdf/2409.17671v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14619v1","updated":"2024-12-19T08:11:42Z","published":"2024-12-19T08:11:42Z","title":"Pitfalls of topology-aware image segmentation","summary":"  Topological correctness, i.e., the preservation of structural integrity and\nspecific characteristics of shape, is a fundamental requirement for medical\nimaging tasks, such as neuron or vessel segmentation. Despite the recent surge\nin topology-aware methods addressing this challenge, their real-world\napplicability is hindered by flawed benchmarking practices. In this paper, we\nidentify critical pitfalls in model evaluation that include inadequate\nconnectivity choices, overlooked topological artifacts in ground truth\nannotations, and inappropriate use of evaluation metrics. Through detailed\nempirical analysis, we uncover these issues' profound impact on the evaluation\nand ranking of segmentation methods. Drawing from our findings, we propose a\nset of actionable recommendations to establish fair and robust evaluation\nstandards for topology-aware medical image segmentation methods.\n","authors":["Alexander H. Berger","Laurin Lux","Alexander Weers","Martin Menten","Daniel Rueckert","Johannes C. Paetzold"],"pdf_url":"https://arxiv.org/pdf/2412.14619v1.pdf","comment":"Code is available at\n  https://github.com/AlexanderHBerger/topo-pitfalls"},{"id":"http://arxiv.org/abs/2412.14613v1","updated":"2024-12-19T08:03:16Z","published":"2024-12-19T08:03:16Z","title":"HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic\n  Evaluation Using a Vision Language Model","summary":"  Vision-language models (VLMs) have shown impressive abilities in text and\nimage understanding. However, existing metrics for evaluating the text\ngenerated by VLMs focus exclusively on overall quality, leading to two\nlimitations: 1) it is challenging to identify which aspects of the text need\nimprovement from the overall score; 2) metrics may overlook specific evaluation\ncriteria when predicting an overall score. To address these limitations, we\npropose HarmonicEval, a reference-free evaluation metric that aggregates\ncriterion-wise scores to produce the overall score in a bottom-up manner.\nFurthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)\ndataset, which comprises 18,000 expert human judgments across four\nvision-language tasks. Our experiments demonstrate that HarmonicEval achieves\nhigher correlations with human judgments than conventional metrics while\nproviding numerical scores for each criterion.\n","authors":["Masanari Ohi","Masahiro Kaneko","Naoaki Okazaki","Nakamasa Inoue"],"pdf_url":"https://arxiv.org/pdf/2412.14613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19101v4","updated":"2024-12-19T08:00:44Z","published":"2024-06-27T11:28:36Z","title":"DocKylin: A Large Multimodal Model for Visual Document Understanding\n  with Efficient Visual Slimming","summary":"  Current multimodal large language models (MLLMs) face significant challenges\nin visual document understanding (VDU) tasks due to the high resolution, dense\ntext, and complex layouts typical of document images. These characteristics\ndemand a high level of detail perception ability from MLLMs. While increasing\ninput resolution improves detail perception capability, it also leads to longer\nsequences of visual tokens, increasing computational costs and straining the\nmodels' ability to handle long contexts. To address these challenges, we\nintroduce DocKylin, a document-centric MLLM that performs visual content\nslimming at both the pixel and token levels, thereby reducing token sequence\nlength in VDU scenarios. We introduce an Adaptive Pixel Slimming (APS)\npreprocessing module to perform pixel-level slimming, increasing the proportion\nof informative pixels. Moreover, we propose a novel Dynamic Token Slimming\n(DTS) module to conduct token-level slimming, filtering essential tokens and\nremoving others to adaptively create a more compact visual sequence.\nExperiments demonstrate DocKylin's promising performance across various VDU\nbenchmarks and the effectiveness of each component.\n","authors":["Jiaxin Zhang","Wentao Yang","Songxuan Lai","Zecheng Xie","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2406.19101v4.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14603v1","updated":"2024-12-19T07:49:40Z","published":"2024-12-19T07:49:40Z","title":"Successive optimization of optics and post-processing with\n  differentiable coherent PSF operator and field information","summary":"  Recently, the joint design of optical systems and downstream algorithms is\nshowing significant potential. However, existing rays-described methods are\nlimited to optimizing geometric degradation, making it difficult to fully\nrepresent the optical characteristics of complex, miniaturized lenses\nconstrained by wavefront aberration or diffraction effects. In this work, we\nintroduce a precise optical simulation model, and every operation in pipeline\nis differentiable. This model employs a novel initial value strategy to enhance\nthe reliability of intersection calculation on high aspherics. Moreover, it\nutilizes a differential operator to reduce memory consumption during coherent\npoint spread function calculations. To efficiently address various degradation,\nwe design a joint optimization procedure that leverages field information.\nGuided by a general restoration network, the proposed method not only enhances\nthe image quality, but also successively improves the optical performance\nacross multiple lenses that are already in professional level. This joint\noptimization pipeline offers innovative insights into the practical design of\nsophisticated optical systems and post-processing algorithms. The source code\nwill be made publicly available at\nhttps://github.com/Zrr-ZJU/Successive-optimization\n","authors":["Zheng Ren","Jingwen Zhou","Wenguan Zhang","Jiapu Yan","Bingkun Chen","Huajun Feng","Shiqi Chen"],"pdf_url":"https://arxiv.org/pdf/2412.14603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14598v1","updated":"2024-12-19T07:39:06Z","published":"2024-12-19T07:39:06Z","title":"Can We Get Rid of Handcrafted Feature Extractors? SparseViT:\n  Nonsemantics-Centered, Parameter-Efficient Image Manipulation Localization\n  Through Spare-Coding Transformer","summary":"  Non-semantic features or semantic-agnostic features, which are irrelevant to\nimage context but sensitive to image manipulations, are recognized as\nevidential to Image Manipulation Localization (IML). Since manual labels are\nimpossible, existing works rely on handcrafted methods to extract non-semantic\nfeatures. Handcrafted non-semantic features jeopardize IML model's\ngeneralization ability in unseen or complex scenarios. Therefore, for IML, the\nelephant in the room is: How to adaptively extract non-semantic features?\nNon-semantic features are context-irrelevant and manipulation-sensitive. That\nis, within an image, they are consistent across patches unless manipulation\noccurs. Then, spare and discrete interactions among image patches are\nsufficient for extracting non-semantic features. However, image semantics vary\ndrastically on different patches, requiring dense and continuous interactions\namong image patches for learning semantic representations. Hence, in this\npaper, we propose a Sparse Vision Transformer (SparseViT), which reformulates\nthe dense, global self-attention in ViT into a sparse, discrete manner. Such\nsparse self-attention breaks image semantics and forces SparseViT to adaptively\nextract non-semantic features for images. Besides, compared with existing IML\nmodels, the sparse self-attention mechanism largely reduced the model size (max\n80% in FLOPs), achieving stunning parameter efficiency and computation\nreduction. Extensive experiments demonstrate that, without any handcrafted\nfeature extractors, SparseViT is superior in both generalization and efficiency\nacross benchmark datasets.\n","authors":["Lei Su","Xiaochen Ma","Xuekang Zhu","Chaoqun Niu","Zeyu Lei","Ji-Zhe Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14598v1.pdf","comment":"12 page, 8 figures, published to AAAI"},{"id":"http://arxiv.org/abs/2412.14596v1","updated":"2024-12-19T07:31:40Z","published":"2024-12-19T07:31:40Z","title":"LDP: Generalizing to Multilingual Visual Information Extraction by\n  Language Decoupled Pretraining","summary":"  Visual Information Extraction (VIE) plays a crucial role in the comprehension\nof semi-structured documents, and several pre-trained models have been\ndeveloped to enhance performance. However, most of these works are monolingual\n(usually English). Due to the extremely unbalanced quantity and quality of\npre-training corpora between English and other languages, few works can extend\nto non-English scenarios. In this paper, we conduct systematic experiments to\nshow that vision and layout modality hold invariance among images with\ndifferent languages. If decoupling language bias from document images, a\nvision-layout-based model can achieve impressive cross-lingual generalization.\nAccordingly, we present a simple but effective multilingual training paradigm\nLDP (Language Decoupled Pre-training) for better utilization of monolingual\npre-training data. Our proposed model LDM (Language Decoupled Model) is first\npre-trained on the language-independent data, where the language knowledge is\ndecoupled by a diffusion model, and then the LDM is fine-tuned on the\ndownstream languages. Extensive experiments show that the LDM outperformed all\nSOTA multilingual pre-trained models, and also maintains competitiveness on\ndownstream monolingual/English benchmarks.\n","authors":["Huawen Shen","Gengluo Li","Jinwen Zhong","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2412.14596v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14592v1","updated":"2024-12-19T07:23:17Z","published":"2024-12-19T07:23:17Z","title":"Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,\n  and Internal Properties","summary":"  Object anomaly detection is essential for industrial quality inspection, yet\ntraditional single-sensor methods face critical limitations. They fail to\ncapture the wide range of anomaly types, as single sensors are often\nconstrained to either external appearance, geometric structure, or internal\nproperties. To overcome these challenges, we introduce MulSen-AD, the first\nhigh-resolution, multi-sensor anomaly detection dataset tailored for industrial\napplications. MulSen-AD unifies data from RGB cameras, laser scanners, and\nlock-in infrared thermography, effectively capturing external appearance,\ngeometric deformations, and internal defects. The dataset spans 15 industrial\nproducts with diverse, real-world anomalies. We also present MulSen-AD Bench, a\nbenchmark designed to evaluate multi-sensor methods, and propose\nMulSen-TripleAD, a decision-level fusion algorithm that integrates these three\nmodalities for robust, unsupervised object anomaly detection. Our experiments\ndemonstrate that multi-sensor fusion substantially outperforms single-sensor\napproaches, achieving 96.1% AUROC in object-level detection accuracy. These\nresults highlight the importance of integrating multi-sensor data for\ncomprehensive industrial anomaly detection.\n","authors":["Wenqiao Li","Bozhong Zheng","Xiaohao Xu","Jinye Gan","Fading Lu","Xiang Li","Na Ni","Zheng Tian","Xiaonan Huang","Shenghua Gao","Yingna Wu"],"pdf_url":"https://arxiv.org/pdf/2412.14592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09583v4","updated":"2024-12-19T07:21:43Z","published":"2024-10-12T16:28:40Z","title":"POPoS: Improving Efficient and Robust Facial Landmark Detection with\n  Parallel Optimal Position Search","summary":"  Achieving a balance between accuracy and efficiency is a critical challenge\nin facial landmark detection (FLD). This paper introduces Parallel Optimal\nPosition Search (POPoS), a high-precision encoding-decoding framework designed\nto address the limitations of traditional FLD methods. POPoS employs three key\ncontributions: (1) Pseudo-range multilateration is utilized to correct heatmap\nerrors, improving landmark localization accuracy. By integrating multiple\nanchor points, it reduces the impact of individual heatmap inaccuracies,\nleading to robust overall positioning. (2) To enhance the pseudo-range accuracy\nof selected anchor points, a new loss function, named multilateration anchor\nloss, is proposed. This loss function enhances the accuracy of the distance\nmap, mitigates the risk of local optima, and ensures optimal solutions. (3) A\nsingle-step parallel computation algorithm is introduced, boosting\ncomputational efficiency and reducing processing time. Extensive evaluations\nacross five benchmark datasets demonstrate that POPoS consistently outperforms\nexisting methods, particularly excelling in low-resolution heatmaps scenarios\nwith minimal computational overhead. These advantages make POPoS a highly\nefficient and accurate tool for FLD, with broad applicability in real-world\nscenarios.\n","authors":["Chong-Yang Xiang","Jun-Yan He","Zhi-Qi Cheng","Xiao Wu","Xian-Sheng Hua"],"pdf_url":"https://arxiv.org/pdf/2410.09583v4.pdf","comment":"Accepted to AAAI 2025, 9 pages, 6 figures. Code:\n  https://github.com/teslatasy/POPoS"},{"id":"http://arxiv.org/abs/2410.20815v2","updated":"2024-12-19T07:19:52Z","published":"2024-10-28T08:02:34Z","title":"Grid4D: 4D Decomposed Hash Encoding for High-fidelity Dynamic Gaussian\n  Splatting","summary":"  Recently, Gaussian splatting has received more and more attention in the\nfield of static scene rendering. Due to the low computational overhead and\ninherent flexibility of explicit representations, plane-based explicit methods\nare popular ways to predict deformations for Gaussian-based dynamic scene\nrendering models. However, plane-based methods rely on the inappropriate\nlow-rank assumption and excessively decompose the space-time 4D encoding,\nresulting in overmuch feature overlap and unsatisfactory rendering quality. To\ntackle these problems, we propose Grid4D, a dynamic scene rendering model based\non Gaussian splatting and employing a novel explicit encoding method for the 4D\ninput through the hash encoding. Different from plane-based explicit\nrepresentations, we decompose the 4D encoding into one spatial and three\ntemporal 3D hash encodings without the low-rank assumption. Additionally, we\ndesign a novel attention module that generates the attention scores in a\ndirectional range to aggregate the spatial and temporal features. The\ndirectional attention enables Grid4D to more accurately fit the diverse\ndeformations across distinct scene components based on the spatial encoded\nfeatures. Moreover, to mitigate the inherent lack of smoothness in explicit\nrepresentation methods, we introduce a smooth regularization term that keeps\nour model from the chaos of deformation prediction. Our experiments demonstrate\nthat Grid4D significantly outperforms the state-of-the-art models in visual\nquality and rendering speed.\n","authors":["Jiawei Xu","Zexin Fan","Jian Yang","Jin Xie"],"pdf_url":"https://arxiv.org/pdf/2410.20815v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2412.14587v1","updated":"2024-12-19T07:13:15Z","published":"2024-12-19T07:13:15Z","title":"Spike2Former: Efficient Spiking Transformer for High-performance Image\n  Segmentation","summary":"  Spiking Neural Networks (SNNs) have a low-power advantage but perform poorly\nin image segmentation tasks. The reason is that directly converting neural\nnetworks with complex architectural designs for segmentation tasks into spiking\nversions leads to performance degradation and non-convergence. To address this\nchallenge, we first identify the modules in the architecture design that lead\nto the severe reduction in spike firing, make targeted improvements, and\npropose Spike2Former architecture. Second, we propose normalized integer\nspiking neurons to solve the training stability problem of SNNs with complex\narchitectures. We set a new state-of-the-art for SNNs in various semantic\nsegmentation datasets, with a significant improvement of +12.7% mIoU and 5.0\nefficiency on ADE20K, +14.3% mIoU and 5.2 efficiency on VOC2012, and +9.1% mIoU\nand 6.6 efficiency on CityScapes.\n","authors":["Zhenxin Lei","Man Yao","Jiakui Hu","Xinhao Luo","Yanye Lu","Bo Xu","Guoqi Li"],"pdf_url":"https://arxiv.org/pdf/2412.14587v1.pdf","comment":"This work has been accepted on Association for the Advancement of\n  Artificial Intelligence 2025"},{"id":"http://arxiv.org/abs/2412.14585v1","updated":"2024-12-19T07:06:25Z","published":"2024-12-19T07:06:25Z","title":"HiCM$^2$: Hierarchical Compact Memory Modeling for Dense Video\n  Captioning","summary":"  With the growing demand for solutions to real-world video challenges,\ninterest in dense video captioning (DVC) has been on the rise. DVC involves the\nautomatic captioning and localization of untrimmed videos. Several studies\nhighlight the challenges of DVC and introduce improved methods utilizing prior\nknowledge, such as pre-training and external memory. In this research, we\npropose a model that leverages the prior knowledge of human-oriented\nhierarchical compact memory inspired by human memory hierarchy and cognition.\nTo mimic human-like memory recall, we construct a hierarchical memory and a\nhierarchical memory reading module. We build an efficient hierarchical compact\nmemory by employing clustering of memory events and summarization using large\nlanguage models. Comparative experiments demonstrate that this hierarchical\nmemory recall process improves the performance of DVC by achieving\nstate-of-the-art performance on YouCook2 and ViTT datasets.\n","authors":["Minkuk Kim","Hyeon Bae Kim","Jinyoung Moon","Jinwoo Choi","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2412.14585v1.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.14580v1","updated":"2024-12-19T07:00:03Z","published":"2024-12-19T07:00:03Z","title":"DiffSim: Taming Diffusion Models for Evaluating Visual Similarity","summary":"  Diffusion models have fundamentally transformed the field of generative\nmodels, making the assessment of similarity between customized model outputs\nand reference inputs critically important. However, traditional perceptual\nsimilarity metrics operate primarily at the pixel and patch levels, comparing\nlow-level colors and textures but failing to capture mid-level similarities and\ndifferences in image layout, object pose, and semantic content. Contrastive\nlearning-based CLIP and self-supervised learning-based DINO are often used to\nmeasure semantic similarity, but they highly compress image features,\ninadequately assessing appearance details. This paper is the first to discover\nthat pretrained diffusion models can be utilized for measuring visual\nsimilarity and introduces the DiffSim method, addressing the limitations of\ntraditional metrics in capturing perceptual consistency in custom generation\ntasks. By aligning features in the attention layers of the denoising U-Net,\nDiffSim evaluates both appearance and style similarity, showing superior\nalignment with human visual preferences. Additionally, we introduce the Sref\nand IP benchmarks to evaluate visual similarity at the level of style and\ninstance, respectively. Comprehensive evaluations across multiple benchmarks\ndemonstrate that DiffSim achieves state-of-the-art performance, providing a\nrobust tool for measuring visual coherence in generative models.\n","authors":["Yiren Song","Xiaokang Liu","Mike Zheng Shou"],"pdf_url":"https://arxiv.org/pdf/2412.14580v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14579v1","updated":"2024-12-19T06:57:37Z","published":"2024-12-19T06:57:37Z","title":"GSRender: Deduplicated Occupancy Prediction via Weakly Supervised 3D\n  Gaussian Splatting","summary":"  3D occupancy perception is gaining increasing attention due to its capability\nto offer detailed and precise environment representations. Previous\nweakly-supervised NeRF methods balance efficiency and accuracy, with mIoU\nvarying by 5-10 points due to sampling count along camera rays. Recently,\nreal-time Gaussian splatting has gained widespread popularity in 3D\nreconstruction, and the occupancy prediction task can also be viewed as a\nreconstruction task. Consequently, we propose GSRender, which naturally employs\n3D Gaussian Splatting for occupancy prediction, simplifying the sampling\nprocess. In addition, the limitations of 2D supervision result in duplicate\npredictions along the same camera ray. We implemented the Ray Compensation (RC)\nmodule, which mitigates this issue by compensating for features from adjacent\nframes. Finally, we redesigned the loss to eliminate the impact of dynamic\nobjects from adjacent frames. Extensive experiments demonstrate that our\napproach achieves SOTA (state-of-the-art) results in RayIoU (+6.0), while\nnarrowing the gap with 3D supervision methods. Our code will be released soon.\n","authors":["Qianpu Sun","Changyong Shu","Sifan Zhou","Zichen Yu","Yan Chen","Dawei Yang","Yuan Chun"],"pdf_url":"https://arxiv.org/pdf/2412.14579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.14576v1","updated":"2024-12-19T06:52:12Z","published":"2024-12-19T06:52:12Z","title":"Alignment-Free RGB-T Salient Object Detection: A Large-scale Dataset and\n  Progressive Correlation Network","summary":"  Alignment-free RGB-Thermal (RGB-T) salient object detection (SOD) aims to\nachieve robust performance in complex scenes by directly leveraging the\ncomplementary information from unaligned visible-thermal image pairs, without\nrequiring manual alignment. However, the labor-intensive process of collecting\nand annotating image pairs limits the scale of existing benchmarks, hindering\nthe advancement of alignment-free RGB-T SOD. In this paper, we construct a\nlarge-scale and high-diversity unaligned RGB-T SOD dataset named UVT20K,\ncomprising 20,000 image pairs, 407 scenes, and 1256 object categories. All\nsamples are collected from real-world scenarios with various challenges, such\nas low illumination, image clutter, complex salient objects, and so on. To\nsupport the exploration for further research, each sample in UVT20K is\nannotated with a comprehensive set of ground truths, including saliency masks,\nscribbles, boundaries, and challenge attributes. In addition, we propose a\nProgressive Correlation Network (PCNet), which models inter- and intra-modal\ncorrelations on the basis of explicit alignment to achieve accurate predictions\nin unaligned image pairs. Extensive experiments conducted on unaligned and\naligned datasets demonstrate the effectiveness of our method.Code and dataset\nare available at https://github.com/Angknpng/PCNet.\n","authors":["Kunpeng Wang","Keke Chen","Chenglong Li","Zhengzheng Tu","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2412.14576v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14571v1","updated":"2024-12-19T06:42:25Z","published":"2024-12-19T06:42:25Z","title":"SCKD: Semi-Supervised Cross-Modality Knowledge Distillation for 4D Radar\n  Object Detection","summary":"  3D object detection is one of the fundamental perception tasks for autonomous\nvehicles. Fulfilling such a task with a 4D millimeter-wave radar is very\nattractive since the sensor is able to acquire 3D point clouds similar to Lidar\nwhile maintaining robust measurements under adverse weather. However, due to\nthe high sparsity and noise associated with the radar point clouds, the\nperformance of the existing methods is still much lower than expected. In this\npaper, we propose a novel Semi-supervised Cross-modality Knowledge Distillation\n(SCKD) method for 4D radar-based 3D object detection. It characterizes the\ncapability of learning the feature from a Lidar-radar-fused teacher network\nwith semi-supervised distillation. We first propose an adaptive fusion module\nin the teacher network to boost its performance. Then, two feature distillation\nmodules are designed to facilitate the cross-modality knowledge transfer.\nFinally, a semi-supervised output distillation is proposed to increase the\neffectiveness and flexibility of the distillation framework. With the same\nnetwork structure, our radar-only student trained by SCKD boosts the mAP by\n10.38% over the baseline and outperforms the state-of-the-art works on the VoD\ndataset. The experiment on ZJUODset also shows 5.12% mAP improvements on the\nmoderate difficulty level over the baseline when extra unlabeled data are\navailable. Code is available at https://github.com/Ruoyu-Xu/SCKD.\n","authors":["Ruoyu Xu","Zhiyu Xiang","Chenwei Zhang","Hanzhi Zhong","Xijun Zhao","Ruina Dang","Peng Xu","Tianyu Pu","Eryun Liu"],"pdf_url":"https://arxiv.org/pdf/2412.14571v1.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2410.04749v2","updated":"2024-12-19T06:41:40Z","published":"2024-10-07T04:59:08Z","title":"LLaVA Needs More Knowledge: Retrieval Augmented Natural Language\n  Generation with Knowledge Graph for Explaining Thoracic Pathologies","summary":"  Generating Natural Language Explanations (NLEs) for model predictions on\nmedical images, particularly those depicting thoracic pathologies, remains a\ncritical and challenging task. Existing methodologies often struggle due to\ngeneral models' insufficient domain-specific medical knowledge and privacy\nconcerns associated with retrieval-based augmentation techniques. To address\nthese issues, we propose a novel Vision-Language framework augmented with a\nKnowledge Graph (KG)-based datastore, which enhances the model's understanding\nby incorporating additional domain-specific medical knowledge essential for\ngenerating accurate and informative NLEs. Our framework employs a KG-based\nretrieval mechanism that not only improves the precision of the generated\nexplanations but also preserves data privacy by avoiding direct data retrieval.\nThe KG datastore is designed as a plug-and-play module, allowing for seamless\nintegration with various model architectures. We introduce and evaluate three\ndistinct frameworks within this paradigm: KG-LLaVA, which integrates the\npre-trained LLaVA model with KG-RAG; Med-XPT, a custom framework combining\nMedCLIP, a transformer-based projector, and GPT-2; and Bio-LLaVA, which adapts\nLLaVA by incorporating the Bio-ViT-L vision model. These frameworks are\nvalidated on the MIMIC-NLE dataset, where they achieve state-of-the-art\nresults, underscoring the effectiveness of KG augmentation in generating\nhigh-quality NLEs for thoracic pathologies.\n","authors":["Ameer Hamza"," Abdullah","Yong Hyun Ahn","Sungyoung Lee","Seong Tae Kim"],"pdf_url":"https://arxiv.org/pdf/2410.04749v2.pdf","comment":"AAAI2025"},{"id":"http://arxiv.org/abs/2412.14568v1","updated":"2024-12-19T06:39:28Z","published":"2024-12-19T06:39:28Z","title":"Improving Geometry in Sparse-View 3DGS via Reprojection-based DoF\n  Separation","summary":"  Recent learning-based Multi-View Stereo models have demonstrated\nstate-of-the-art performance in sparse-view 3D reconstruction. However,\ndirectly applying 3D Gaussian Splatting (3DGS) as a refinement step following\nthese models presents challenges. We hypothesize that the excessive positional\ndegrees of freedom (DoFs) in Gaussians induce geometry distortion, fitting\ncolor patterns at the cost of structural fidelity. To address this, we propose\nreprojection-based DoF separation, a method distinguishing positional DoFs in\nterms of uncertainty: image-plane-parallel DoFs and ray-aligned DoF. To\nindependently manage each DoF, we introduce a reprojection process along with\ntailored constraints for each DoF. Through experiments across various datasets,\nwe confirm that separating the positional DoFs of Gaussians and applying\ntargeted constraints effectively suppresses geometric artifacts, producing\nreconstruction results that are both visually and geometrically plausible.\n","authors":["Yongsung Kim","Minjun Park","Jooyoung Choi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2412.14568v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2412.11530v2","updated":"2024-12-19T06:32:22Z","published":"2024-12-16T08:08:35Z","title":"RoMeO: Robust Metric Visual Odometry","summary":"  Visual odometry (VO) aims to estimate camera poses from visual inputs -- a\nfundamental building block for many applications such as VR/AR and robotics.\nThis work focuses on monocular RGB VO where the input is a monocular RGB video\nwithout IMU or 3D sensors. Existing approaches lack robustness under this\nchallenging scenario and fail to generalize to unseen data (especially\noutdoors); they also cannot recover metric-scale poses. We propose Robust\nMetric Visual Odometry (RoMeO), a novel method that resolves these issues\nleveraging priors from pre-trained depth models. RoMeO incorporates both\nmonocular metric depth and multi-view stereo (MVS) models to recover\nmetric-scale, simplify correspondence search, provide better initialization and\nregularize optimization. Effective strategies are proposed to inject noise\nduring training and adaptively filter noisy depth priors, which ensure the\nrobustness of RoMeO on in-the-wild data. As shown in Fig.1, RoMeO advances the\nstate-of-the-art (SOTA) by a large margin across 6 diverse datasets covering\nboth indoor and outdoor scenes. Compared to the current SOTA DPVO, RoMeO\nreduces the relative (align the trajectory scale with GT) and absolute\ntrajectory errors both by >50%. The performance gain also transfers to the full\nSLAM pipeline (with global BA & loop closure). Code will be released upon\nacceptance.\n","authors":["Junda Cheng","Zhipeng Cai","Zhaoxing Zhang","Wei Yin","Matthias Muller","Michael Paulitsch","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2412.11530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11973v6","updated":"2024-12-19T06:29:38Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) and Task-agnostic\nIncremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning\n(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is\ndesigned to prevent overfitting when the data samples are scarce. Furthermore,\nthe sparse reuse of WSN weights is considered for Video Incremental Learning\n(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It\nenables compact encoding of videos and identifies reusable subnetworks across\nvarying bandwidths. We have integrated FSO into different architectural\nframeworks for continual learning, including VIL, TIL, and FSCIL. Our\ncomprehensive experiments demonstrate FSO's effectiveness, significantly\nimproving task performance at various convolutional representational levels.\nSpecifically, FSO enhances higher-layer performance in TIL and FSCIL and\nlower-layer performance in VIL.\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v6.pdf","comment":"IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (T-PAMI)"},{"id":"http://arxiv.org/abs/2412.14561v1","updated":"2024-12-19T06:26:16Z","published":"2024-12-19T06:26:16Z","title":"GBRIP: Granular Ball Representation for Imbalanced Partial Label\n  Learning","summary":"  Partial label learning (PLL) is a complicated weakly supervised\nmulti-classification task compounded by class imbalance. Currently, existing\nmethods only rely on inter-class pseudo-labeling from inter-class features,\noften overlooking the significant impact of the intra-class imbalanced features\ncombined with the inter-class. To address these limitations, we introduce\nGranular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for\nimbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and\nmulti-center loss to construct a granular ball-based nfeature space through\nunsupervised learning, effectively capturing the feature distribution within\neach class. GBRIP mitigates the impact of confusing features by systematically\nrefining label disambiguation and estimating imbalance distributions. The novel\nmulti-center loss function enhances learning by emphasizing the relationships\nbetween samples and their respective centers within the granular balls.\nExtensive experiments on standard benchmarks demonstrate that GBRIP outperforms\nexisting state-of-the-art methods, offering a robust solution to the challenges\nof imbalanced PLL.\n","authors":["Jintao Huang","Yiu-ming Cheung","Chi-man Vong","Wenbin Qian"],"pdf_url":"https://arxiv.org/pdf/2412.14561v1.pdf","comment":"AAAI25"},{"id":"http://arxiv.org/abs/2409.01179v3","updated":"2024-12-19T06:26:04Z","published":"2024-09-02T11:19:54Z","title":"Recoverable Compression: A Multimodal Vision Token Recovery Mechanism\n  Guided by Text Information","summary":"  With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance.\n","authors":["Yi Chen","Jian Xu","Xu-Yao Zhang","Wen-Zhuo Liu","Yang-Yang Liu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2409.01179v3.pdf","comment":"AAAI2025 Accepted"},{"id":"http://arxiv.org/abs/2403.10650v3","updated":"2024-12-19T06:25:45Z","published":"2024-03-15T19:35:10Z","title":"PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time\n  Adaptation","summary":"  Real-world vision models in dynamic environments face rapid shifts in domain\ndistributions, leading to decreased recognition performance. Using unlabeled\ntest data, continuous test-time adaptation (CTTA) directly adjusts a\npre-trained source discriminative model to these changing domains. A highly\neffective CTTA method involves applying layer-wise adaptive learning rates for\nselectively adapting pre-trained layers. However, it suffers from the poor\nestimation of domain shift and the inaccuracies arising from the pseudo-labels.\nThis work aims to overcome these limitations by identifying layers for\nadaptation via quantifying model prediction uncertainty without relying on\npseudo-labels. We utilize the magnitude of gradients as a metric, calculated by\nbackpropagating the KL divergence between the softmax output and a uniform\ndistribution, to select layers for further adaptation. Subsequently, for the\nparameters exclusively belonging to these selected layers, with the remaining\nones frozen, we evaluate their sensitivity to approximate the domain shift and\nadjust their learning rates accordingly. We conduct extensive image\nclassification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,\ndemonstrating the superior efficacy of our method compared to prior approaches.\n","authors":["Sarthak Kumar Maharana","Baoming Zhang","Yunhui Guo"],"pdf_url":"https://arxiv.org/pdf/2403.10650v3.pdf","comment":"AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14559v1","updated":"2024-12-19T06:22:19Z","published":"2024-12-19T06:22:19Z","title":"ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation\n  Model","summary":"  The scaling law has been validated in various domains, such as natural\nlanguage processing (NLP) and massive computer vision tasks; however, its\napplication to motion generation remains largely unexplored. In this paper, we\nintroduce a scalable motion generation framework that includes the motion\ntokenizer Motion FSQ-VAE and a text-prefix autoregressive transformer. Through\ncomprehensive experiments, we observe the scaling behavior of this system. For\nthe first time, we confirm the existence of scaling laws within the context of\nmotion generation. Specifically, our results demonstrate that the normalized\ntest loss of our prefix autoregressive models adheres to a logarithmic law in\nrelation to compute budgets. Furthermore, we also confirm the power law between\nNon-Vocabulary Parameters, Vocabulary Parameters, and Data Tokens with respect\nto compute budgets respectively. Leveraging the scaling law, we predict the\noptimal transformer size, vocabulary size, and data requirements for a compute\nbudget of $1e18$. The test loss of the system, when trained with the optimal\nmodel size, vocabulary size, and required data, aligns precisely with the\npredicted test loss, thereby validating the scaling law.\n","authors":["Shunlin Lu","Jingbo Wang","Zeyu Lu","Ling-Hao Chen","Wenxun Dai","Junting Dong","Zhiyang Dou","Bo Dai","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.14559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20633v3","updated":"2024-12-19T06:22:05Z","published":"2024-05-31T05:49:37Z","title":"Skeleton-OOD: An End-to-End Skeleton-Based Model for Robust\n  Out-of-Distribution Human Action Detection","summary":"  Human action recognition is crucial in computer vision systems. However, in\nreal-world scenarios, human actions often fall outside the distribution of\ntraining data, requiring a model to both recognize in-distribution (ID) actions\nand reject out-of-distribution (OOD) ones. Despite its importance, there has\nbeen limited research on OOD detection in human actions. Existing works on OOD\ndetection mainly focus on image data with RGB structure, and many methods are\npost-hoc in nature. While these methods are convenient and computationally\nefficient, they often lack sufficient accuracy, fail to consider the exposure\nof OOD samples, and ignore the application in skeleton structure data. To\naddress these challenges, we propose a novel end-to-end skeleton-based model\ncalled Skeleton-OOD, which is committed to improving the effectiveness of OOD\ntasks while ensuring the accuracy of ID recognition. Through extensive\nexperiments conducted on NTU-RGB+D 60, NTU-RGB+D 120, and Kinetics-400\ndatasets, Skeleton-OOD demonstrates the superior performance of our proposed\napproach compared to state-of-the-art methods. Our findings underscore the\neffectiveness of classic OOD detection techniques in the context of\nskeleton-based action recognition tasks, offering promising avenues for future\nresearch in this field. Code is available at\nhttps://github.com/YilliaJing/Skeleton-OOD.git.\n","authors":["Jing Xu","Anqi Zhu","Jingyu Lin","Qiuhong Ke","Cunjian Chen"],"pdf_url":"https://arxiv.org/pdf/2405.20633v3.pdf","comment":"Accepted by Neurocomputing"},{"id":"http://arxiv.org/abs/2412.14547v1","updated":"2024-12-19T05:55:18Z","published":"2024-12-19T05:55:18Z","title":"Bright-NeRF:Brightening Neural Radiance Field with Color Restoration\n  from Low-light Raw Images","summary":"  Neural Radiance Fields (NeRFs) have demonstrated prominent performance in\nnovel view synthesis. However, their input heavily relies on image acquisition\nunder normal light conditions, making it challenging to learn accurate scene\nrepresentation in low-light environments where images typically exhibit\nsignificant noise and severe color distortion. To address these challenges, we\npropose a novel approach, Bright-NeRF, which learns enhanced and high-quality\nradiance fields from multi-view low-light raw images in an unsupervised manner.\nOur method simultaneously achieves color restoration, denoising, and enhanced\nnovel view synthesis. Specifically, we leverage a physically-inspired model of\nthe sensor's response to illumination and introduce a chromatic adaptation loss\nto constrain the learning of response, enabling consistent color perception of\nobjects regardless of lighting conditions. We further utilize the raw data's\nproperties to expose the scene's intensity automatically. Additionally, we have\ncollected a multi-view low-light raw image dataset to advance research in this\nfield. Experimental results demonstrate that our proposed method significantly\noutperforms existing 2D and 3D approaches. Our code and dataset will be made\npublicly available.\n","authors":["Min Wang","Xin Huang","Guoqing Zhou","Qifeng Guo","Qing Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14547v1.pdf","comment":"Accepted by AAAI2025"},{"id":"http://arxiv.org/abs/2412.14546v1","updated":"2024-12-19T05:52:16Z","published":"2024-12-19T05:52:16Z","title":"{S$^3$-Mamba}: Small-Size-Sensitive Mamba for Lesion Segmentation","summary":"  Small lesions play a critical role in early disease diagnosis and\nintervention of severe infections. Popular models often face challenges in\nsegmenting small lesions, as it occupies only a minor portion of an image,\nwhile down\\_sampling operations may inevitably lose focus on local features of\nsmall lesions. To tackle the challenges, we propose a {\\bf S}mall-{\\bf\nS}ize-{\\bf S}ensitive {\\bf Mamba} ({\\bf S$^3$-Mamba}), which promotes the\nsensitivity to small lesions across three dimensions: channel, spatial, and\ntraining strategy. Specifically, an Enhanced Visual State Space block is\ndesigned to focus on small lesions through multiple residual connections to\npreserve local features, and selectively amplify important details while\nsuppressing irrelevant ones through channel-wise attention. A Tensor-based\nCross-feature Multi-scale Attention is designed to integrate input image\nfeatures and intermediate-layer features with edge features and exploit the\nattentive support of features across multiple scales, thereby retaining spatial\ndetails of small lesions at various granularities. Finally, we introduce a\nnovel regularized curriculum learning to automatically assess lesion size and\nsample difficulty, and gradually focus from easy samples to hard ones like\nsmall lesions. Extensive experiments on three medical image segmentation\ndatasets show the superiority of our S$^3$-Mamba, especially in segmenting\nsmall lesions. Our code is available at\nhttps://github.com/ErinWang2023/S3-Mamba.\n","authors":["Gui Wang","Yuexiang Li","Wenting Chen","Meidan Ding","Wooi Ping Cheah","Rong Qu","Jianfeng Ren","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2412.14546v1.pdf","comment":"Accept by AAAI 2025"},{"id":"http://arxiv.org/abs/2412.14545v1","updated":"2024-12-19T05:51:46Z","published":"2024-12-19T05:51:46Z","title":"Summary of Point Transformer with Federated Learning for Predicting\n  Breast Cancer HER2 Status from Hematoxylin and Eosin-Stained Whole Slide\n  Images","summary":"  This study introduces a federated learning-based approach to predict HER2\nstatus from hematoxylin and eosin (HE)-stained whole slide images (WSIs),\nreducing costs and speeding up treatment decisions. To address label imbalance\nand feature representation challenges in multisite datasets, a point\ntransformer is proposed, incorporating dynamic label distribution, an auxiliary\nclassifier, and farthest cosine sampling. Extensive experiments demonstrate\nstate-of-the-art performance across four sites (2687 WSIs) and strong\ngeneralization to two unseen sites (229 WSIs).\n","authors":["Kamorudeen A. Amuda","Almustapha A. Wakili"],"pdf_url":"https://arxiv.org/pdf/2412.14545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16214v2","updated":"2024-12-19T05:50:06Z","published":"2024-07-23T06:42:55Z","title":"Diff-Shadow: Global-guided Diffusion Model for Shadow Removal","summary":"  We propose Diff-Shadow, a global-guided diffusion model for shadow removal.\nPrevious transformer-based approaches can utilize global information to relate\nshadow and non-shadow regions but are limited in their synthesis ability and\nrecover images with obvious boundaries. In contrast, diffusion-based methods\ncan generate better content but they are not exempt from issues related to\ninconsistent illumination. In this work, we combine the advantages of diffusion\nmodels and global guidance to achieve shadow-free restoration. Specifically, we\npropose a parallel UNets architecture: 1) the local branch performs the\npatch-based noise estimation in the diffusion process, and 2) the global branch\nrecovers the low-resolution shadow-free images. A Reweight Cross Attention\n(RCA) module is designed to integrate global contextual information of\nnon-shadow regions into the local branch. We further design a Global-guided\nSampling Strategy (GSS) that mitigates patch boundary issues and ensures\nconsistent illumination across shaded and unshaded regions in the recovered\nimage. Comprehensive experiments on datasets ISTD, ISTD+, and SRD have\ndemonstrated the effectiveness of Diff-Shadow. Compared to state-of-the-art\nmethods, our method achieves a significant improvement in terms of PSNR,\nincreasing from 32.33dB to 33.69dB on the ISTD dataset.\n","authors":["Jinting Luo","Ru Li","Chengzhi Jiang","Xiaoming Zhang","Mingyan Han","Ting Jiang","Haoqiang Fan","Shuaicheng Liu"],"pdf_url":"https://arxiv.org/pdf/2407.16214v2.pdf","comment":"Proceedings of the 39th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2412.08125v2","updated":"2024-12-19T05:46:29Z","published":"2024-12-11T06:21:33Z","title":"Progressive Multi-granular Alignments for Grounded Reasoning in Large\n  Vision-Language Models","summary":"  Existing Large Vision-Language Models (LVLMs) excel at matching concepts\nacross multi-modal inputs but struggle with compositional concepts and\nhigh-level relationships between entities. This paper introduces Progressive\nmulti-granular Vision-Language alignments (PromViL), a novel framework to\nenhance LVLMs' ability in performing grounded compositional visual reasoning\ntasks. Our approach constructs a hierarchical structure of multi-modal\nalignments, ranging from simple to complex concepts. By progressively aligning\ntextual descriptions with corresponding visual regions, our model learns to\nleverage contextual information from lower levels to inform higher-level\nreasoning. To facilitate this learning process, we introduce a data generation\nprocess that creates a novel dataset derived from Visual Genome, providing a\nwide range of nested compositional vision-language pairs. Experimental results\ndemonstrate that our PromViL framework significantly outperforms baselines on\nvarious visual grounding and compositional question answering tasks. The code\nis available at: https://github.com/lqh52/PromViL.\n","authors":["Quang-Hung Le","Long Hoang Dang","Ngan Le","Truyen Tran","Thao Minh Le"],"pdf_url":"https://arxiv.org/pdf/2412.08125v2.pdf","comment":null}]}}